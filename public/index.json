[{"content":"Immusia（读作 ɪˈmuːziə, 是拉丁语 Immersio (沉浸) 和 Musica (音乐) 的结合) 是我的第一个 Vision Pro App, 也是我的第一个 VR App\n本文记录了我的创作理念，和一些实现细节\n缘由 之所以想做这样的一个 app, 是因为我由于科幻作品的影响，产生的对 VR 设备的想象。更具体一点，是 Black Mirror。Black Mirror 擅长探讨 高科技对 人文/伦理/社会 的冲击，选材大胆，对我有着比较深的影响。在其最精彩的前几集，多次出现类似 头戴设备的概念，给了观影者极大的想象空间。\n核心功能 Immusia 的核心理念始终是 沉浸式 的音乐聆听体验。借助 “概念专辑” 这类已经被大众认可的丰富精神财产，音乐被赋予了更具像化的概念，从而使音乐可视化有了更合理的依据\n2D 资产 -\u0026gt; 3D 资产 有大量平面资源可以转为双目3D，动机很自然(Vision Pro 的相册 app 也在后来内置了这个功能）， 相关技术也随处可寻，但是视觉效果还是相当可观\n3D Player 为了充分发挥 VR 设备的空间优势，我对传统播放器做了以下改动：\n所有歌词文本都为具有深度的 3D 字体。尽管 Apple 不推荐这么做，我还是坚持使用它，原因是经过实测， depth 似乎赋予了歌词一种真实感 歌词的移动方向不再局限于 y 方向，而是可以结合 XY 和 Z。我最喜欢的移动方式，被我称为 \u0026ldquo;Water Fall\u0026rdquo;， 歌词会向 -Y 和 +Z 方向移动 Immersive Player Window Mode 由于 Swift API 的限制，app之间的 沉浸式空间是互斥的，因此纯沉浸式的 app 存在一些局限性。为了达到一个通用音乐播放器的最低要求，我加入了窗口模式，这样用户就可以在使用其他 app 时同时使用 Immusia\n窗口模式的大部分 UI 都和系统原生 Apple Music 保持一致。为了更方便查看艺术家信息和专辑信息，在左右两侧分别添加了小窗口。同时，沉浸模式下的 3D 播放器 也被保留。\n值得一提的是，为了体现空间感，我没有采用平铺的专辑列表，而是实现了一种非常类似 Apple 在 IPod 上曾经使用过的 Cover Flow 效果。它足够优雅和美观，也不会占用太多资源。\nEnvironments 环境 在 Environment 方面，我早就感到和知晓 RealityKit 的局限性和 Performance 问题，Metal(CompositorService, to be specific) 是最好的选择。然而我没有图形学方面的储备，所以花了一段时间尝试寻找合适的人选（外包团队）进行合作。显然我高估了国内在这一领域的人才储备，总之经过一段时间的尝试并无果之后，我就开始自学 Metal。在 Shadertoy和一些电子教材 的帮助之下，我得以对图形学入门，并创作了一些环境（场景）\nInterstellar Interstellar 是我搭建的第一个环境。在听太空主题的音乐时，脑海里一直有一些关于太空场景的想象，而且静态的太空场景也相对比较容易实现，对当时对图形编程不太熟悉的我来说是一个不错的上手项目\n起初我对 Interstellar 的构想比较简单：一个孤单的蓝色星球\n但是随着进度不断推进，我的想法越来越多，技术也逐渐成熟，因此我大胆地加入了不同元素：月球/太阳/星星/星云 等\n在这个过程中也遇到了很多问题，包括但不限于：\nColorSpace 的选取 透明场景下 BlendMode 和 Alpha 的设置 大量复杂 Shader 造成的计算问题，通过简化逻辑和烘焙纹理解决 CompareFunction 和 Winding 的不一致造成的遮挡错误问题 \u0026hellip; 所幸在 GPT 的帮助下，没有浪费我 太多 的时间\n这个场景中的很多元素都是我从 ShaderToy 移植而来，希望 License 不会有很大的问题\n在背景音乐的选择方面，此前选择的是现成的 太空背景音乐音效，现在正在考虑 realtime GPU Sound\n一些不错的生成结果：\nPlastic Beach Star Gate 一次和朋友闲聊，偶然听他提起正在为新歌制作一段类似 Daft Punk 的《Contact》 末尾的太空音效。我去 Youtube 上找到了一个粉丝为这首歌制作的一版 MV, 里面恰好选取了 《2001: A Space Odyssey》 中主人公 David Bowman 穿越时空隧道（Star Gate) 的片段，这一段视觉效果和音乐的节奏有着非常好的配合\n我很喜欢这个场景，于是它成为了我的第二个 Environment idea。\n一些想法 一个 VR app 涉及的技术栈太多：前端/后端/设计/GPU 渲染/UX 只是我能想起来的几个，而且互联网相对较丰富的也只是平面资源，在空间的视角下，每一部分都可以衍生出新的学问。Vision Pro 用于作为先行 Demo 的 Encounter Dinosaurs, 在 WWDC 上有一个专门的 Episode, 介绍其中的 UX 设计。这是一个全新的领域 Marketing: 我还没有进行任何形式的 Marketing 相关人才的欠缺：国内外在 VR 技术上的 技术和人才累积 似乎根本不在一个水平面上，国内的 App，即使是由大团队诸如 QQ 音乐/ 爱奇艺等，仍显粗糙；然而国外的小团队（最少1个人）都可以做出足够摘取 Apple Design Awards 的 App。这应该与当地电影工业的发展有着最直接的联系 ","permalink":"https://mickqian.github.io/posts/misc/the-making-of-immusia/","summary":"\u003cp\u003e\u003cstrong\u003eImmusia\u003c/strong\u003e（读作 \u003cstrong\u003eɪˈmuːziə\u003c/strong\u003e, 是拉丁语 \u003cstrong\u003eImmersio\u003c/strong\u003e (沉浸) 和 \u003cstrong\u003eMusica\u003c/strong\u003e (音乐) 的结合) 是我的第一个 Vision Pro App, 也是我的第一个 VR App\u003c/p\u003e\n\u003cp\u003e本文记录了我的创作理念，和一些实现细节\u003c/p\u003e\n\u003ch2 id=\"缘由\"\u003e缘由\u003c/h2\u003e\n\u003cp\u003e之所以想做这样的一个 app, 是因为我由于科幻作品的影响，产生的对 VR 设备的想象。更具体一点，是 Black Mirror。Black Mirror 擅长探讨 高科技对 人文/伦理/社会 的冲击，选材大胆，对我有着比较深的影响。在其最精彩的前几集，多次出现类似 头戴设备的概念，给了观影者极大的想象空间。\u003c/p\u003e\n\u003ch2 id=\"核心功能\"\u003e核心功能\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eImmusia\u003c/strong\u003e  的核心理念始终是 沉浸式 的音乐聆听体验。借助 “概念专辑” 这类已经被大众认可的丰富精神财产，音乐被赋予了更具像化的概念，从而使音乐可视化有了更合理的依据\u003c/p\u003e\n\u003ch3 id=\"2d-资产---3d-资产\"\u003e2D 资产 -\u0026gt; 3D 资产\u003c/h3\u003e\n\u003cp\u003e有大量平面资源可以转为双目3D，动机很自然(Vision Pro 的相册 app 也在后来内置了这个功能）， 相关技术也随处可寻，但是视觉效果还是相当可观\u003c/p\u003e\n\u003ch3 id=\"3d-player\"\u003e3D Player\u003c/h3\u003e\n\u003cp\u003e为了充分发挥 VR 设备的空间优势，我对传统播放器做了以下改动：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e所有歌词文本都为具有深度的 3D 字体。尽管 Apple 不推荐这么做，我还是坚持使用它，原因是经过实测， depth 似乎赋予了歌词一种真实感\u003c/li\u003e\n\u003cli\u003e歌词的移动方向不再局限于 y 方向，而是可以结合 XY 和 Z。我最喜欢的移动方式，被我称为 \u0026ldquo;Water Fall\u0026rdquo;， 歌词会向 -Y 和 +Z 方向移动\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"immersive-player\"\u003eImmersive Player\u003c/h3\u003e\n\u003ch3 id=\"window-mode\"\u003eWindow Mode\u003c/h3\u003e\n\u003cp\u003e由于 Swift API 的限制，app之间的 沉浸式空间是互斥的，因此纯沉浸式的 app 存在一些局限性。为了达到一个通用音乐播放器的最低要求，我加入了窗口模式，这样用户就可以在使用其他 app 时同时使用 Immusia\u003c/p\u003e","title":"(The Making Of) Immusia"},{"content":"Rocky Rocky 是 The Beatles 吉他手 George Harrison 的电吉他\nGeorge Harrison 本人曾在多个场合使用它：\nGeorge Harrison Performing With Rocky in 《Im a Walrus》\nGeorge Harrison With Rocky\nGeorge Harrison Playing Rock in Abbey Road Studio\n关于它的制作过程：\nStratocaster® guitars were almost impossible to find in England in the late 1950s and early 1960s, so when George Harrison actually found one in a shop during the pre-fame early days of the Beatles, he meant to get it but was scooped by the guitarist for Rory Storm and the Hurricanes (whose drummer went by the stage name Ringo Starr).\nA few dizzying years later, when the sessions for Beatles Help! album got under way in early 1965, Harrison had better luck - he and John Lennon sent roadie Mal Evans out to get one for each of them, and Evans soon returned with a matching pair of Sonic Blue Strat® guitars. Harrison\u0026rsquo;s guitar, serial number 83840, still bore a decal from a music store where it was purchased at one point - \u0026ldquo;Grimwoods; The music people; Maidstone and Whitstable\u0026rdquo;. Thus, Help! marks the first appearance of a Stratocaster in Beatles music; heard in the low drone throughout that album\u0026rsquo;s \u0026ldquo;Ticket to Ride\u0026rdquo; and in the solo for \u0026ldquo;You\u0026rsquo;re Going to Lose That Girl.\u0026rdquo; Near the end of 1965, both Strats were put to even more prominent use on groundbreaking album Rubber Soul, most notably on the ringing chordal solo in \u0026ldquo;Nowhere Man\u0026rdquo;, and again on mid-1966\u0026rsquo;s Revolver.\nIn 1967, sometime between the end of the Sgt. Pepper\u0026rsquo;s Lonely Hearts Club Band sessions and the June 25 live worldwide telecast of \u0026ldquo;All You Need is Love\u0026rdquo;, Harrison took up paint and brush himself to give his Stratocaster a multicolored psychedelic dayglo paint job. It also appeared prominently in the \u0026ldquo;I Am the Walrus\u0026rdquo; segment of 1967\u0026rsquo;s Magical Mystery Tour film. The guitar remained a favorite of Harrison\u0026rsquo;s for the rest of the decade, and by December 1969 Harrison had painted \u0026ldquo;Bebopalula\u0026rdquo; on the upper body, \u0026ldquo;Go Cat Go\u0026rdquo; on the pickguard and \u0026ldquo;Rocky\u0026rdquo; - the guitar\u0026rsquo;s nickname - on the headstock.\n\u0026mdash; 摘取自 # LIMITED EDITION GEORGE HARRISON ROCKY STRAT®\n其中提到，Rocky 是由一把 61\u0026rsquo; Stratocaster, 并且是由 George Harrison 本人改造而来。\n下面这段材料揭露了关于改装的细节（主要是颜料方面）：\n“During ’67, everybody started painting everything,” Harrison says, “and I decided to paint it. I got some Day-Glo paint, which was quite a new invention in them days, and just sat up late one night and did it.” (Harrison points out that some of his ex-wife Patti Boyd’s nail polish was used to paint the headstock.)\nThe guitar made appearances that year in the Beatles’ live performance of “All You Need Is Love” on Our World, the first global satellite TV program, and in the film Magical Mystery Tour, in the segment where the Beatles mime to “I Am the Walrus,”\nRocky 使用的主要颜料是 Day-Glo (Day-Glo Color Corp. 生产的一种荧光涂料), 而琴颈部位则是用他当时妻子 Patti Boyd 的指甲油涂绘而成\n我本人非常喜欢 Rocky 的配色，奈何负担不起 Rocky Custom Shop 的高昂费用，也由于墨产 Rocky Player Series 的低性价比而未选择入手\n恰巧我有一把 Fender Vintera 60\u0026rsquo;s Stratocaster, 从年代和琴型上都与原版相对接近。于是我把它送到一位网友那里，完成 Rocky 的复刻\n在改造前，它原本是日落色的，大概这样：\n日落色\n在使用 硝基漆 和一些荧光涂料改造之后：\n可以看到还原度很高，但同时保留了一些原创部分。喷绘的效果还是挺令我满意的\n我带着它参加了一些小型演出\nFender Blues Tweed Deluxe Blues Deluxe™ 是 Fender 的一款晶体管音箱，我选择它有两个原因：\n音色：我对吉他音箱研究不多，但是 Fender 音箱的清音不用多说，很悦耳 外观：这款复古气息的粗花呢黄色音箱本身就是一款装饰品 奈何功率太大，我很少使用它，最多拍视频的时候才想起来用一下\n","permalink":"https://mickqian.github.io/posts/music/my-gears/","summary":"A (brief) introduction of my gears","title":"My Gears"},{"content":" [! WARNING] This work is in progress\nNotations Meaning Classifier-guided Diffusion An implementation of Conditional Diffusion Models, which requires a classifier $\\nabla_{x}p(x|y)$ to guide its reverse process Classifier-free Diffusion An implementation of Conditional Diffusion Models, which doesn\u0026rsquo;t require a classfier, the condition serves as an input to its noise predictor Latent Diffusion Models(LDM) A type of Diffusion Models where diffusion processes are done in latent space Introduction Diffusion models usually choose a UNet as its backbone for Noise Predictor, first adopted by Ho et al [^1] , which was inherited from Pixel-CNN++(widely used as the generator in VAE) with a few changes. Although some works have introduced attention blocks into low-level design, its high-level remains intact.\nDiT is proposed to apply Transformer into Diffusion Models, adhering to the best practices of Vision Transformers(ViTs)\nAlso, the scaling behavios of transformers is also explored in DiT\nDiT Design Space Notations Meaning $C$ the channels of input image $I$ the dimension of $z$ $z \\in R^{I \\times I \\times C}$ The latents, also the input of $DiT$ Patch The unit of input. $p$ the dimension of a single patch Classifier-free Diffusion An implementation of Conditional Diffusion Models, which doesn\u0026rsquo;t require a classfier, the condition serves as an input to its noise predictor Latent Diffusion Models(LDM) A type of Diffusion Models where diffusion processes are done in latent space 1. Patchify Patchify is the first layer of DiT, which converts the spatial input $z$ into a sequence to $T$ tokens, each of dimension $d$: $$ z \\in R^{I \\times I \\times C} \\to T \\cdot Token $$ where:\n$Token \\in R^{p \\times p \\times C}$ $T = (\\frac{I}{p})^2$ 2. Positional Embeddings Following patchify, we apply standard ViT frequency-based positional embeddings (the sine-cosine version) to all input tokens: $Token \\to Patch$\n3. Transformer Blocks Following patchify, the input tokens are processed by a sequence of transformer blocks.\nIn-context conditioning In addition to noised image inputs, diffusion models sometimes process additional conditional information:\n$t$: noise timestamps (of DDPM) $c$: class labels c(of input images) natural language description(or caption) The conditions are represented as additional $Token$s, appened to the [input sequence](## 2. Positional Embeddings)\nCross-attention block The conditional tokens are send to the cross-attention block of transformer block\nAdaptie Layer Norm(adaLN) Block The Adaptive Layer Norm replaces the standard layer norm\n[! NOTE] To be completed\n4. Transformer Decoder After the transformer blocks, a transformer decoder is reponsible for decoding the each latent token back to tensor of size $p \\times p \\times C$.\nThe decoder is simply a standrad linear layer\nSora The following is some major takeaways of the tenichal report of Sora\nSora is a diffusion model/diffusion transformer based on DiT\nUnified Representation of Visual Data The major part of the technial report is about the Unified Representation, which is the training data of Sora\nSources We take inspiration from large language models which acquire generalist capabilities by training on internet-scale data\nBased on the description, Sora might crawl a huge amount of data from internet\nPatch Following the text token concept of LLM, and patch concept from [DiT](##1. Patchify), Sora has visual patches.\nThe transformation of videos into patches went through 2 steps:\nCompress videos into lower-dimensional latents Decompose the latents into spacetime patches [! TIP] spacetime implies that the tokens are embedded with timestep information\nWhile the original DiT tokens are fixed-sized based on the size of the latents, the spacetime patches used by Sora is derived from videos/images of variable resolutions, durations and aspect ratios. This gives huge flexibility in inference time, since the output is formed with any patches you like.\nScaling Transformers They find transformers scaled effectively as video models, same as in other domains, including language modeling,13,14 computer vision,15,16,17,18 and image generation.27,28,29\nData preprocessing Native size Different from prior approaches, which crops videos/images to standard size(e.g., 4 seconds videos at 256 * 256 resolution), they find that training on data at its native size benefits.\nNative aspect ratios They empirically find that training on videos at their native aspect ratios improves composition and framing. The model trained on square crops sometimes generates videos where the subject is only partially in view.\nLanguage understanding The language-understanding-ability is a crucial part of text-to-video models, as text is the major input\nRe-captioning Re-captioning is a technique to generate descriptive captions for images/videos with the help of a highly descriptive captioner model\nFirst introduced in the training of DALL$\\cdot$E, it is used in Sora, where GPT serves the role of captioner, who turn short user prompts into longer detailed captions\nPrompting with images and videos Being abled to be prompted with inputs other than text, including images and videos, Sora can perform a wide range of image and video editing tasks.\nSome applications not mentioned in the report Based on the presented applications, Sora might be able to do the tasks of:\nvideo generation conditioned on text/image: generate videos based on the given future/previous text and image Model and implementation details are not included in this report. \u0026ndash; video-generation-models-as-world-simulators\n","permalink":"https://mickqian.github.io/posts/ai/models/dit-sora-and-more/","summary":"Review on DiT and sora","title":"DiT, Sora, and more"},{"content":"Terminologies Term Meaning Full fine-tuning Fine-Tune all the weights of a pretrained model Intrinsic dimension An attribute of a dataset, essentially the minimum variable needed to encode the data low intrinsic dimension A description of a dataset, describing that the intrinsic dimension of the dataset is low $h$ The output of the model Introduction PEFT(Parameter Efficient Fine-Tuning) is a technique to reduce the training cost of full fine-tuning by minimize the parameter count and the computation complexity.\nAccording to UniPELT, existing PELT usually involves following variants:\nThe functional form of $\\Delta h$ The form of insertion into Transformer Parallel: At input layer Sequential: At output layer The representation modifies attention layer ffn llayer Composition function of $h$ and $\\Delta h$ Adapter Tuning Only fine-tune the parameters of the layers close to downstream tasks.\nWhile training, the parameter of the original pre-train model is frozen, with a newly-added adapter structure:\nDown-project layer: project the high-dim feature to lower dimension Non-linear Up-project layer: project back to high-dim Skip-connection: $identity$ in the worst case Prefix Tuning Prefix: Prepend learnable task-related virtual tokens to input tokens at $W_{k} \u0026amp; W_{v}$ of each layer An MLP after prefix layer(only in training): down-project a smaller prefix $P_{\\theta}^{\u0026rsquo;}$ to actual prefix$P_ {\\theta}$, to stablize the training [! NOTE] Similar to text prompt, but continuous and implicit\nPrompt Tuning A simplified version of Prefix Tuning, with:\nPrefix virtual tokens prepended only at input layer MLP removed. P-Tuning Notice the problem of LLM: The expression of the prompt has a significant impact on downstream tasks\nP-Tuning is proposed to change the input Prompt to learnable embedding.\nLoRA All of the PEFT methods mentioned above has some problems:\neither: increase the model depth and inference time, e.g.Adapter Tuning or: with learnable parameters which are hard to train It is observed that low intrinsic dimension is the key part of LLMs. Based on this observation, the attention matrix can be re-designed as: $$ h = \\underbrace{W_{0}}{\\text{original weight}}x + \\underbrace{\\Delta W}{\\text{Adapte}}x = W_{0}x + BAx $$ where:\n$A \\in \\mathbb{R}^{d \\times r} \\sim \\mathcal{N}(0, \\sigma^{2})$ $B \\in \\mathbb{R}^{r \\times d}$ $d \u0026gt; r$ Advantages being:\nNo additional depth introduced UniPELT UniPELT provides a unified view of existing PEFTs, and compares each choices of variants:\nParallel insertion form is bettern than Sequantial Modified representation: When the amout of parameter modified is huge, ffn is better ffn is task-related Otherwise Attention attention captures the text pattern Scaling composition function is better ","permalink":"https://mickqian.github.io/posts/ai/rl/peft/","summary":"PEFTs","title":"Tuning"},{"content":"Terminologies Term Meaning Receptive field(a.k.a. sensory space) A concept originally from biology, adopted in modern artificial deep neural networks (especially CNN), describing the size of input image which can affects the output of neurons $d_{model}$ dimension of the word embedding(usually 512 = 64 * 8) $d_{k}$ dimension of $w_q, w_k$ $d_{v}$ dimension of $w_v$ $w_{q} \\in \\mathbb{R}^{d_{model} \\times d_{k}}$ $w_k \\in \\mathbb{R}^{d_{model} \\times d_{k}}$ $w_{v} \\in \\mathbb{R}^{d_{model} \\times d_{v}}$ $B$ Batch size $S$ Sequence Length $X$ input Structural Prior translation equivariance An attribute of model, the ability of model to recognize objects does not varies with the geometric transformations of the input(shift, rotate, projection, etc) The Attention of human Selective attention is a mechanism unique to human vision. By swiftly scanning the image, human acquires important areas(a.k.a. focus). After this, human pays more attention to these areas, as there are more valuable information.\nSelf-Attention Self Scaled-Dot Attention:\n$$ \\begin{align} X \u0026amp;= X_{text_encoding}+ X_{positional_embedding}\\ Q \u0026amp;= X \\cdot w_{q} \\in \\mathbb{R}^{S \\times d_{k}}\\ K \u0026amp;= X \\cdot w_{k} \\in \\mathbb{R}^{S \\times d_{k}}\\ V \u0026amp;= X \\cdot w_{v} \\in \\mathbb{R}^{S \\times d_{v}}\\ a \u0026amp;= softmax\\left(\\frac{Q \\cdot K^{\\top}}{d_{k}}\\right)\\in \\mathbb{R}^{S \\times S}\\ Attention \u0026amp;= a V\\in \\mathbb{R}^{S \\times d_{k}}\\ \\end{align} $$\n[!TIP]\nAttention is the weighted (attention score) v of each token over other tokens The major different between attention and typical RNN is: The generation of next token doesn\u0026rsquo;t rely on hidden state from previous timestamp, but instead alter the embedding of the token directly with positional embedding.\n[!TIP] Some researchers seems Attention as a kind of soft addressing\nCross-Attention Different from Self-Attention, the $Q, K$ of cross-attention comes from another sequence($X_{2}$): $$ \\begin{align} Q \u0026amp;= X_{1} \\cdot w_{q} \\in \\mathbb{R}^{n \\times d_{k}}\\ K \u0026amp;= X_{2} \\cdot w_{k} \\in \\mathbb{R}^{m \\times d_{k}}\\ V \u0026amp;= X_{2} \\cdot w_{v} \\in \\mathbb{R}^{m \\times d_{v}}\\ a \u0026amp;= softmax\\left(\\frac{Q \\cdot K^{\\top}}{d_{k}}\\right)\\in \\mathbb{R}^{m \\times n}\\ \\end{align} $$\nThus, the attention matrix $a$ represents the attention between $X_{1}$ and $X_{2}$\nFeatures Multi-head $$ MultiHeadAttention(Q, K, V) = Concat(Attention_{i})W^{O}, i \\in (0, h) $$\nlet each head focus on one part of input, concatenating and increasing the receptive field of the NN Grammar \u0026amp; Context \u0026amp; Rare words are what heads are focusing on Compared to multi-layer attention, it can be trained parallelly Parallelism $$ Attention_{0:t, 0:t} =concat(Attention_{0:t-1, 0:t-1}, Attention_{0:t-1, t}) $$\nmaking it possible to train parallelly.\nScaling with $d_{k}$ without scaling, Softmax can easily causes gradient vanishing $\\sqrt{d_{k}}$: the variance of $q \\cdot k \\to d_{k}$, to let the variance close to 1: $Var(A \\cdot B) = Var\\left(\\sum\\limits{A_{ij}B_{ji}}\\right)=\\sum\\limits Var(A_{ij}B_{ji}) = d_{k}^{2}$ Production \u0026amp; Multiplication production: increase representation ability? multiplication: faster. performance increase along with $d_{k}$ Layer Norm \u0026amp; Batch Norm LN: Apply normalization to a whole batch. BN: Apply normalization to one position across different batches BN is often applied by CNN\nFor sequences with different lengths, same feature across sequences is irrelevant(BN is not designed to deal with variant-length sequences), so NLP prefers normalization within a sequence : LN.\nParallelism Encoder: sequential Decoder: only in training, using sequence mask(predicting next tokens of different sequences at the same time) Long-Distance Dependency Self-Attention can capture the co-dependent features in long-distance, since it avoids accumulating \u0026amp; calculating hidden states for several timestamps.\nFundamental Ideas $Q, K ,V$ is essentially a database with global semantic relationship.\nSparse Attention In standard attention mechanism, the attention between tokens are pair-wise\nHowever, it is observed that most of the time, the attention matrix $A$ is sparse\nSparse Attention Notation Meaning Attention kernel The tokens required for the attention to predict next token OpenAI reduces the time complexity by \u0026ldquo;keep the value in small region, enforcing most elements as zero\u0026rdquo;\nIt is observed that attention has gained inductive bias similar in CNN:\nshallow layers: patterns in local connection deep layers: global patterns To introduce the sparse feature of CNN into attention, they introduce a concept: Connectivity Pattern: $$ S = {S_{1}, \u0026hellip; , S_{n}} $$\nwhere $S_{i}$ is the indices at timestamp i.\nAnd attention is transformed to:\n$$ \\begin{align} a(x_{i}, S_{i}) = softmax\\left(\\frac{(W_{q}x_{i})K_{S_{i}}^{\\top}}{\\sqrt{d}}\\right)V_{S_{i}}\\ \\end{align} $$\ndecompose the full attention with sparse attentions However:\nthe kept attention region is decided by human, not dynamic Position-based Sparse Attention Atomic Sparse Attention Single-form connection of attention units\nGlobal Attention: adding some global nodes as the center of information broadcast Band Attention(a.k.a sliding window attention, or local attention): Due to the strong locality of data, limit the query of its neighboring nodes Dilated Attention: Increase inductive field by using expanding window with a hole Random Attention: 为了增加非局部交互的能力，每个查询随机采样一些边缘。这是基于观察随机图可以与完整图具有相似的光谱属性 Block Local Attention: Split the input sequence into query blocks, each block is associated with a local memory block. A query block would only focus on the key from its memory block Compound Sparse Attention Combination of the atomic attentions mentioned above\nExtended Sparse Attention Design special sparse structure for specific data\nContent-based Sparse Attention Build sparse graph on input content\nLinearized Attention Though being able to parallize, it has the complexity of $\\mathcal{O}(n^{2})$ in both time and space.\nSome work have achieved linear complexity in various way\n[!TIP] The time complexity of multipling $\\mathbb{R}^{a \\times b} \\cdot \\mathbb{R}^{b \\times c}$ is $\\mathcal{O}(abc)$\nRemove Softmax The existence of Softmax enforce the calculation of $QK^{\\top}$, which is the source of $\\mathcal{O}(n^{2})$.\nRemoving the softmax, the attention became: $A = QK^{\\top}V$:\nCalculate $K^{\\top}V$: $\\mathcal{O}(d^{2}n) \\approx \\mathcal{O}(n)$ Calculate $Q(K^{\\top}V)$: $\\mathcal{O}(nd^{2}) \\approx \\mathcal{O}(n)$ Replace Softmax with sim By rewriting the $e^{q_{i}^{\\top}k_{j}}$ with normal similarity function: $$ \\begin{equation} Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}){i} = \\frac{\\sum\\limits{j=1}^{n} \\text{sim}(\\boldsymbol{q}{i}, \\boldsymbol{k}j)\\boldsymbol{v}{j}}{\\sum\\limits{j=1}^{n} \\text{sim}(\\boldsymbol{q}{i}, \\boldsymbol{k}{j})} \\end{equation} $$\nAdding non-negative activation function(kernal method) to $q, k$: $sim(q_{i}, k_{j}) = \\phi(q_{i})\\varphi(k_{j})^{\\top}$ 《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》: $\\phi(x) = \\varphi(x) = \\text{elu}(x) + 1$ Apply Softmax to $Q, K$ separately: $\\begin{equation}Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax_2\\left(\\boldsymbol{Q}\\right)softmax_1(\\boldsymbol{K})^{\\top}\\boldsymbol{V}\\end{equation}$ , where $softmax_{i}$ means softmax in $i$-th dimension 《Efficient Attention: Attention with Linear Complexities》 Apply taylor expansion to $e^{\\boldsymbol{q}_i^{\\top}\\boldsymbol{k}_j}$: $\\begin{equation}\\text{sim}(\\boldsymbol{q}_i, \\boldsymbol{k}_j) = 1 + \\left( \\frac{\\boldsymbol{q}_i}{\\Vert \\boldsymbol{q}_i\\Vert}\\right)^{\\top}\\left(\\frac{\\boldsymbol{k}_j}{\\Vert \\boldsymbol{k}_j\\Vert}\\right)\\end{equation}$ Reformer Approximately find the maximum Attention quickly, by LSH(Locality Sensitive Hashing)\nThis inspires us to reduce time complexity by:\nintroducing the sparse bias into attention mechanism combine structural bias(deleting connections in some neurons) Linformer Project $K, V$ with two matrixes before Attention:\n$$ \\begin{align} E, F \u0026amp;\\in \\mathbb{R}^{m \\times n}\\ Attention(Q, K,V) \u0026amp;= softmax(Q(EK)^{\\top})FV \\end{align} $$\n[!TIP] Linformer is sub-sampling the sequence, so it\u0026rsquo;s nature to think of pooling\nby disentangle attention matrix with kernal feature map\nLow-rank Self-Attention it is observed that most of the time, the attention matrix $a$ is low-rank\nLow-rank Parameterization Parameterize the attention matrix with simpler structure, as an inductive bias\nLow-rank Approximation Approximate attention matrix with a low-rank matrix\nAttention with Prior Replace standard attention with prior attention distribution\nPrior that Models locality Prior from Lower Modules Adopt prior(attention) from previous attention layer\nAttention with Prior Only Derive the attention only with prior, not from the pari-wise dependency of input sequence\nImproved multi-head Head Behavior Modeling It is not guaranteed that multi-head can increase the inductive field, some works have tried:\nIncrease the feature representation ability of each head guide the interactions between each heads to achieve taht. Multi-head with restricted Spans It might be helpful to combine global heads and local heads, reasons being:\nLocality Effenciency Multi-head with Refined Aggregation Aggregate the output of each head with more complexity, rather simple concatenating them.\nImproved FFN Activation Position-wise FFN Variants GLU [GLU](Gated Linear Unit) replaces the FFN with: $$ O = (U \\odot V)W_{o}, U = \\phi_{u}(XW_{u}), V = \\phi_{v}(XW_{v}) $$ In comparison, FFN: $$ O = \\phi(XW_{u})W_{o} $$\n[!NOTE] GLU replaces first MLP with the dot-product of two MLPs\nFlash Attention GAU Simplified attention: $$ A = \\frac{1}{n}\\text{relu}^{2}\\left(\\frac{Q(Z)K(Z)^{T}}{\\sqrt{s}}\\right)= \\frac{1}{ns}\\text{relu}^{2}(Q(Z)K(Z)^{T}), Z = \\phi_{z}(XW_{z}) $$\nMove the activation before QK-step Removes the V replace softmax with relu With Time Complexity $O(n^2)$\nApplications NLP Computer Vision Audio Multi-modal ","permalink":"https://mickqian.github.io/posts/ai/models/the-attention-mechanism/","summary":"Attention","title":"The Attention Family"},{"content":"Introduction The mathematics in GAN and WGAN\nNotations Notation Meaning Jensen-Shannon Divergence An improved, symmetric version of $KL$ Divergence:\n$$D_{JS}(p||q) = \\frac{1}{2} D_{KL}\\left(p | \\frac{p+q}{2}\\right)+ \\frac{1}{2} D_{KL}\\left(q | \\frac{p+q}{2}\\right)$$ GAN Notation Meaning $p_{z}$ distribution of noise input $z$ $p_{g}$ distribution of generator output $x$ $p_{r}$ distribution of real sample $x$ Arch The original GAN is an architecture (does not define the implementation details) consisting of two models:\nDiscriminator $D$: Estimates the probability of a given sample comes from the real dataset Generator $G$: Generates synthetic samples with a noise variable input $z$ (for diversity) These two models compete against each other, forming an adversarial relationship.\nObjective Design The objective function is defined as the combinations of performances of $D$ and $G$:\n$$ \\mathcal{L}(D,G) = \\underbrace{\\mathbb{E}_{x \\sim p_{r}(r)}[\\log D(x)]}_{\\text{for D to identify real samples}} + \\underbrace{\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1 - D(G(z)))]}_{\\text{for G to generate fake samples}} $$ Using the same objective function, the direction of optimization varies:\n$D$: $D(x) \\to 1, D(G(z)) \\to 0$, so for $D$ it\u0026rsquo;s maximize $G$: $p_{g} \\to p_{r}$, since $D(p_{r}) \\to 1$, so for $G$ it\u0026rsquo;s minimize [!TIP] Other interpretations of objective function:\nThe JS-Divergence of $p_{r}, p_{g}$: $G$ tries to minimize Energy: $D$ tries to model energe-function $U(x)$, $G$ tries to generate sample $\\hat{x}$ with (local) minimum energy $U(x)$ Optimal We derive the optimal status for both models for xxx reasons.\nDiscriminator Rewrite objective function $\\mathcal{L}(D,G)$:\n$$ \\begin{align*} \\mathcal{L}(D,G) \u0026= \\mathbb{E}_{x \\sim p_{r}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}}[\\log (1 - D(G(z)))]\\\\\\\\ \u0026= \\mathbb{E}_{x \\sim p_{r}}[\\log D(x)] + \\mathbb{E}_{x \\sim p_{g}}[\\log (1 - D(x))]\\\\\\\\ \u0026= \\int_{x}{p_{r}\\log D(x) + p_{g}\\log (1 - D(x))}\u0026 \\text{actually L(D)}\\\\\\\\ \\end{align*} $$ $$ \\begin{align*} \u0026amp;\\frac{d(L(D^{\\ast}))}{ dD^{\\ast}} = \\frac{p_{r}}{D(x^{\\ast})} + \\frac{p_{g}}{1 - D(x^{\\ast})} = 0\\\\ \u0026amp;\\Rightarrow D^{\\ast}(x) = \\frac{p_{r}}{p_{r} + p_{g}}\\\\ \u0026amp;\\Rightarrow D^{\\ast}(x) = \\frac{1}{2} \\end{align*} $$\nThat is to say, when $G$ is trained to optimal, $D$ will output probability as 0.5\nGenerator $$ \\mathcal{L}(G) = \\mathcal{L}( D = D_{0}, G) = 2D_{JS}(p_{r}||p_{g}) - 2 \\log 2 $$ So the target of training $G$ can be interpreted as increasing the distance between $p_{r}$ and generated distribution $p_{g}$:\n$$ \\mathcal{L}( D, f^{\\ast}) = \\mathbb{E}_{x \\sim p_{r}}[f(x) ] - \\mathbb{E}_{z \\sim p_{z}}[f(G(z))] $$ [!TIP] Almost in all GANs, for $G$, the objective can always be interpreted as (some kind of) difference between $p_{r}$ and $p_{g}$\nProblems of GAN Hard to achieve Nash Equilibrium Ideally, two models are trained simultaneously(or iteratively) to find a Nash Equilibrium to a two-player non-cooperative game. However, they update independently without coordination, so the convergence is not guaranteed.\nA simulation of $D$ and $G$ to update regardless of each other\n[!NOTE] Maybe the idea of advesarial and cooperative(for a better overall result) are fundamentally contradictory\nVanishing gradient When $D$ does a great job but $G$ does not, $D(x) \\to 1, x \\in p_{g}$, which is a constant value, this will lead to objective function being a fixed constants, thus the $\\ell \\to 0$.\nThe log_gradients of the generator decays quickly. Also, the better the $D$, the smaller the gradient\nHence, the ability of $D$ is faced with a dilemma: it can be neither too good or too gad.\nMode Collapse During training, the generator may collapse to a setting where it always produces same outputs. This is a common failure case for GANs, commonly referred to as Mode Collapse(or Mode Dropping).\n[!TIP] The reason of mode collapse can be interpreted as: the generator finds a local optima where the loss-map is really sharp, leading to lack in diversity, due to the nature of KL-divergence\nLack of a reliable evaluation metric The score given by $D$ does not provide an objective metric of the quality of image.\nSGAN Notations Meaning $\\delta$ distribution: Dirac delta function A function, whose value is zero everywhere except at zero, and $\\int \\delta = 1$ $T$ SGAN designs the Discriminator loss as:\n$$ \\begin{equation}\\mathcal{D}{SGAN}[p(x),q(x)] = \\max{T}, \\frac{1}{2}\\mathbb{E}{x\\sim p(x)}[\\log \\sigma(T(x))] + \\frac{1}{2}\\mathbb{E}{x\\sim q(x)}[\\log (1 - \\sigma(T(x)))] + \\log 2\n\\end{equation} $$ which is basically a dual form of JS-Divergence.\nConsider a case, where $p(x)$ and $q(x)$ has no intervention: $$ \\begin{equation}p(x)=\\delta(x-\\alpha),q(x)=\\delta(x-\\beta)\\end{equation} $$ This changes loss function into: $$ \\begin{equation}\\mathcal{D}[p(x),q(x)] = \\max_T, \\frac{1}{2}[\\log \\sigma(T(\\alpha))] + \\frac{1}{2}[\\log (1 - \\sigma(T(\\beta)))] + \\log 2\\end{equation} $$\nAnd while training, $T(\\alpha) \\to +\\infty, T(\\beta) \\to -\\infty$\nWGAN Notation Meaning Wasserstein Distance(a.k.a. Earth Mover’s distance, short for EM distance) $\\mathcal{W}$ A measure of the distance between two PD, as the minimum energy cost of moving and transforming two PDs for them to be the same, a.k.a. the optimal transport cost. $$\\mathcal{W}(p,q) = \\inf_{\\gamma \\sim \\prod(p,q)}{\\mathbb{E}_{(x,y) \\sim \\gamma}[c(x,y)]}$$ Lipschitz continuity A strong form of uniform continuity for functions. If $\\vert f(x_{1} - f(x_{2})\\vert \\le K\\vert x_{1} - x_{2}\\vert$, f is called $K$-Lipschitz, formally $\\vert f\\vert _{L} \\le K$ infimum $\\inf$ the greast lower bound, in most cases the minimum supremum $\\sup$ the least upper bound, in most cases the maximum critic $f(.)$ the form of $D$ in WGAN. No longer outputting the realism (a probability) of sample, it learns a specific distance (or a score, of distributions) from samples. $f(x) = D(x)$ [!TIP] W-Distance is better than JS and KL for providing a smooth measure of distributions, even when they are disjoint\nWGAN(Wasserstein Generative Adversarial Nets) is proposed to replace JS-divergence with W-distance, due its smooth-measurement nature.\nStarting from the W-Distance of $p_{r}, p_{g}$, the objective function of WGAN is derived into the form:\n$$ \\mathcal{L} = \\mathcal{W}_{K}(p_{r}, p_{g}) = \\sup_{f, |f|_{L} \\le K}{\\mathbb{E}_{x \\sim p_{r}}[f(x) ] - \\mathbb{E}_{x \\sim p_{g}}[f(x)]} $$ WGAN consists of :\n$f(.)$ : Be used to approximate the W-Distance between two distributions to whom inputs belongs. It is trained by maximizing the objective function, formally, the subtraction of two scores. It is meant to find the maximum from all possible $f$ $G$: Generates samples. It is trained to minimize the objective function [!TIP]\nIn order to let critic $f$ subject to Lipschitz continuity, weight clipping is applied to indirectly achieve that. Unlike original GAN, critic can be trained as good as possible but not leading to problems like gradient vanishing and mode collapse WGAN-GP Although improved, WGAN sometimes suffer from problems like Hard to achieve Nash Equilibrium, still.\nIdentifying and proving weight clipping as a reason of that, authors of WGAN-GP replace it with gradient penalty(GP):\n$$ \\begin{align} \\mathcal{L} = \\underbrace{\\mathbb{E}_{\\tilde x \\sim p_{g}}[D(\\tilde x)] - \\mathbb{E}_{\\tilde x \\sim p_{r}}[D(x)]}_{\\text{Original critic loss}} + \\underbrace{\\lambda\\mathbb{E}_{\\tilde x \\sim p_{\\tilde x }}[(||\\nabla_{\\hat x}(D(\\hat x))||_{2}- 1)^2]}_{\\text{gradient penalty}} \\end{align} $$ GAN-QP Consider the following divergence: QP-div, quadratic potential divergence: $$ \\begin{equation}\\begin{aligned}\u0026amp;\\mathcal{D}[p(x),q(x)] \\ =\u0026amp; \\max_{T}, \\mathbb{E}_{(x_r,x_f)\\sim p(x_r)q(x_f)}\\left[T(x_r,x_f)-T(x_f,x_r) - \\frac{(T(x_r,x_f)-T(x_f,x_r))^2}{2\\lambda d(x_r,x_f)}\\right]\\end{aligned}\\end{equation} $$ where:\n$\\lambda \u0026gt; 0$ $d$ is any distance function It\u0026rsquo;s natural to build Loss Function for $D$ and $G$ with the Divergence, but for $G$, the quadratic term is neglected, since minimizing the denominator $d$ is not feasible. $$ \\begin{equation}\\begin{aligned}\u0026amp;T= \\mathop{\\arg\\max}T, \\mathbb{E}{(x_r,x_f)\\sim p(x_r)q(x_f)}\\left[T(x_r,x_f)-T(x_f,x_r) - \\frac{(T(x_r,x_f)-T(x_f,x_r))^2}{2\\lambda d(x_r,x_f)}\\right] \\ \u0026amp;G = \\mathop{\\arg\\min}G,\\mathbb{E}{(x_r,x_f)\\sim p(x_r)q(x_f)}\\left[T(x_r,x_f)-T(x_f,x_r)\\right]\\end{aligned}\\end{equation} $$\nf-GAN Notation Meaning f-divergence A generalized version of KL-divergence and JS-divergence: $$\\mathcal{D}_{f}(P|Q) = \\int{q(x)f(\\frac{p(x)}{q(x)})dx}$$, where $f$ is a convex function with measures the difference between PDs $T$ The gradient of $f = \\frac{p(x)}{q(x)}$ In a nutshell, the $f$-GAN:\nDecide an $f$ Learn a model $\\mathcal{D}$ to score the input $x \\sim p_{r}, y \\sim p_{g}$, whose results can be used to approximate the $f$-divergence between $p_{r}$ and $p_{g}$ Formulate the objective function as: $$\\mathcal{L} = \\mathbb{E}_{x \\sim P_{\\text{data}}} [T^*(D(x))] - \\mathbb{E}_{z \\sim P_z} [f^*(T^*(D(G(z))))]$$ BiGAN Changes the task of $D$, $D(x, z)$ outputs the possibility of $x$ coming from real image.\nBesides, the divergence of $p(x|z)$ and prior $\\mathcal{N}(0, I)$ is no longer included in the loss, the target is achieved by optimizing $D$\n$$ \\mathcal{L} = (0 - D_{x\\sim p_{g}, z\\sim \\mathcal{N}(0, I)}(x,z)))^{2} + (1 - D_{x\\sim p_{r}, z\\sim p(z|x)}(x,z)))^{2} $$\nSAGAN GAN fails to capture the invariant geometric pattern within some categories, e.g. the fur of dogs. This is not brought by GAN itself, since it doesn\u0026rsquo;t imply any implementation, but brought by relying on Convolution to capture dependency between different regions.\nSAGAN(Self-Attention GAN) proproses two changes:\nIntroduce self-attention into GAN: promotes the long-distance dependency and description of geometric features of image generation Introduce spectral normalization into experiments, achieving better results Set different learning rates for $G$ and $D$: $l_{r_{D}}$ = 4e-4, $l_{r_{G}}$ = 1e-4 hinge adversarial loss: $L_{D} = -\\mathbb{E}{(x,y)\\sim p{data}}[min(0, -1 + D(x,y))] -\\mathbb{E}{z\\sim p{z,}y \\sim p_{data}}[min(0, -1 - D(G(z),y))]$, restricting ability of $D$ BigGAN Notations Notations Meaning Orthogonal matrix A matrix, which row/column vectors are unit vector, and co-orthogonal. The reverse matrix of an orthogonal matrix is equal to its transpose matrix, $A^{T}A = AA^{T} = I$. The transpose of a real-valued orthogonal matrix is orthogonal. Conjugate transpose A matrix as a result of taking the conjugate of each elements, then transpose the original matrix. For real-valued matrixs, conjugate transpose is equal to transpose, since the conjugate of a real value is itself Singular Value Decomposition(SVD) An algorithm for decomposing matrix as eigenvector and eigenvalue, representing $A$ as: $A = U\\Sigma V^{\\ast}$, where $\\Sigma$ is eigenvalue, $U$ is left eigenvector, $V$ is right eigenvector Eigenvalue Special value of a matrix, indicating the energy of a matrix in according eigenvector direction. In the direction of a eigenvector with bigger eigenvalue, data varies faster(e.g. the variation of the projected-matrix is bigger in the direction), so keeping the biggest eigenvalues is ideal in data compresion/dimension reduction Singular Value eigenvalue of the matrix Spectral Norm (a.k.a. Induced 2-norm) Maximum Singular value of the matrix:$$|A|{2} = \\max{|x|{2}\\ne0} \\frac{|Ax|{2}}{|x|_2}$$By restricting spectral norm, Lipschitz can also be controlled. BigGAN bases on SAGAN, introduce several improvements:\nSend $z$ to multiple layers of $G$ with skip-net instead of input layer only Truncation trick: Truncate $z$ by setting threshold, exceeded sample is resampled until falling into the range Hinge adversarial loss: Spectral Norm VAE-GAN The original output of VAE is blurry. By adding a discriminator, we hope it will recognize the blurs in the image.\n","permalink":"https://mickqian.github.io/posts/ai/models/the-gan-family/","summary":"Personal takeaways of GAN \u0026amp; its variants","title":"GAN \u0026 Variants"},{"content":"Terminologies General:\nTerm Meaning Reinforcement Learning A branch/paradigm of machine learning, concerned with how an intelligent agent behaves in a dynamic environment. BLUE(bilingual evaluation understudy) An algorithm for evaluating the quality of text which has been machine-translated from one natural language to another Reward Model(Actor model) A model aligned with human feedback, predicting the reward of given actions $G_{t}$ Return(aka the future reward), total sum of discounted rewards after time $t$: $G_{t} = {\\sum}^{\\infty}_{k = 0}\\gamma^{k}R_{t + k + 1}$ $V_{\\pi}(s)$ State-value function, measures the expected return of state $s$: $V(s) = \\mathbb{E}_{\\pi}[G_{t}\\vert S_{t} = s]$ under $\\pi$ $Q_{\\pi}(s,a)$ Action-value function, measures the expected return of action $a$ under state $s$: $Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_{t} \\vert S_{t} = s, A_{t} = a]$ under $\\pi$ Bellman Equations A set of equations that decompose the value function into immediate reward + discounted future values $A_{q}$ the action to update $Q$ $A_{t+1}$ the actual taken action In RL Algorithms (mostly adjectives):\nTerm Meaning Model-based Algorithms of RL relying on a (environment dynamic) model, which defines $P(s\u0026rsquo;\\vert s,a), R(s,a)$ Model-free Algorithms of RL learning by the interaction of the model with environment Policy-Based(Policy Gradient) Methods A branch of RL: quantize each action as PDF Value-Based Methods A branch of RL: quantize each action as value(PMF ? ) Current policy The policy(actions) actually taken by an agent in an episode On-policy Using the action in current(actually exploited/taken) policy to update $V$ Off-policy Using an action not from current policy to update $V$ Introduction Value-based Dynamic Programming We can use Dynamic Programming to iteratively update and query value functions ($V_{\\pi}$), with the help of Bellman equations, when the model is fully known.\nMonte-Carlo #model_free\nInstead of modeling the environment, MC methods learns from episodes of raw experience, approximating the observed mean return as expected return.\nTo optimally learn in MC, we take following steps:\nImprove the policy greedily: $\\pi(s) = \\underset{a}{argmax}Q(s, a)$ Generate a new episode with the combination of the new policy $\\pi$ and randomness(e.g. $\\epsilon$-greedy), balancing between exploitation and exploration Estimate $Q$ with the generated episode $\\pi$ Temporal Difference methods #model-free\n[!NOTE] TD learning can learn from incomplete episodes\nBootstrapping Estimate the rewards, rather than exclusively carrying out the episode.\nValue Estimation The estimated Value funciont $V$ is updated towards an estimated return $R_{t+1} + \\gamma V(S_{t+1})$\nSARSA #on-policy\n[!TIP] Define $A_{q}$ as the action to update $Q$\nState-Action-Reward-State-Action In each $t$:\nChoose $A_{t} = \\underset{a \\in A}{argmax}{Q(S_{t}, a)}$ with $\\epsilon$-greedy Obtain $R_{t + 1}$ Set $A_{t+1} \\sim \\pi(\\cdot|s) = A_{q}$, under current policy Update $Q$ with the advantage of actual $A_{t+1}$ over expected reward: $$ Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha(\\underbrace{R_{t+1} + \\gamma Q(S_{t + 1}, A_{t + 1})}_{\\text{value of current policy, on-policy}} - \\underbrace{Q(S_{t},A_{t})}_{\\text{expected value}}) $$ $t = t + 1$ [!NOTICE] $A_{q} == A_{t + 1}$, making it on-policy\nQ-Learning #off-policy\nQ-learning is an off-policy method, with the steps in one episodes ($t, S_{t}$) being:\nChoose $A_{t} = \\underset{a \\in A}{argmax}Q(S_{t}, a)$ with $\\epsilon$-greedy Obtain $R_{t + 1}$ Set $A_{t+1} \\sim \\pi(\\cdot|s)$, $A_{q} = \\underset{a \\in A}{\\max} Q(S_{t + 1}, a)$ Update $Q$ with the advantage of optimal $A_{t + 1}$ over expected reward:\n$$Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha(\\underbrace{R_{t+1} + \\gamma \\underset{a \\in A}{\\max} Q(S_{t + 1}, a)}_{\\text{best value after $A_{t}$, off-policy}} - \\underbrace{Q(S_{t}, A_{t})}_{\\text{expected value}}) $$ $t = t + 1$ [!NOTICE] $A_{q} = \\underset{a \\in A}{\\max} Q(S_{t + 1}, a) \\ne A_{t + 1}$, making it off-policy\nDQN #off-policy\nDeep Q-Network, An improvement of Q-Learning:\nExperience Replay: All the episode steps $e_{t} = (S_{t}, A_{t}, R_{t}, S_{t+1})$ are stored in one replay memory $D_{t} = {e_{1}, \u0026hellip;, e_{t}}$. During Q-learning updates, samples are drawn at random from the replay memory and thus one sample could be used multiple times. Periodically Updated Target: Q is optimized towards target values that are only periodically updated(not updated in each iteration anymore). The Q network is cloned and kept frozen as the optimization target every C steps (C is a hyperparameter). [!WARNING] Known for overestimating value function $Q$\nPolicy Gradient $$ \\begin{align*} J(\\theta) = \\underset{s \\in S}{\\sum\\limits} d^{\\pi}(s)V^{\\pi}(s) = \\underset{s \\in S}{\\sum\\limits} d^{\\pi} \\underset{a \\in A}{\\sum\\limits} \\pi_{\\theta}(a|s)Q^{\\pi}(s,a) \\end{align*} $$ Actor-Critic Actor-Critic learns the value function in addition to the policy, assisting the policy update.\nIt consists of two models:\nActor updates the policy $\\theta$ of $\\pi_\\theta(a|s)$, suggested by critic Critic updates the value estimation function $Q(a|s) | V_{w}(s)$ The main process being, for $t \\in (1, T)$:\nSample $a \\sim \\pi_{\\theta}(a|s), r_{t} \\sim R(s,a), s\u0026rsquo; \\sim P(s\u0026rsquo;|s,a)$, next action $a\u0026rsquo; \\sim \\pi_{\\theta}(a\u0026rsquo;|s\u0026rsquo;)$ Update Actor $\\theta$: $$ \\theta \\leftarrow \\theta + \\alpha_{\\theta} Q_{w}(s,a)\\nabla_{\\theta} ln \\pi_{\\theta}( a|s) $$\nto maximize the reward 5. Compute the correction (TD error, measures the quality of current policy $a\u0026rsquo;$): $$ \\delta_{t} = \\underbrace{r_{t} + \\gamma Q_{w}(s', a')}_{\\text{Action-Value of a'}} - \\underbrace{Q_{w}(s,a)}_{\\text{actual reward}} $$ 6. Update Critic: $w \\leftarrow w + \\alpha_{w}\\delta_{t}\\nabla_{w}Q_{w}(s,a)$ to reduce estimate error (ideally, $\\delta_{t} \\leftarrow 0$, as $a\u0026rsquo; \\sim \\pi_{\\theta}(a\u0026rsquo;|s\u0026rsquo;)$) 7. Update $a \\leftarrow a\u0026rsquo;, s \\leftarrow s'$\n[!TIP] Adversarial training, resembles GAN: (generator, discriminator)\nA2C Model Meaning Actor $\\pi_{\\theta}$ The target model Critic Estimate $V$ $$ L(\\theta) = -\\log \\pi_{\\theta}(a_{t}|s_{t})\\hat A_{t} $$\nwhere:\n$\\hat A$: advantage function, the advantage of $a_{t}$ compared with average, normally $V$ [!WARNING] This objective function can lead to massive change to policy\nA3C Asynchronous Advantage Actor-Critic focuses on parallel training. Multiple actors are trained in parallel and get synced with global parameters.\nDPG #model-free #off-policy\nDeterministic Policy Gradient models the policy as deterministic function $a = \\mu(s)$.\nIt is trained by maximizing the objective function: the expected discounted reward:\n$$ J(\\theta) = \\int_{S}\\rho^{\\mu}(s)Q(s, \\mu_{\\theta}(s))ds $$\nwhere:\n$\\rho^{\\mu}(s\u0026rsquo;)$: discounted state distribution $\\mu$: the deterministic action predictor DDPG #model-free #off-policy\nDeep Deterministic Policy Gradient\nCombining DQN (experience replay, freezing target model) and DPG\nKey design:\nBetter exploration: $\\mu’(s) = \\mu_{\\theta}(s) + \\mathcal{N}$, adding noise $\\mathcal{N}$ to policy Soft updates: Moving average of parameter $\\theta$ TD3 Twin Delayed Deep Deterministic applied tricks on DDPG to prevent overestimating value function:\nClipped Double Q-learning: Action selection and Q-value estimation are made by two networks separately. Delayed update of target the policy network: Instead of updating actor and critic in one iteration, TD3 updates the actor at a lower frequency than critic, waiting for it to become stable. It helps reducing the variance. Target policy smoothing: Introduce a smoothing regularization strategy by adding $\\epsilon \\sim clip(\\mathcal{N}(0, \\sigma), -c , +c)$ to the value function $Q_{w}(s\u0026rsquo;, \\mu_{\\theta}(s\u0026rsquo;) + \\epsilon))$. It mitigates the risk of deterministic policies overfitting the value function. SAC Soft Actor-Critic learns a more random policy by incorporating the entropy of the policy $H(\\pi)$ into the reward.\nThree key components:\nAn actor-critic architecture An off-policy approach Entropy Maximization to encourage exploration The policy is trained by maximizing the objective function: expected return + the entropy $$ J(\\theta) = \\sum\\limits_{t = 1}^athbb{E}_{s_{t},a_{t} \\sim \\rho_{\\pi_ {\\theta}}} [r(s_{t},a_{t}) + \\alpha \\mathcal{H}(\\pi_{\\theta}(* | s_{t}))] $$ PPO (Proximal Policy Optimization) #on-policy\n[!TIP]\nclipped objective Proximal stands for Reward Model As a successor of A2C, PPO defines 2 more models:\nModel Meaning Reward $r_{\\theta}$ Calculate $R$ Reference $\\pi_{ref}$ Apply constraint and guidance to Actor $r^{\\ast}$ Ground-truth reward function $r_\\phi$ MLE of $r^{\\ast}$ $$ L(\\theta) = \\underbrace{-\\hat A_{t} \\cdot min(r_{t}(\\theta), clip(r_{t}(\\theta), 1 - \\epsilon, 1 + \\epsilon))}{\\text{A2C loss, $\\le 1$ + $\\epsilon$}} - \\underbrace{\\beta D{KL}(\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x))}_{\\text{penalty of being too distant to normal response}} $$\nwhere:\n$r_{t}(\\theta) = \\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}$: the ratio of new policy to old policy $\\epsilon$: normally 0.1 or 0.2 Generate two outputs from same input $x$: $y_{1}, y_{2}$ Objective: $\\mathcal{L} = \\underset{\\pi_{\\theta}}{\\max}\\mathbb{E}[r_{\\theta}(x,y_{2})]$ Update: Optimize with the reward of current batch TRO(Trust Region Optimization): using gradient constraint to make sure the update process doesn\u0026rsquo;t sabotage the stability of learning process. [!TIP]\n$r$ and $\\pi$ can be optimized iteratively. RLHF and PPO is difficult to train. DPO(Direct Preference Optimization) [!NOTE] The major difference Direct: directly optimize with reward, rather than $V | Q$: expected rewards from a reward model\nRewrite objective:\n$$ \\begin{align*} \\pi \u0026amp;= \\underset{\\pi}{\\max}(r_{\\phi}(x,y) - \\beta D_{KL}(\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)))\\\\ \u0026amp;= \\underset{\\pi}{\\max}(r_{\\phi}(x,y) - \\beta \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)})\\\\ \u0026amp;= \\underset{\\pi}{\\min}( \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} - r_{\\phi}(x,y)/\\beta)\\\\ \u0026amp;= \\underset{\\pi}{\\min}( \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x) e^{r_{\\phi}(x,y)/\\beta} }) \\end{align*} $$\n^0639d4\nDefine partition function: $Z = \\Sigma_{y}{\\pi_{ref}(y|x) e^{r_{\\theta}(x,y)/\\beta}}$, which relates to the reward of $\\theta$ over $ref$\nWe can get the optimal strategy $\\pi^{\\ast}$ under $r_{\\phi}$(irrelevant of $\\theta$):\n$$ \\pi^{*}(y|x) = \\pi_{ref}(y|x)e^{\\frac{r_{\\phi} (x,y)}{\\beta}} \\frac{1}{Z(x)} $$\n^5ee375\nThen Eq [[#^0639d4]] became:\n$$ \\begin{align*} \\pi \u0026amp;= \\underset{\\pi}{\\min}\\left( \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x) e^{r_{\\phi}(x,y)}{\\beta}}\\right)\\\\ \u0026amp;= \\underset{\\pi}{\\min}\\left( \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi^{\\ast}(y|x) Z(x)}\\right)\\\\ \u0026amp;= \\underset{\\pi}{\\min}\\left( \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi^{\\ast}(y|x)}\\right)\\\\ \u0026amp;= \\underset{\\pi}{\\min}\\left( D_{KL}(\\pi_{\\theta}(y|x) || \\pi^{\\ast}(y|x))\\right) \\end{align*} $$\nApparently, the optimal $\\pi$ is: $\\pi_{\\theta} \\to \\pi^{*}$.\nNoticing that the reward function of E.Q. [[#^5ee375]] can be rewritten(reparameterized) as(where $\\pi_{ref}$ is the human-preference data as ground-truth):\n$$ r_{\\phi} (x,y) = \\beta \\log \\frac{\\pi^{\\ast}(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x) $$\n[!TIP] the reward function can be represted with best policy trained under it\nBy replacing $r_{\\phi} (x,y)$ in the objective of RLHF as $\\pi^{*}$, we get an objective function without the reward function:\n$$ \\begin{align} \\mathcal{L}_{\\text{DPO}}(\\pi_{\\theta}; \\pi_{ref}) = -{{\\mathbb{E}_{(x, y_{w}, y_{l}) \\sim D}[\\log \\sigma{({\\beta \\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{ref}(y_{w}|x)} - \\beta\\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{ref}(y_{l}|x)} }})]}} \\end{align} $$ From this equation, we found that: Training the reward model in RLHF is equivalent to training $\\pi_{\\theta}$ with the derived objective function.\nThat is to say, no need of 4 models, we can achieve the same target of RLHF with directly training $\\pi_{\\theta}$.\nMethods RLHF RLHF(Reinforcement learning from human feedback) is a technique that trains a reward model.\nIt has following key concepts:\nReward Model: trained in advance directly from human feedback human feedback: data collected by asking humans to rank instances of the agent\u0026rsquo;s behavior The procedure is given by 3 steps\n1. SFT Pre-train a (target) model: $\\pi^{SFT}$\n2. Reward Modeling Phase Train a reward model: $r_{\\phi}(x,y) = r, r \\in (0, + \\infty)$, where $r$ is the reward of the given input.\nInitialization: Often initialized from Pretrained Models\nData:\n$D$: $Prompt: x \\to (Generation: y, Reward: r)$, generated by human or models Human Feedback: Ranking the outputs of different models under same prompt with $r$ effective ways of ranking: Comparing two/ ELO $(y_{win}, y_{loss})$ : sampled from generation Train the RM with Data The Objective is (negative log-likelihood loss): $$ \\begin{align*} \\mathcal{L}_{R}(r_{\\phi}, D) = -{{\\mathbb{E}_{(x, y_{w}, y_{l}) \\sim D}[\\log{\\sigma({r_{\\phi}(x, y_{w}) - r_{\\phi}(x, y_{l})}})]}} \\end{align*} $$ maximize the gap of rewards between better/worse response\n3. RL Fine-Tuning Phase: $\\pi_{\\theta}(x) = p(y)$ In the past, training LM with RL was considered impossible. One of the proposed feasible plan is PGR(Policy Gradient RL)/PPO(Proximal Policy Optimization) Conclusion ","permalink":"https://mickqian.github.io/posts/ai/rl/reinforcement-learning/","summary":"Personal takeaways of RL/RLHF/DPO","title":"Reinforcement Learning"},{"content":"Terminologies Term Meaning Diffusion Models models that can sample from a highly complex probability distribution(e.g. images of cars) Non-equilibrium thermodynamics a branch of thermodynamics that deals with physical systems that are not in thermodynamic equilibrium, where \u0026ldquo;there are no net macroscopic flows of matter nor of energy within a system or between systems\u0026rdquo;. It is often used by diffusion models as a technique to sample from distribution. Diffusion the net movement of anything, generally from a region of higher concentration to a region of lower concentration. Also a technique of Non-equilibrium thermodynamics DDPM model that improves the performance of diffusion models by variational inference DDIM a generalized version of DDPM, with better performance and less diversity and quality Jensen inequality $f(\\sum\\limits a_{i}x_{i}) \\le \\sum\\limits a_{i}f(x_{i})$, where $a \\ge 0, \\sum\\limits a_{i} = 1$\nIn other words, the Expected Value of a convex function $\\ge$ the value of the function at the Expected Input Variational Lower Bound(short for VLB, a.k.a. Evidence Lower BOund, short for ELBO) A easy-to-train lower bound of Log-Likelihood, derived by using a prior $p(z)$ to approximate (implies variational) an intractable posterior $q$. Jacobian matrix A matrix derived from a vector of function of several variables, with all its first-order partial derivatives. Suppose $f: R^{n} \\to R^{m}$:\n$$\nJ = [\\frac{\\partial{f}}{\\partial{x_{1}}}\u0026hellip;\\frac{\\partial{f}}{\\partial{x_{n}}}]\n$$ Notations Notation Meaning $x_{0}$ the data point, where $t$ is the total count of timestamp $x_{t}$ the data after applying $t$ times of forward iteration $\\epsilon_t$ the (standard gaussian) noise $\\epsilon_{\\theta}(x_t,t)$ our model to predict the noise at each timestamp $\\mu_{\\theta}(x_t, t)$ parameterized model to predict $x_{t-1}$ at time $t$ $p(x_{0:T})$ the joint distribution of $x_{0}, x_{1} \u0026hellip; x_{T}$ Introduction This article will introduce the definitions of DDPM and DDIM.\nAs stated earlier, the work of DDIM is based on DDPM.\nDDPM Diffusion Models often involves modeling two processes:\nforward process: noise data($x_{0}$) to data point($x_{t}$) reverse process: data point to noise data, the reversion of forward process Forward Process As a improvement of Diffusion Models, DDPM models the forward process as:\n$$ \\begin{equation} x_{t} = \\alpha_{t} x_{t-1} + \\beta_{t}\\epsilon_{t}, \\epsilon_{t} \\sim \\mathcal{N}(0,1), 0 \\le t \\le T, t \\in \\mathbb{Z} \\end{equation} $$\nwhere $\\alpha, \\beta \u0026gt; 0, \\alpha_{t}^{2}+ \\beta_{t}^{2} = 1$. This can be viewed as:\nthe remains from the previous data: $\\alpha_{t} x_{t-1}$ the destruction by introducing noise $\\epsilon_{t}$ Accordingly, the conditional probability of $x_t$ would be: $$ p(x_{t}| x_{t-1}) = \\mathcal{N}(x_{t};\\alpha_{t}x_{t-1}, \\beta_{t}^{2}I) $$\n[!NOTE]\nAll $\\alpha, \\beta, T$ are constants Apparently, this is a Markovian process Reverse Process By applying the forward process for $T$ times, we have $t$ pairs of $(x_{t-1}, x_{t})$. This is our training data.\nReversing the forward process, the task of the reverse process should be: learn how to get $x_0$ from $x_{t}$, formally $x_0 \\to x_{t}$.\nDDPM splits this process into $t$ steps of $x_t \\to x_{t-1}$.\n[!TIP] The Methodology of DDPM DDPM is a Likelihood-based Model.\nIn the paper, they model each single step as a gaussian transition: $$ p_{\\theta}(x_{t-1}|x_{t}) = \\mathcal{N}(x_{t-1};\\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t},t)) $$ where:\n$\\mu_{\\theta}(x_{t}, t)$: the mean value $\\Sigma_{\\theta}(x_{t},t)$: the variance predictor (of reverse process). [!NOTE]\nThe noise is not gaussian noise multiplied by a factor, but predicted directly. Not to confuse: * $\\Sigma_{\\theta}(x_{t},t)$: the noise in reverse process. It has been tested positive to the reverse process * $\\epsilon_\\theta(x_{t},t) \\to \\epsilon_{t}$ : the noise predictor (of forward process) Mean Value Predictor: $\\mu_{\\theta}(x_{t}, t)$ In order to model the $\\mu_{\\theta}(x_{t}, t)$, from Bayes\u0026rsquo; Theorem we have: $$ p(x_{t-1}| x_{t},x_0)= \\frac{p(x_t|x_{t-1}) p(x_{t-1} | x_0)}{p(x_{t}|x_{0}) } $$\nThe process of induction would be:\nPredict $x_t$, $x_{t-1}$ from $x_0$ Replace all the variables($x_{0}$) with $x_{t}$ in Equation 4 Predict $x_{0}$ with $x_{t}$ Applying forward process $p(x_{t}|x_{t-1})$ for $t$ times, we can rewrite $x_{t}$ as:\n$$ x_{t} = \\bar \\alpha_{t} x_{0} + \\bar{\\beta_{t}} ^ {2}\\epsilon_{t}, \\bar \\alpha_{t} = \\prod \\alpha_{i}, \\bar \\beta_{t} = \\sqrt{1-\\bar \\alpha_{t}^{2}} $$ And the probability version: $$ p(x_t|x_{0}) = \\mathcal{N}(x_{t}; \\bar \\alpha_{t} x_{0}, \\bar \\beta_{t} ^ {2}I) $$\nNow that we have $x_t$ from $x_0$, update Eq 4 (since $\\mathcal{N}$ can be represented as probabilities, the result is conformed to $\\mathcal{N}$ as well):\n$$ p(x_{t-1}| x_{t},x_{0}) = \\mathcal{N}\\left(x_{t-1}; \\underbrace{\\frac{\\alpha_{t}\\bar \\beta_{t-1}^{2}}{\\bar \\beta_{t}^{2}}x_{t} + \\frac{\\bar \\alpha_{t-1}\\beta_{t}^{2}}{\\bar \\beta_{t}^{2}}x_{0}}_{\\text{$\\tilde \\mu_t(x_{t}, x_{0})$}},\\frac{\\bar \\beta_{t-1}^{2}\\beta_{t}^{2}}{\\bar \\beta_{t}^{2}}I\\right) $$ Let\u0026rsquo;s define the predicted mean value of $x_{t-1}$ as $\\tilde \\mu_t(x_{t}, x_{0}) = \\frac{\\alpha_{t}\\bar \\beta_{t-1}^{2}}{\\bar \\beta_{t}^{2}}x_{t} + \\frac{\\bar \\alpha_{t-1}\\beta_{t}^{2}}{\\bar \\beta_{t}^{2}}x_{0}$.\nNotice the meaning of it: With Bayes\u0026rsquo; Theorem, using $x_{t}$ and $x_{0}$, we can derive the mean value of $x_{t-1}$.\nSo naturally, we can make our $\\mu_{\\theta}(x_{t}, t)$, who have the same estimated output as $\\tilde \\mu_t(x_{t}, x_{0})$, learn the distribution of it: $$ \\mu_{\\theta}(x_{t}, t) = \\tilde \\mu_t(x_{t}, x_{0}) $$\n[!NOTE] Different ways of modeling $\\mu_\\theta$ is also acceptable, it\u0026rsquo;s just that this is a better way (or not)\nHowever, we don\u0026rsquo;t have $x_0$ to pass to $\\tilde \\mu_t(x_{t}, x_{0})$. Luckily, we can predict $x_0$ from rewriting Equation 7: $$ x_{0}= \\frac{x_{t} - \\sqrt{1- \\bar \\alpha_{t}}}{\\sqrt{\\bar \\alpha_{t}}}\\epsilon_{t} $$\n[!TIP] This is actually an embodiment of the predictor–corrector method\nSince we don\u0026rsquo;t have $\\epsilon$ in the reverse process, we can make a neural work to learn it: $\\epsilon_\\theta(x_t,t) \\to \\epsilon_t$ : $$ x_{0}= \\frac{x_{t} - \\sqrt{1- \\bar \\alpha_{t}}}{\\sqrt{\\bar \\alpha_{t}}}\\epsilon_{\\theta}(x_{t}, t) $$\nUpdate the Eq 10: $$ \\mu_{\\theta}(x_{t}, t) = \\tilde \\mu_t(x_{t}, x_{0}) = \\tilde \\mu_t\\left(x_{t}, \\frac{x_{t} - \\sqrt{1- \\bar \\alpha_{t}}}{\\sqrt{\\bar \\alpha_{t}}}\\epsilon_{\\theta}(x_{t}, t)\\right)= \\frac{1}{\\alpha_{t}}\\left(x_{t} - \\frac{\\beta_{t}^2}{\\bar\\beta_{t}}\\epsilon_{\\theta}(x_{t},y, t)\\right) $$\nReverse Noise Predictor: $\\Sigma_{\\theta}(x_{t},t)$ It still remains to design $\\Sigma_{\\theta}(x_{t},t)$, since it encourages diversity.\nThe DDPM paper suggested not learning it, since it:\nresulted in unstable training and poorer sample quality\nBy fixing it at some value $\\Sigma_{\\theta}(x_{t},t) = \\sigma_{t}^{2}I$ , where either $\\sigma_{t}^{2} = \\beta_{t}$ or $\\tilde{\\beta_t}$ yielded similar performance.\nTraining \u0026amp; Defining Loss Conclusively, we have only defined one trainable model: $\\epsilon_{\\theta}(x_t, t)$\nTo reconstruct $x_{0}$ The training target can be MLE, the objective function being log-likelihood of reconstructing $x_{0}$:\n$$ \\begin{align*} \\ln p(x_{0}) \u0026= \\int{\\ln p(x_{0:T})dx_{1:T}} \u0026 \\text{marginalization of marginal distribution}\\\\\\\\ \u0026= \\ln \\int{p(x_{0:T})dx_{1:T}}\\\\\\\\\\\\ \u0026= \\ln \\mathbb{E}_{q(x_{1:T}|x_{0})}[\\int{\\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}}]\\\\\\\\ \u0026\\ge \\mathbb{E}_{q(x_{1:T}|x_{0})}\\left[\\ln \\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}\\right] \u0026 \\text{Jensen Inequality of $\\log$}\\\\\\\\ \u0026= \\underbrace{\\mathbb{E}_{q(x_{1}|x_{0})}\\left[\\ln p_{\\theta}({x_{0}|x_{1}})\\right]}_{\\text{reconstruction term}} - \\sum_{t = 2}^{T}\\mathbb{E}_{q(x_{t}|x_{0})}\\left[D_{KL}(q(x_{t-1}|x_{t},x_{0})||p_{\\theta}(x_{t-1}|x_{t}))\\right]\\\\\\\\ \u0026= \\sum\\limits_{t=1}^{T} \\gamma \\mathbb{E}_{q(x_{t}|x_{0})}[||\\epsilon_{t} - \\epsilon_{\\theta}(x_{t}, t) || ^{2}] \u0026 \\text{$\\gamma$ being some constants} \\end{align*} $$ To optimize the pixels We design the loss function of $\\theta$ as the Euclidean distance of the true and predicted mean of $x_{t-1}$: $$ \\begin{align*} \\ell \u0026amp;= ||x_{t-1} - \\hat x_{t-1}|| ^ 2 \\newline \u0026amp;= ||x_{t-1} - \\mu_\\theta(x_{t},t)|| ^ 2 \\newline \u0026amp;=|| (\\frac{1}{\\alpha_{t}}(x_{t} - \\beta_{t}\\epsilon_{t}) ) ^ {2} - \\frac{1}{\\alpha_{t}}(x_{t} - \\beta_{t}\\epsilon_{\\theta}(x_{t}, t)) ^ {2}||\\newline \u0026amp;= \\frac{\\beta_{t}^{2}}{\\alpha_{t}^{2}} ||\\epsilon_{t} - \\epsilon_{\\theta}(x_{t}, t) || ^2 \\end{align*} $$\nDDIM While the original DDPM is capable to generate satisfying images, it is known for poor performance: since the denoising(reverse) diffusion process usually take $T \\sim 1000$ times of noise-prediction.\nDDIM is proposed to boost the reverse process as Non-Markovian Process, by directly taking any model trained on DDPM and sampling only $T_{ddim}, T_{ddim} \\le T$ timestamps, with some timestamps skipped. As a side-effect, the quality is compromised a little.\nReverse Process It still takes the same approach as DDPM: predict $x_0$ from $x_t$ first.\nFrom Eq 4, we can see that the sampling/training do involves $x_t$, but doesn\u0026rsquo;t actually involves $p(x_{t-1}|x_{t})$ (which is defined in our reverse model). Instead, it defines:\n$$ q_\\sigma(x_{t-1}|x_{t}, x_0) = \\mathcal{N}(x_{t-1};\\sqrt{\\bar \\alpha_{t-1}}x_0 + \\sqrt{1 - \\bar \\alpha_{t-1} - \\sigma_t^2}\\frac{x_t - \\sqrt{\\bar \\alpha_t}x_0}{\\sqrt{1 - \\bar \\alpha_t}}, \\sigma_t^2I) $$\nHence, the relation between $x_{t-1}$ and $x_t$ is:\n$$ x_{t-1} = \\sqrt{\\alpha_{t-1}} \\underbrace{\\left( \\frac{x_t - \\sqrt{1 - \\alpha_{t}}\\epsilon_{\\theta}(x_{t},t)} {\\sqrt{\\bar \\alpha_{t}}} \\right)}_{\\text{predicted $x_0$}} + \\underbrace{\\sqrt{1 - \\bar \\alpha_{t-1} - \\sigma_{t}^{2}\\epsilon_{t}(x_{t},t)}}_{\\text{predicted noise}} + \\underbrace{\\sigma_{t} \\epsilon_{t}}_{\\text{random noise}} $$ where: $$ \\sigma_t = \\eta \\sqrt{(\\frac{1 - \\bar \\alpha_t}{1 - \\bar \\alpha_{t-1}}) \\left(1 - \\frac{\\bar \\alpha_t}{\\bar \\alpha_{t-1}}\\right)} $$ where $\\eta \\in (0,1)$ is a constant, indicating the level of random noise:\n$\\eta = 1$: The random noise is maximized, which is DDPM. $\\eta = 0$: The random noise is totally removed, making it a deterministic process/Implicit model, which is DDIM. It relies entirely on the predicted noise, while sacrificing some diversity with lowering random noise level. As for the timestamps chosen, they are determined empirically.\nTip\nIn fact, $\\eta$ represents the degree of moving some of the noise from predicted noise $\\epsilon_t$ to sampled noise $\\epsilon$: the bigger the $\\eta$, the less deterministic, the larger random noise will be introduced to the reverse process.\nShort Summary Conclusively, both models apply the same forward process, and have the same target: $x_{t} \\to x_{0}$, though they have differences in the reverse process:\nDDPM maximize the random noise, and in order to mitigate the negative effects it has, takes more timestamps in the reverse process DDIM boost the performance by only selecting some of the timestamps, and reduce the random noise level Conditioned Generation While being able to generate high quality images with reasonable speed with the models mentioned above, it is a common feature to generated conditioned output.\nGiven condition $y$, our goal is to derive $p(x_{t-1}|x_{t},y)$\nClassifier Guided Diffusion Using bayes\u0026rsquo; rule, we have: $$ p(x_{t-1}|x_{t},y) = \\frac{p(x_{t-1}|x_{t})p(y|x_{t-1},x_{t})}{p(y|x_{t})} $$\nUsing the notations in Reverse Process: $$ p(x_{t-1}|x_{t},y) \\propto \\exp(-||x_{t-1}-\\mu(x_{t})-\\Sigma_{t}^{2}\\underbrace{\\nabla_{x_{t}}\\log p(y|x_{t})}{\\text{classifier}}||^{2}/2\\Sigma{t}^{2}) $$\nSo $\\mu_{\\theta}(x_{t}, t,y) = \\mu(x_{t})+\\Sigma_{t}^{2}\\nabla_{x_{t}}\\log p(y|x_{t})), \\Sigma_{t} = \\sigma_{t}$\n[!NOTE]\nThe gradient of the prob is easy to get with autograd, if the classifier can output the prob The classifier guides the model only when inferencing Classifier-Free Diffusion To infer without a classifier, we need to blend the condition $y$ into training process.\nBy directly modeling the conditioned reverse process as $p(x_{t-1}|x_{t},y) = \\mathcal{N}(x_{t-1};\\mu(x_{t},y), \\sigma_{t}^{2}I)$, following the modeling of Eq. 11, we have: $$ \\mu(x_{t}, y) = \\frac{1}{\\alpha_{t}}\\left(x_{t} - \\frac{\\beta_{t}^2}{\\bar\\beta_{t}}\\epsilon_{\\theta}(x_{t},y, t)\\right) $$\nThe $\\epsilon_{\\theta}(x_{t},y, t)$ can be trained to predict the noise under condition.\n[!WARNING] The conditioned noise predictor depends on $y$, so retraining is required if the $y$ is changed\nScore-based generative models Notation Meaning Ancestral Sampling A sample method, auto-regressive Score Distillation Sampling A sampling method(sampler) to generate samples from a diffusion model by optimizing a loss function. Basically, it utilizes(distills) the score function of a teacher diffusion model, to train a larger model, with the final result as a sample (as $t \\to 0$). score function The gradient of the log-likelihood function $U(x) = -\\log q(x)$ An energy function. The lower the likelihood, the higher the energy $\\mu_{\\theta}(x_t, t)$ parameterized model to predict $x_{t-1}$ at time $t$ $p(x_{0:T})$ the joint distribution of $x_{0}, x_{1} \u0026hellip; x_{T}$ Langevin dynamic A Markov chain Monte Carlo(MCMC) method for obtraining random samples Fisher Divergence Most often, we don\u0026rsquo;t care about the probability $q(x)$ of a certain input $x$, but how it changes through time: therefore, we can utilize score function(gradient, changes) $s(x):=\\nabla_{x}\\log q(x)$\n[!TIP] It is also an advantage of modeling the score: don\u0026rsquo;t have to make sure probability sum up to 1\nWith $s(x)$ allowing us to sample from $q(x)$ using thermodynamics, our goals changes to: model $q(x)$\n$$ dx_{t} = \\nabla \\log q(x) d_{t} + d{W_{t}} $$\nLoss We learn a model $s_{\\theta}$ to match(approximate) the score $\\nabla \\log q$:\n$$ s_{\\theta} \\approx \\nabla \\log q(x) $$ \u0026ndash; This is score matching.\nTypically, score matching is formalized as minimizing Fisher divergence function . By expanding the integral, and performing an integration by parts, we have our loss function: $$ \\mathcal{L} = \\mathbb{E}{q}[||s{\\theta}(x) - \\nabla \\log q(x)||^{2}] $$ However, it\u0026rsquo;s infeasible since it requires access to unknown score $\\nabla \\log q(x)$.\nFortunately, we have score matching techniques(e.g. Hyvärinen scoring rule) which minimize the Fisher divergence without knowledge of the gorund-truth score: $$ \\mathcal{L} = \\mathbb{E}{q}\\left[\\nabla{x} s_{\\theta}(x)+ \\frac{1}{2}||s_{\\theta}(x)||_{2}^{2}\\right] $$\nSince $s_{\\theta}$ is modeled by ourself, its output and gradients can be easily calculated. We use Monte-Carlo methods with gradient descent to optimize it.\nSample / Inference But how do we generate a sample.\nOnce we have trained a score-based model $s_{\\theta}(x)$, we can use an iterative procedure called Langevin dynamics to draw samples from it: $$ x_{i + 1} \\leftarrow x_{i} + \\epsilon \\nabla_{x} \\log p(x) + \\sqrt{2\\epsilon} z_{i}, z_{i} \\sim \\mathcal{N}(0, I) $$ Notice some white noise is injected, to avoid all samples collapse into some limited local optimas.\nThis seems decent, but in fact: in low-density regions, the estimated scores are inaccurate.\nIt\u0026rsquo;s natural to augment the low-density regions by perturbing our datapoint: injecting $\\mathcal{N}$. It can solve the problem in low-density, however since the training data is perturbed, the generated samples are too.\nMultiple (decreasing) noise levels $\\sigma$ are applied as an input to score funcion $s$, with the output of previous model $i$ as the input of the next model $i+1$. The whole process resembles an Anneald Langevn Dynamics\nSDE Notation Meaning SDE A DE in which one or more of the terms is a stohastic process $\\mathcal{U}(T_{a},T_{b})$ Uniform distribution over the time interval $[T_{a}, T_{b}]$ In DDPM, we define $t$ as discrete timestamps, however it\u0026rsquo;s more natural to model it as continuous time. Forward Process With this premise, we model the forward process with Stochastic DE(Differential equation), but not funtion on timestamps: $$ dx = f_{t}(x) + g_{t}dw $$\nReverse Process Similarly, we want to model $p(x_{t}|x_{t + \\Delta{t}})$: $$ \\begin{align} p(x_{t}|x_{t + \\Delta{t}}) \u0026amp;= \\frac{p(x_{t + \\Delta_{t}} | x_{t})p(x_{t})}{p(x_{t+ \\Delta{t}})} \\\\ \u0026amp;= p(x_{t + \\Delta{t}} | x_{t})\\exp(\\log p(x_{t}) - \\log p(x_{t+\\Delta{t}})) \\\\ \u0026amp;\\propto \\left(-\\frac{||x_{t + \\Delta_{t}} - x_{t} - f_{t}(x_{t})\\Delta{t}||^{2}}{2g_{t}^{2}\\Delta t}+ \\log p(x_{t}) - \\log p(x_{t+\\Delta{t}})\\right) \\end{align} $$\nIn order to calculate the unknown diff, we apply Taylor expansion: $$ \\log p(x_{t+\\Delta{t}}) \\approx \\log p(x_{t}) + (x_{t+\\Delta t} - x_{t}) \\cdot \\nabla_{x_{t}}\\log p(x_{t}) + \\underbrace{\\Delta t \\frac{\\partial \\log p(x_{t})}{\\partial t}}{\\text{$x{t}$\u0026rsquo;s deritive of $t$}} $$\nUpdate Equation 26-3 with it, we have: $$ p(x_{t}|x_{t + \\Delta{t}}) \\sim \\mathcal{N}(f_{t+\\Delta t}(x_{t + \\Delta t}) - g_{t + \\Delta t}^{2}\\nabla_{x_{t + \\Delta t}} \\log p(x_{t + \\Delta t})\\Delta t; g_{t + \\Delta t}^{2}\\Delta t I) $$\nand the SDE of reverse process: $$ dx = [f_{t}(x) - g_{t}^{2}\\nabla_{x}\\log p_{t}(x)]dt + g_{t}dw $$\nTraining $$ \\mathcal{L} = \\mathbb{E}{t \\in \\mathcal{U}(0, T)}\\mathbb{E}{p_{t}(x)}[\\lambda(t)||\\nabla_{x}\\log p_{t}(x) - s_{\\theta}(x,t)||^{2}_{2}] $$ where:\n$\\lambda : \\mathbb{R} \\to \\mathbb{R}_{\u0026gt;0}$ is a positive weighting function Probability flow ODE Notation Meaning PF(Probability flow) ODE The ODE of an SDE Despite capable of generating high-quality samples, samplers based on Langevin MCMC and SDE solvers do not provide a way to compute the exact log-likelihood of score-based generative models.\nIt has been proved that, it is possible to convert any SDE into an ODE(ordinary differential equation) without changing its marginal distributions $p_{t}(x)$\nForward Process With a sequence of complex calculations(including F-P function \u0026amp; Dirac function), we have:\n$$ dx = [f(x,t) - \\frac{1}{2}(g^{2}(t)-\\sigma_{t}^{2})\\nabla_{x}\\log p_{t}(x)]dt $$\nReverse Process The reverse process of PF-ODE is given by:\n$$ dx = [f(x,t) - \\frac{1}{2}g^{2}(t)\\nabla_{x}\\log p_{t}(x)]dt $$\n[!TIP] When $\\nabla_{x}\\log p_{t}(x)$ replaces $s_{\\theta}(x,t)$, PF ODE becomes a special case of a neural ODE\nSamplers Euclidean $$ \\begin{equation}\\left.\\frac{d\\boldsymbol{x}t}{dt}\\right|{t=t_{n+1}}\\approx \\frac{\\boldsymbol{x}{t{n+1}} - \\boldsymbol{x}{t_n}}{t{n+1} - t_n}\\end{equation} $$ 一阶近似\nHeun solver DPM solver AMED solver ","permalink":"https://mickqian.github.io/posts/ai/models/denoising-diffusion-models/","summary":"Personal takeaways of DDIM/DDPM","title":"Denoising Diffusion Models"},{"content":"Terminology Notations Mean $X \\sim p_{r}$ the input data $z$ the encoded latent $\\theta$ the parameterized model $\\phi$ the encoder $p_{\\theta}(x)$ the likelihood of the data-reconstruction $p(z)$ the distribution of latent variable $z$ as a prior, often $\\mathcal{N}(0,1)$ $q_{\\phi}(z|x)$ variational distribution $q_{\\phi}(z|x)$ variational distribution MDL(Minimum Description Length) Self-Information $I$ the amount of information, interpreted as the level of \u0026ldquo;surprise\u0026rdquo;\n$$I(\\mathcal{w}{n}) = f(P(\\mathcal{w}{n})) = -\\log(P(\\mathcal{w}_{n})) \\ge 0$$ Entropy $H(X)$ the average amount of information in a message. A measure of uncertainty. $$H(X) = E[I(X)] = E[-\\ln(P(X))]$$\nBackground AutoEncoder is proposed to compress data and reduct dimensionality as a generalization of PCA, and largely used in signal processing, until someone found new samples can be generated by adding noise to latents and decoded by decoder.\nHowever, the ability of AutoEncoder to generate new samples by the distribution of the latents $z$, this is why \u0026amp; when Variational AutoEncoder is developed.\n[!TIP] AE is an approach of MDL\nRequirements In order to be able to generate new samples using decoder, we will be happy if $z \\sim \\mathcal{N}(0, 1)$ Modeling We apply Maximum Likelihood Estimation here.\nLog Likelihood is defined as: $$ Likelihood = \\log P_{\\theta}(X) $$\nwhich represents the ability of the model to reconstruct the input data.\nHence, from the definition of the loss function:\n$$ \\mathcal{L}(\\theta) = - \\mathbb E_{x \\sim data}[\\log p_{\\theta}(x)] $$\nNormally, the $x\\sim data$ is neglected.\nOur goal is to minimize the loss function, in the mean time force encoder to encode $X$ as $z \\sim \\mathcal{N}(\\mu, \\sigma^{2}I)$\nImplicit Model We define $z$ as an implicit variable, making our model an implicit model.\nRewrite the log-likelihood: $$ p_{\\theta}(x) = \\int{p_{\\theta}}(x|z)p_{\\theta }(z)dz $$ where $\\theta$ is the parameter of the implicit model (encoder and decoder).\nHowever there\u0026rsquo;s a common problem for implicit models: the integration relies on the exhaustion on implicit variable $z$.\nIn our case, as $z \\sim \\mathcal{N}(\\mu, \\sigma^{2}I)$, it is deem impossible.\nMC Monte-Carlo is a method to approximate an intractable equation(integration) by sampling a lot of data ($p_{\\theta}(x | z)$): $$ \\begin{align*} p_{\\theta}(x) \u0026amp;= \\int{p_{\\theta}}(x|z)p_{\\theta }(z)dz\\\\ \u0026amp;\\approx \\frac{1}{m} \\sum\\limits_{j =1}^{m} p_{\\theta}(x | z_{j}) \\end{align*} $$ But that does not enforce $z \\sim \\mathcal{N}(\\mu, \\sigma^{2}I)$.\nVariational Bayes Deriving ELBO Considering the log-likelihood can be rewritten in the following process:\n$$ \\begin{align*} \\log p_{\\theta}(x) \u0026amp;= \\log p_{\\theta}(x) \\int_{z}p_{\\phi}(z|x)dz \u0026amp;\\text{Normalization} \\\\ \u0026amp;= \\int_{z}p_{\\theta}(z|x)\\log p_{\\theta}(x)dz \\\\ \u0026amp;= \\int_{z}p_{\\theta}(z|x) \\log \\frac{p_{\\theta}(x,z)}{p(z|x)} dz \u0026amp;\\text{Bayes\u0026rsquo; Theorem} \\\\ \u0026amp;= \\int_{z}(p_{\\theta}(z|x)\\log p_{\\theta}(x,z) - p_{\\theta}(z|x)\\log p(z|x))dz \\\\ \u0026amp;= \\log p_{\\theta}(x,z) - \\log p_{\\theta}(z|x) \\end{align*} $$\nSince the posterior $\\log p_{\\theta}(z|x)$ is intractable (only involves integration on latent variable $z$, see Bayes\u0026rsquo; Theorem), a new distribution(which is easy to learn) $q_{\\phi}(z|x)$ is used to approximate it, where $\\phi$ is the encoder.\nLet\u0026rsquo;s continue by replacing:\n$$ \\begin{align*} \\underbrace{\\log p(x)}_{\\text{evidence}} \u0026= \\log p_\\theta(x,z) - \\log q_{\\phi}(z|x) \\newline \u0026= \\int_{z} q_{\\phi}(z|x)\\log\\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)}dz \\newline \u0026= \\int_{z}q_{\\phi}(z|x)\\log(\\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)} \\cdot \\frac{q_{\\phi}(z|x)}{p(z|x)})dz \\newline \u0026= \\int_{z}q_{\\phi}(z|x)\\log(\\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)})dz + \\int_{z}q_{\\phi}(z|x)\\log(\\frac{q_{\\phi}(z|x)}{p(z|x)})dz \\newline \u0026= \\mathcal L(\\theta,\\phi; x) + D_{KL}(q_{\\phi}, p_{\\theta}) \\newline \u0026\\ge \\underbrace{\\mathcal L(\\theta,\\phi; x)}_{\\text{ ELBO }} \u0026 \\text{$D_{KL}\\ge 0$} \\end{align*} $$ $\\mathcal L(\\theta, \\phi; x) = \\mathbb{E}_{z \\sim q(.|x)}{\\log \\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)}}$ is ELBO(Evidence Lower Bound), as it is the lower bound of evidence $\\mathcal{L}(\\theta)$, omitting the KL term. Maximizing ELBO is directly:\nmaximizing log-likelihood minimizing KL-Divergence of posterior $p_{\\theta}$ and variational distribution $q_{\\phi}$ Maximizing ELBO And we can break it down further: $$ \\begin{align*} \\underbrace{\\mathcal L(\\theta, \\phi; x)}_{\\text{ELBO}} \u0026= \\int_{z}q_{\\phi}(z|x)\\log(\\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)})dz = \\mathcal{H}[q_{\\phi}(z|x)] + \\mathbb{E}_{z}[p_{\\theta}(x,z)] \\\\\\\\ \u0026= \\int_{z}q_{\\phi}(z|x)\\log(\\frac{p(z) * p_{\\theta}(x|z) }{q_{\\phi}(z|x)})dz \u0026 \\text{Bayes' Theorem}\\\\\\\\ \u0026= \\int_{z}q_{\\phi}(z|x)\\log\\frac{p(z) }{q_{\\phi}(z|x)}dz + \\int_{z}q_{\\phi}(z|x)\\log p_{\\theta}(x|z)dz\\\\\\\\ \u0026= \\underbrace{-D_{KL}(q_{\\phi}(z|x), p(z))}_{\\text{$\\mathcal L_{reg}$}} + \\underbrace{\\mathbb E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]}_{\\text{$\\mathcal L_{reconstruct}$}}\\\\\\\\ \\end{align*} $$ [!Note] $\\int_{z}p(z)*f(z)dz = \\mathbb E_{z \\sim p(.)}[f(z)]$, which is the expectation of p with z sampled from $p(z)$\nThis is ELBO:\n$\\mathcal L_{reg}$: the KL-divergence of variational distribution and prior distribution $\\mathcal L_{reconstruct}$: the Expectation of log reconstruct-likelihood under variational distribution Since $\\mathcal{L}(\\theta) = -\\log p(x) \\le - \\text{ELBO}$, by maximizing ELBO, we can indirectly minimize $L(\\theta)$.\nHence, we define $\\mathcal{L} = -\\text{ELBO}$.\nTraining $$ \\begin{align*} \\text{ELBO} \u0026amp;= \\underbrace{-D_{KL}(q_{\\phi}(z|x), p(z))}{\\text{$\\mathcal L{reg}$}} + \\underbrace{\\mathbb E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]}{\\text{$\\mathcal L{reconstruct}$}}\\\\ \u0026amp;= \\underbrace{-D_{KL}(q_{\\phi}(z|x), p(z))}{\\text{$\\mathcal L{reg}$}} + MSE(x, \\hat x) \\end{align*} $$\nAs $z$ is sampled from $\\sim q_{\\phi}(z|x)$, which is a variational distribution, the gradient of ELBO will not be able to propagate back to encoder $\\phi$ (in-differentiable, chain rule).\nThus, re-parameterization is applied: $z = \\mu + \\epsilon \\times \\sigma, \\hat z \\sim \\mathcal{N}(0, I)$, where $\\phi(X) = (\\mu, \\epsilon)$. This way, the gradient is passed back to $\\phi$, by representing $z$ with the output of $\\phi$, where $z$ participates in the loss-calculation\nProblems Blurry output the prior: $p(z) \\sim \\mathcal{N}(0, I)$ MSE is used to measure $L_{reconstruct}$ DAE: corrupt X，降低图片的冗余度（图片的冗余性一般都很高）\nDall E 两阶段：\nclip 构造对比学习的正负样本对 文本 -\u0026gt; clip encoder -\u0026gt; text embedding -\u0026gt; (diffusion) prior -\u0026gt; image embedding -\u0026gt; diffusion model decoder -\u0026gt; image transformer encoder 本质上是自回归模型，可以基于自注意力和输入，自回归地生成同类型的内容\n![[Pasted image 20230618153050.png]]\n![[Pasted image 20230618154911.png]]\n","permalink":"https://mickqian.github.io/posts/ai/models/vae/","summary":"Takeaways from the maths of VAE","title":"VAE"},{"content":" Desafinado One Note Samba Só Danço Samba Doralice O Pato Chega De Saudade (No More Blues) Aguas De Março Menina Flor Corcovado ","permalink":"https://mickqian.github.io/posts/music/bossa-nova-songs-progression/","summary":"Chord progression of some well-known bossa-nova songs","title":"Bossa Nova Songs Progression"},{"content":" Single 1. Something 2. Here Comes the Sun 3. While My Guitar Gently Weeps 4. All Those Years Ago 5. Wah-Wah Album Beatles records are excluded\n1. All Things Must Pass 2. Brainwashed 3. George Harrison 4. Living in the Material World Guitar Solo 1. Dark Sweet Lady 2. Something 3. The Light That Has Lighted the World 4. Give me Love 5. Any Road 6. Cheer Down 7. Stuck Inside a Cloud 8. Crippled Inside 9. How Do You Sleep? 10. Isn\u0026rsquo;t it a Pity 11. Rising Sun ","permalink":"https://mickqian.github.io/posts/music/music-of-george-harrison/","summary":"personal rankings of George Harrison\u0026rsquo;s Singles/Albums/Guitar Solos","title":"Music of George Harrison"}]