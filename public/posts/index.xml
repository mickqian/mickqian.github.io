<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Mick&#39; Blog</title>
    <link>https://mickqian.github.io/posts/</link>
    <description>Recent content in Posts on Mick&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 10 Feb 2025 19:46:29 +0800</lastBuildDate><atom:link href="https://mickqian.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The most iconic album covers</title>
      <link>https://mickqian.github.io/posts/music/the-most-iconic-album-covers/</link>
      <pubDate>Mon, 10 Feb 2025 19:46:29 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/music/the-most-iconic-album-covers/</guid>
      <description>&lt;p&gt;编辑上一篇介绍 The Velvet Underground &amp;amp; Nico 的推文的过程，重新唤醒了我对 Cover Art 的喜爱。从大学开始模仿专辑封面，收集 CD 和 黑胶，再到为 Cover Art 制作 app，热爱从未减退。&lt;/p&gt;
&lt;p&gt;所以想趁这个机会，介绍（兼带补习）一下剩下的 most iconic album covers。在参考各大榜单后， The most iconic album covers of all time (本人精选版) 新鲜出炉：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Abbey Road - The Beatles&lt;/li&gt;
&lt;li&gt;The Dark Side of the Moon - Pink Floyd&lt;/li&gt;
&lt;li&gt;Nevermind - Nirvana&lt;/li&gt;
&lt;li&gt;Sgt. Pepper&amp;rsquo;s Lonely Hearts Club Band - The Beatles&lt;/li&gt;
&lt;li&gt;The Velvet Underground &amp;amp; Nico - The Velvet Underground&lt;/li&gt;
&lt;li&gt;Born in the U.S.A. - Bruce Springsteen&lt;/li&gt;
&lt;li&gt;Aladdin Sane - David Bowie&lt;/li&gt;
&lt;li&gt;Never Mind the Bollocks, Here&amp;rsquo;s the Sex Pistols - Sex Pistols&lt;/li&gt;
&lt;li&gt;Ramones - Ramones&lt;/li&gt;
&lt;li&gt;Horses - Patti Smith&lt;/li&gt;
&lt;li&gt;London Calling - The Clash&lt;/li&gt;
&lt;li&gt;Ready to Die - The Notorious B.I.G.&lt;/li&gt;
&lt;li&gt;Rumours - Fleetwood Mac&lt;/li&gt;
&lt;li&gt;Sticky Fingers - The Rolling Stones&lt;/li&gt;
&lt;li&gt;Illmatic - Nas&lt;/li&gt;
&lt;li&gt;To Pimp a Butterfly - Kendrick Lamar&lt;/li&gt;
&lt;li&gt;Straight Outta Compton - N.W.A&lt;/li&gt;
&lt;li&gt;Elvis Presley - Elvis Presley&lt;/li&gt;
&lt;li&gt;Led Zeppelin IV - Led Zeppelin&lt;/li&gt;
&lt;li&gt;The Rise and Fall of Ziggy Stardust and the Spiders From Mars - David Bowie&lt;/li&gt;
&lt;li&gt;Unknown Pleasures - Joy Division&lt;/li&gt;
&lt;li&gt;Bitches Brew - Miles Davis&lt;/li&gt;
&lt;li&gt;Purple Rain - Prince&lt;/li&gt;
&lt;li&gt;Back to Black - Amy Winehouse&lt;/li&gt;
&lt;li&gt;Is This It - The Strokes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;数据来源：&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>编辑上一篇介绍 The Velvet Underground &amp; Nico 的推文的过程，重新唤醒了我对 Cover Art 的喜爱。从大学开始模仿专辑封面，收集 CD 和 黑胶，再到为 Cover Art 制作 app，热爱从未减退。</p>
<p>所以想趁这个机会，介绍（兼带补习）一下剩下的 most iconic album covers。在参考各大榜单后， The most iconic album covers of all time (本人精选版) 新鲜出炉：</p>
<ol>
<li>Abbey Road - The Beatles</li>
<li>The Dark Side of the Moon - Pink Floyd</li>
<li>Nevermind - Nirvana</li>
<li>Sgt. Pepper&rsquo;s Lonely Hearts Club Band - The Beatles</li>
<li>The Velvet Underground &amp; Nico - The Velvet Underground</li>
<li>Born in the U.S.A. - Bruce Springsteen</li>
<li>Aladdin Sane - David Bowie</li>
<li>Never Mind the Bollocks, Here&rsquo;s the Sex Pistols - Sex Pistols</li>
<li>Ramones - Ramones</li>
<li>Horses - Patti Smith</li>
<li>London Calling - The Clash</li>
<li>Ready to Die - The Notorious B.I.G.</li>
<li>Rumours - Fleetwood Mac</li>
<li>Sticky Fingers - The Rolling Stones</li>
<li>Illmatic - Nas</li>
<li>To Pimp a Butterfly - Kendrick Lamar</li>
<li>Straight Outta Compton - N.W.A</li>
<li>Elvis Presley - Elvis Presley</li>
<li>Led Zeppelin IV - Led Zeppelin</li>
<li>The Rise and Fall of Ziggy Stardust and the Spiders From Mars - David Bowie</li>
<li>Unknown Pleasures - Joy Division</li>
<li>Bitches Brew - Miles Davis</li>
<li>Purple Rain - Prince</li>
<li>Back to Black - Amy Winehouse</li>
<li>Is This It - The Strokes</li>
</ol>
<p>数据来源：</p>
<ol>
<li>The 100 Best Album Covers of All Time - Billboard</li>
<li>The 25 Most Iconic Album Covers Of All Time - uDiscoverMusic</li>
<li>The 100 Best Album Covers of All Time - Rolling Stones</li>
<li>The top 20 most recognisable album covers - currys</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>Cover Art Sticky Fingers</title>
      <link>https://mickqian.github.io/posts/music/cover-art-sticky-fingers/</link>
      <pubDate>Sun, 02 Feb 2025 13:34:49 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/music/cover-art-sticky-fingers/</guid>
      <description>&lt;iframe allow=&#34;autoplay *; encrypted-media *;&#34; frameborder=&#34;0&#34; height=&#34;450&#34; style=&#34;width:100%;max-width:660px;overflow:hidden;background:transparent;&#34; sandbox=&#34;allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation&#34; src=&#34;https://embed.music.apple.com/us/album/sticky-fingers-2015-remaster/1440812661&#34;&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;创作背景&#34;&gt;创作背景&lt;/h2&gt;
&lt;p&gt;1971 年，在乐队前核心主创 Brian Jones 悲剧般的去世的两年后，The Rolling Stones 发行了他们的第九张录音室专辑。Andy Warhol 为这张专辑构思并拍摄了原版黑胶封面，Craig Braun 则负责了封面包装的设计。&lt;/p&gt;
&lt;p&gt;他们的合作始于 1967年，《The VU &amp;amp; Nico》专辑封面的大获成功让 Braun 从一个印刷工成功跻身上流社会。&lt;/p&gt;
&lt;p&gt;Mick Jagger(乐队核心人物，歌手)和 Andy Warhol 有着不错的私交。在一个 party 上，他们在讨论下一张专辑的封面设计时，Andy 说：&amp;ldquo;在封面上放一个蓝色牛仔裤的拉链不是很有趣吗？&amp;rdquo;, Mick 回答：“是啊，那听起来是一个伟大的主意，男人”。同年晚些时候，Mick 写信告知 Warhol 可以继续这个项目。在这之前，Craig Braun 所属的公司已经对专辑封面有了一些构思，但这之后乐队唱片公司的总裁 Marshall 把这个想法告诉了 Braun，后者随即开始为这个构思重新设计&lt;/p&gt;
&lt;p&gt;![[image-4.png]]&lt;/p&gt;
&lt;h2 id=&#34;封面设计&#34;&gt;封面设计&lt;/h2&gt;
&lt;p&gt;原版专辑封面展示了一个身着牛仔裤的男性的胯部特写。&lt;/p&gt;
&lt;p&gt;文字方面，在封面左上侧，对应牛仔裤的右侧口袋，有红色橡皮图章样式的 “The Rolling Stones&amp;quot; 的字样。在它下方，画面左侧，对应右大腿位置，有同样样式的专辑名称 “Sticky Fingers&amp;quot; 字样，它们所使用的字体为 Kabel Black&lt;/p&gt;
&lt;p&gt;回到牛仔裤主体本身，与 《The VU &amp;amp; Nico》的设计相似，牛仔裤拉链被做成了可操作（working）的形式
![[image-6.png]]&lt;/p&gt;
&lt;p&gt;打开封面后，可以看到子封面：内裤一个身着白色内裤的男性胯部。内裤上似乎用金色橡皮图章印有美国波普艺术家 Andy Warhol 的风格化名字，在其正下方则写着版权声明： &amp;ldquo;THIS PHOTOGRAPH MAY NOT BE—ETC.&amp;rdquo;&lt;/p&gt;</description>
      <content:encoded><![CDATA[<iframe allow="autoplay *; encrypted-media *;" frameborder="0" height="450" style="width:100%;max-width:660px;overflow:hidden;background:transparent;" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation" src="https://embed.music.apple.com/us/album/sticky-fingers-2015-remaster/1440812661"></iframe>
<h2 id="创作背景">创作背景</h2>
<p>1971 年，在乐队前核心主创 Brian Jones 悲剧般的去世的两年后，The Rolling Stones 发行了他们的第九张录音室专辑。Andy Warhol 为这张专辑构思并拍摄了原版黑胶封面，Craig Braun 则负责了封面包装的设计。</p>
<p>他们的合作始于 1967年，《The VU &amp; Nico》专辑封面的大获成功让 Braun 从一个印刷工成功跻身上流社会。</p>
<p>Mick Jagger(乐队核心人物，歌手)和 Andy Warhol 有着不错的私交。在一个 party 上，他们在讨论下一张专辑的封面设计时，Andy 说：&ldquo;在封面上放一个蓝色牛仔裤的拉链不是很有趣吗？&rdquo;, Mick 回答：“是啊，那听起来是一个伟大的主意，男人”。同年晚些时候，Mick 写信告知 Warhol 可以继续这个项目。在这之前，Craig Braun 所属的公司已经对专辑封面有了一些构思，但这之后乐队唱片公司的总裁 Marshall 把这个想法告诉了 Braun，后者随即开始为这个构思重新设计</p>
<p>![[image-4.png]]</p>
<h2 id="封面设计">封面设计</h2>
<p>原版专辑封面展示了一个身着牛仔裤的男性的胯部特写。</p>
<p>文字方面，在封面左上侧，对应牛仔裤的右侧口袋，有红色橡皮图章样式的 “The Rolling Stones&quot; 的字样。在它下方，画面左侧，对应右大腿位置，有同样样式的专辑名称 “Sticky Fingers&quot; 字样，它们所使用的字体为 Kabel Black</p>
<p>回到牛仔裤主体本身，与 《The VU &amp; Nico》的设计相似，牛仔裤拉链被做成了可操作（working）的形式
![[image-6.png]]</p>
<p>打开封面后，可以看到子封面：内裤一个身着白色内裤的男性胯部。内裤上似乎用金色橡皮图章印有美国波普艺术家 Andy Warhol 的风格化名字，在其正下方则写着版权声明： &ldquo;THIS PHOTOGRAPH MAY NOT BE—ETC.&rdquo;</p>
<p>![[image-5.png]]</p>
<p>由于实体拉链的十分坚固，导致第一批专辑在运输时，部分专辑的 side two 的 Sister Morphine 曲目被损坏。Craig 通过将拉头拉至拉链的中间规避了这个问题</p>
<h2 id="不同版本-variations">不同版本 Variations</h2>
<ul>
<li>在美国发行的初版黑胶唱片中，专辑标题和乐队名称的字样更小，而且位于封面顶部</li>
<li>在英国发行的版本中，相比美国版，字样更大且位于封面左侧</li>
<li>希腊曾经发行过一个只带有印刷拉链的版本</li>
</ul>
<h2 id="争议--影响">争议 &amp; 影响</h2>
<ul>
<li>关于模特：尽管乐队粉丝普遍推测专辑封面上的模特是乐队主唱 Mick Jagger, 但参与拍摄的人员却声称，Warhol 拍摄了几名男性，不包括 Mick。封面上的人究竟是谁，这个问题至今仍不得而知。有趣的是，外部的牛仔裤和内部的内裤可能属于不同的主人</li>
<li>在西班牙，因为被认为“淫秽“，专辑封面被换成了 &ldquo;Can of fingers&rdquo; cover</li>
<li>![[image.png]]</li>
<li>在 1992 年于俄罗斯发行的版本中，字样使用了西里尔字母，牛仔裤上的皮带孔被换成了苏联军队的锤子和星星样式。此外，模特似乎被换成了女性</li>
<li>“[The Rolling Stones] knew if they put jeans and a working zipper that people were going to want to see what was back there,” Braun said.</li>
</ul>
<h2 id="其他">其他</h2>
<ul>
<li>除了专辑封面外，专辑内页标志性的 The Rolling Stones tongue logo 也是第一次首次出现，这是 Andy 的酷儿组织 The Factory 的一个门徒 John Pasch 的作品</li>
<li>![[image-1.png]]</li>
</ul>
<p>所以， CASETiFY 什么时候出这版专辑的手机壳？</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Velvet Underground &amp; Nico 专辑封面背后的故事</title>
      <link>https://mickqian.github.io/posts/music/the-vu--nico%E4%B8%93%E8%BE%91%E5%B0%81%E9%9D%A2%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B/</link>
      <pubDate>Sun, 02 Feb 2025 13:34:49 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/music/the-vu--nico%E4%B8%93%E8%BE%91%E5%B0%81%E9%9D%A2%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B/</guid>
      <description>&lt;iframe allow=&#34;autoplay *; encrypted-media *;&#34; frameborder=&#34;0&#34; height=&#34;450&#34; style=&#34;width:100%;max-width:660px;overflow:hidden;background:transparent;&#34; sandbox=&#34;allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation&#34; src=&#34;https://embed.music.apple.com/cn/album/the-velvet-underground-nico-45th-anniversary-edition/1440851613&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;借此 CASETiFY 推出 Andy Warhol x CASETiFY 联名系列之际，我重温了经典专辑 《The VU &amp;amp; Nico》的封面创作历史，希望这篇介绍能为读者了解这个作品提供一些参考&lt;/p&gt;
&lt;h2 id=&#34;创作背景&#34;&gt;创作背景&lt;/h2&gt;
&lt;p&gt;1966 年, 在 Andy Warhol 的安排下，The Velvet Underground 开始和 Nico 录制他们的首张专辑。Warhol 把自己的 香蕉画作 送给乐队作为专辑封面。该专辑于 1967 年正式发布。&lt;/p&gt;
&lt;p&gt;专辑灵感可能来源于这种廉价的三角形烟灰缸：&lt;/p&gt;
&lt;p&gt;















  
  
      
      
  &lt;figure align=center class=&#34;figure d-block text-center&#34;&gt;
  &lt;picture align=center &gt;
  &lt;img class=&#34;figure-img img-fluid&#34; src=&#34;https://mickqian.github.io/Attachments/Music/Cover%20Art%20Sticky%20Fingers/IMG-20250211004214244.png?v=48aa1c842049b4ae8dae25dfa903988e&#34; alt=&#34;|410x364&#34; loading=&#34;lazy&#34; height=&#34;413px&#34; width=&#34;465px&#34; /&gt;
&lt;/picture&gt;

  &lt;figcaption class=&#34;figure-caption&#34;&gt;&lt;p&gt;|410x364&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;封面设计&#34;&gt;封面设计&lt;/h2&gt;
&lt;p&gt;















  
  
      
      
  &lt;figure align=center class=&#34;figure d-block text-center&#34;&gt;
  &lt;picture align=center &gt;
  &lt;img class=&#34;figure-img img-fluid&#34; src=&#34;https://mickqian.github.io/Attachments/Music/Cover%20Art%20Sticky%20Fingers/IMG-20250211004214291.png?v=48aa1c842049b4ae8dae25dfa903988e&#34; alt=&#34;The Velvet Underground &amp;amp; Nico|252x250&#34; loading=&#34;lazy&#34; height=&#34;1420px&#34; width=&#34;1431px&#34; /&gt;
&lt;/picture&gt;

  &lt;figcaption class=&#34;figure-caption&#34;&gt;&lt;p&gt;The Velvet Underground &amp;amp; Nico|252x250&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;原专辑&#34;&gt;原专辑&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;主体部分为一根成熟的黄色香蕉，使用丝网印刷工艺进行印刷&lt;/li&gt;
&lt;li&gt;底部为 &amp;ldquo;Andy Warhol&amp;rdquo; 的签名印章，字体采用 &lt;a href=&#34;https://fontsinuse.com/typefaces/3897/coronet&#34;&gt;&lt;strong&gt;Coronet&lt;/strong&gt;&lt;/a&gt; Bold，因印刷工艺导致轻微扭曲失真&lt;/li&gt;
&lt;li&gt;顶部香蕉的右上方有一个指向香蕉顶部的箭头和一行小字：&amp;ldquo;PEEL SLOWLY AND SEE&amp;rdquo;，邀请唱片所有者“慢慢剥开并查看”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在早期发行的版本中，香蕉皮部分是一张可移除的贴纸。从顶部撕开后，会露出粉色的香蕉果肉。这部分材料由 Craig Braun 研发，他凭借可移除粘合剂的研发经验被 MGM 唱片公司邀请，协助 Andy Warhol 开发可移除香蕉皮部分。由于制作这样的材料需要特殊的机器，乐队也对“Perfect Peel&amp;quot; 非常执着，专辑被推迟发行&lt;/p&gt;</description>
      <content:encoded><![CDATA[  <iframe allow="autoplay *; encrypted-media *;" frameborder="0" height="450" style="width:100%;max-width:660px;overflow:hidden;background:transparent;" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation" src="https://embed.music.apple.com/cn/album/the-velvet-underground-nico-45th-anniversary-edition/1440851613"></iframe>
<p>借此 CASETiFY 推出 Andy Warhol x CASETiFY 联名系列之际，我重温了经典专辑 《The VU &amp; Nico》的封面创作历史，希望这篇介绍能为读者了解这个作品提供一些参考</p>
<h2 id="创作背景">创作背景</h2>
<p>1966 年, 在 Andy Warhol 的安排下，The Velvet Underground 开始和 Nico 录制他们的首张专辑。Warhol 把自己的 香蕉画作 送给乐队作为专辑封面。该专辑于 1967 年正式发布。</p>
<p>专辑灵感可能来源于这种廉价的三角形烟灰缸：</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/Music/Cover%20Art%20Sticky%20Fingers/IMG-20250211004214244.png?v=48aa1c842049b4ae8dae25dfa903988e" alt="|410x364" loading="lazy" height="413px" width="465px" />
</picture>

  <figcaption class="figure-caption"><p>|410x364</p></figcaption>
</figure>
</p>
<h2 id="封面设计">封面设计</h2>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/Music/Cover%20Art%20Sticky%20Fingers/IMG-20250211004214291.png?v=48aa1c842049b4ae8dae25dfa903988e" alt="The Velvet Underground &amp; Nico|252x250" loading="lazy" height="1420px" width="1431px" />
</picture>

  <figcaption class="figure-caption"><p>The Velvet Underground &amp; Nico|252x250</p></figcaption>
</figure>
</p>
<h3 id="原专辑">原专辑</h3>
<ul>
<li>主体部分为一根成熟的黄色香蕉，使用丝网印刷工艺进行印刷</li>
<li>底部为 &ldquo;Andy Warhol&rdquo; 的签名印章，字体采用 <a href="https://fontsinuse.com/typefaces/3897/coronet"><strong>Coronet</strong></a> Bold，因印刷工艺导致轻微扭曲失真</li>
<li>顶部香蕉的右上方有一个指向香蕉顶部的箭头和一行小字：&ldquo;PEEL SLOWLY AND SEE&rdquo;，邀请唱片所有者“慢慢剥开并查看”</li>
</ul>
<p>在早期发行的版本中，香蕉皮部分是一张可移除的贴纸。从顶部撕开后，会露出粉色的香蕉果肉。这部分材料由 Craig Braun 研发，他凭借可移除粘合剂的研发经验被 MGM 唱片公司邀请，协助 Andy Warhol 开发可移除香蕉皮部分。由于制作这样的材料需要特殊的机器，乐队也对“Perfect Peel&quot; 非常执着，专辑被推迟发行</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/Music/Cover%20Art%20Sticky%20Fingers/IMG-20250211004214390.png?v=48aa1c842049b4ae8dae25dfa903988e" alt="|527x260" loading="lazy" height="1036px" width="2098px" />
</picture>

  <figcaption class="figure-caption"><p>|527x260</p></figcaption>
</figure>
</p>
<p>单声道版本和立体声版本存在细微差别。相较于立体声版本，单声道版本的香蕉的位置较高</p>
<h3 id="andy-warhol-banana-peel-phone-case">Andy Warhol Banana &ldquo;Peel&rdquo; Phone Case</h3>
<p>Casetify 的手机壳在保留原封面主要元素的基础上，针对手机尺寸进行了适当调整。</p>
<p>















  
  
      
      
  <picture align=center  class="d-block text-center">
  <img class="img-fluid" src="https://mickqian.github.io/Attachments/Music/Cover%20Art%20Sticky%20Fingers/IMG-20250211004214499.png?v=48aa1c842049b4ae8dae25dfa903988e" alt="" loading="lazy" height="1200px" width="2100px" />
</picture>
</p>
<ul>
<li>主体部分，为适应手机的宽度，对香蕉形状和角度进行了一些调整</li>
<li>原版的底部印章被移至左上方，新字体未知，但和原版封面的专辑名所使用的字体较为接近</li>
</ul>
<p>与原版专辑一致，香蕉皮部分的也是粘性贴纸，可以撕开并贴回。在未撕开时，贴纸部分光滑平整。撕开过的贴纸部分会产生细微的横向条纹，呈现更粗糙的手感</p>
<h2 id="文化影响">文化影响</h2>
<ul>
<li>Warhol 的传记作者 Gopnik 将香蕉皮与 foreskin 相比较，并写道这个封面 “将 Velvets 乐队与 The Factory 所代表的硬核酷儿文化联系在了一起” (<strong>The Factory</strong> 是 Andy Warhol 创建的 queer 文化组织)</li>
<li>Lou Reed: &ldquo;The banana actually made it into an erotic art show.&rdquo; 卢 里德：这个香蕉实际上把它（专辑封面）变成了一个情色艺术作品</li>
</ul>
<p>值得一提的是，这并非 Andy Warhol 创作的唯一“情色艺术作品”。在 1969 年他为 The Rolling Stones（滚石乐队）的专辑 《Sticky Fingers》创作的封面上，有一名穿着牛仔裤的男子躯干特写，其左侧的隆起清晰可见。封面上设计了一个真实的拉链，可以拉下来露出内衣。</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/Music/Cover%20Art%20Sticky%20Fingers/IMG-20250211004214599.jpg?v=48aa1c842049b4ae8dae25dfa903988e" alt="Sticky Fingers" loading="lazy" height="1500px" width="1500px" />
</picture>

  <figcaption class="figure-caption"><p>Sticky Fingers</p></figcaption>
</figure>
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>(The Making Of) Immusia</title>
      <link>https://mickqian.github.io/posts/misc/the-making-of-immusia/</link>
      <pubDate>Sat, 20 Jul 2024 20:16:13 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/misc/the-making-of-immusia/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Immusia&lt;/strong&gt;（读作 &lt;strong&gt;ɪˈmuːziə&lt;/strong&gt;, 是拉丁语 &lt;strong&gt;Immersio&lt;/strong&gt; (沉浸) 和 &lt;strong&gt;Musica&lt;/strong&gt; (音乐) 的结合) 是我的第一个 Vision Pro App, 也是我的第一个 VR App&lt;/p&gt;
&lt;p&gt;本文记录了我的创作理念，和一些实现细节&lt;/p&gt;
&lt;h2 id=&#34;缘由&#34;&gt;缘由&lt;/h2&gt;
&lt;p&gt;之所以想做这样的一个 app, 是因为我由于科幻作品的影响，产生的对 VR 设备的想象。更具体一点，是 Black Mirror。Black Mirror 擅长探讨 高科技对 人文/伦理/社会 的冲击，选材大胆，对我有着比较深的影响。在其最精彩的前几集，多次出现类似 头戴设备的概念，给了观影者极大的想象空间。&lt;/p&gt;
&lt;h2 id=&#34;核心功能&#34;&gt;核心功能&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Immusia&lt;/strong&gt;  的核心理念始终是 沉浸式 的音乐聆听体验。借助 “概念专辑” 这类已经被大众认可的丰富精神财产，音乐被赋予了更具像化的概念，从而使音乐可视化有了更合理的依据&lt;/p&gt;
&lt;h3 id=&#34;2d-资产---3d-资产&#34;&gt;2D 资产 -&amp;gt; 3D 资产&lt;/h3&gt;
&lt;p&gt;有大量平面资源可以转为双目3D，动机很自然(Vision Pro 的相册 app 也在后来内置了这个功能）， 相关技术也随处可寻，但是视觉效果还是相当可观&lt;/p&gt;
&lt;h3 id=&#34;3d-player&#34;&gt;3D Player&lt;/h3&gt;
&lt;p&gt;为了充分发挥 VR 设备的空间优势，我对传统播放器做了以下改动：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有歌词文本都为具有深度的 3D 字体。尽管 Apple 不推荐这么做，我还是坚持使用它，原因是经过实测， depth 似乎赋予了歌词一种真实感&lt;/li&gt;
&lt;li&gt;歌词的移动方向不再局限于 y 方向，而是可以结合 XY 和 Z。我最喜欢的移动方式，被我称为 &amp;ldquo;Water Fall&amp;rdquo;， 歌词会向 -Y 和 +Z 方向移动&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;immersive-player&#34;&gt;Immersive Player&lt;/h3&gt;
&lt;h3 id=&#34;window-mode&#34;&gt;Window Mode&lt;/h3&gt;
&lt;p&gt;由于 Swift API 的限制，app之间的 沉浸式空间是互斥的，因此纯沉浸式的 app 存在一些局限性。为了达到一个通用音乐播放器的最低要求，我加入了窗口模式，这样用户就可以在使用其他 app 时同时使用 Immusia&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p><strong>Immusia</strong>（读作 <strong>ɪˈmuːziə</strong>, 是拉丁语 <strong>Immersio</strong> (沉浸) 和 <strong>Musica</strong> (音乐) 的结合) 是我的第一个 Vision Pro App, 也是我的第一个 VR App</p>
<p>本文记录了我的创作理念，和一些实现细节</p>
<h2 id="缘由">缘由</h2>
<p>之所以想做这样的一个 app, 是因为我由于科幻作品的影响，产生的对 VR 设备的想象。更具体一点，是 Black Mirror。Black Mirror 擅长探讨 高科技对 人文/伦理/社会 的冲击，选材大胆，对我有着比较深的影响。在其最精彩的前几集，多次出现类似 头戴设备的概念，给了观影者极大的想象空间。</p>
<h2 id="核心功能">核心功能</h2>
<p><strong>Immusia</strong>  的核心理念始终是 沉浸式 的音乐聆听体验。借助 “概念专辑” 这类已经被大众认可的丰富精神财产，音乐被赋予了更具像化的概念，从而使音乐可视化有了更合理的依据</p>
<h3 id="2d-资产---3d-资产">2D 资产 -&gt; 3D 资产</h3>
<p>有大量平面资源可以转为双目3D，动机很自然(Vision Pro 的相册 app 也在后来内置了这个功能）， 相关技术也随处可寻，但是视觉效果还是相当可观</p>
<h3 id="3d-player">3D Player</h3>
<p>为了充分发挥 VR 设备的空间优势，我对传统播放器做了以下改动：</p>
<ul>
<li>所有歌词文本都为具有深度的 3D 字体。尽管 Apple 不推荐这么做，我还是坚持使用它，原因是经过实测， depth 似乎赋予了歌词一种真实感</li>
<li>歌词的移动方向不再局限于 y 方向，而是可以结合 XY 和 Z。我最喜欢的移动方式，被我称为 &ldquo;Water Fall&rdquo;， 歌词会向 -Y 和 +Z 方向移动</li>
</ul>
<h3 id="immersive-player">Immersive Player</h3>
<h3 id="window-mode">Window Mode</h3>
<p>由于 Swift API 的限制，app之间的 沉浸式空间是互斥的，因此纯沉浸式的 app 存在一些局限性。为了达到一个通用音乐播放器的最低要求，我加入了窗口模式，这样用户就可以在使用其他 app 时同时使用 Immusia</p>
<p>窗口模式的大部分 UI 都和系统原生 Apple Music 保持一致。为了更方便查看艺术家信息和专辑信息，在左右两侧分别添加了小窗口。同时，沉浸模式下的 3D 播放器 也被保留。</p>
<p>值得一提的是，为了体现空间感，我没有采用平铺的专辑列表，而是实现了一种非常类似 Apple 在 IPod 上曾经使用过的 Cover Flow 效果。它足够优雅和美观，也不会占用太多资源。</p>
<h2 id="environments-环境">Environments 环境</h2>
<p>在 Environment 方面，我早就感到和知晓 RealityKit 的局限性和 Performance 问题，Metal(<strong>CompositorService</strong>, to be specific) 是最好的选择。然而我没有图形学方面的储备，所以花了一段时间尝试寻找合适的人选（外包团队）进行合作。显然我高估了国内在这一领域的人才储备，总之经过一段时间的尝试并无果之后，我就开始自学 Metal。在 Shadertoy和一些电子教材 的帮助之下，我得以对图形学入门，并创作了一些环境（场景）</p>
<h3 id="interstellar">Interstellar</h3>
<p><strong>Interstellar</strong> 是我搭建的第一个环境。在听太空主题的音乐时，脑海里一直有一些关于太空场景的想象，而且静态的太空场景也相对比较容易实现，对当时对图形编程不太熟悉的我来说是一个不错的上手项目</p>
<p>起初我对 <strong>Interstellar</strong> 的构想比较简单：一个孤单的蓝色星球</p>
<p>但是随着进度不断推进，我的想法越来越多，技术也逐渐成熟，因此我大胆地加入了不同元素：月球/太阳/星星/星云 等</p>
<p>在这个过程中也遇到了很多问题，包括但不限于：</p>
<ul>
<li>ColorSpace 的选取</li>
<li>透明场景下 BlendMode 和 Alpha 的设置</li>
<li>大量复杂 Shader 造成的计算问题，通过简化逻辑和烘焙纹理解决</li>
<li>CompareFunction 和 Winding 的不一致造成的遮挡错误问题</li>
<li>&hellip;</li>
</ul>
<p>所幸在 GPT 的帮助下，没有浪费我 <em>太多</em> 的时间</p>
<p>这个场景中的很多元素都是我从 <a href="https://www.shadertoy.com">ShaderToy</a> 移植而来，希望 License 不会有很大的问题</p>
<p>在背景音乐的选择方面，此前选择的是现成的 太空背景音乐音效，现在正在考虑 realtime GPU Sound</p>
<p>一些不错的生成结果：</p>
<center>
<iframe width="640" height="360" frameborder="0" src="https://www.shadertoy.com/embed/4cfcDX?gui=true&t=10&paused=true&muted=false" allowfullscreen></iframe>
</center>
<h3 id="plastic-beach">Plastic Beach</h3>
<h3 id="star-gate">Star Gate</h3>
<p>一次和朋友闲聊，偶然听他提起正在为新歌制作一段类似 <strong>Daft Punk</strong> 的《Contact》 末尾的太空音效。我去 Youtube 上找到了一个粉丝为这首歌制作的一版 MV, 里面恰好选取了 《2001: A Space Odyssey》 中主人公 <strong>David Bowman</strong> 穿越时空隧道（<strong>Star Gate</strong>) 的片段，这一段视觉效果和音乐的节奏有着非常好的配合</p>
<p>我很喜欢这个场景，于是它成为了我的第二个 Environment idea。</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/lhdAxlvKklw?si=w04dsfXkvZyNufbL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen class="center-iframe"></iframe>
<h2 id="一些想法">一些想法</h2>
<ul>
<li>一个 VR app 涉及的技术栈太多：前端/后端/设计/GPU 渲染/UX 只是我能想起来的几个，而且互联网相对较丰富的也只是平面资源，在空间的视角下，每一部分都可以衍生出新的学问。Vision Pro 用于作为先行 Demo 的 Encounter Dinosaurs, 在 WWDC 上有一个专门的 Episode, 介绍其中的 UX 设计。这是一个全新的领域</li>
<li>Marketing: 我还没有进行任何形式的 Marketing</li>
<li>相关人才的欠缺：国内外在 VR 技术上的 技术和人才累积 似乎根本不在一个水平面上，国内的 App，即使是由大团队诸如 QQ 音乐/ 爱奇艺等，仍显粗糙；然而国外的小团队（最少1个人）都可以做出足够摘取 Apple Design Awards 的 App。这应该与当地电影工业的发展有着最直接的联系</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>My Gears</title>
      <link>https://mickqian.github.io/posts/music/my-gears/</link>
      <pubDate>Sat, 20 Jul 2024 19:53:50 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/music/my-gears/</guid>
      <description>A (brief) introduction of my gears</description>
      <content:encoded><![CDATA[<h2 id="rocky">Rocky</h2>
<p><a href="https://www.fender.com/en-US/george-harrison-rocky-stratocaster.html"><strong>Rocky</strong></a> 是 The Beatles 吉他手 <strong>George Harrison</strong> 的电吉他</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/BmU4eSv0zXM?si=kmdp2USeukhhFwSr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen class="center-iframe"></iframe>
<br>
<p>George Harrison 本人曾在多个场合使用它：</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/Music/My%20Gears/IMG-20240720203521507_scaled.jpg?v=48aa1c842049b4ae8dae25dfa903988e#center" alt="George Harrison Performing With Rocky in 《Im a Walrus》" loading="lazy" height="380px" width="480px" />
</picture>

  <figcaption class="figure-caption"><p>George Harrison Performing With Rocky in 《Im a Walrus》</p></figcaption>
</figure>
</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/Music/My%20Gears/IMG-20240720205140321_scaled.png?v=48aa1c842049b4ae8dae25dfa903988e#center" alt="George Harrison With Rocky" loading="lazy" height="271px" width="400px" />
</picture>

  <figcaption class="figure-caption"><p>George Harrison With Rocky</p></figcaption>
</figure>
</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/Music/My%20Gears/IMG-20240720205008915_scaled.png?v=48aa1c842049b4ae8dae25dfa903988e#center" alt="George Harrison Playing Rock in Abbey Road Studio" loading="lazy" height="620px" width="500px" />
</picture>

  <figcaption class="figure-caption"><p>George Harrison Playing Rock in Abbey Road Studio</p></figcaption>
</figure>
</p>
<p>关于它的制作过程：</p>
<blockquote>
<p>Stratocaster® guitars were almost impossible to find in England in the late 1950s and early 1960s, so when George Harrison actually found one in a shop during the pre-fame early days of the Beatles, he meant to get it but was scooped by the guitarist for Rory Storm and the Hurricanes (whose drummer went by the stage name Ringo Starr).</p>
<p>A few dizzying years later, when the sessions for Beatles <em>Help!</em> album got under way in early 1965, Harrison had better luck - he and John Lennon sent roadie Mal Evans out to get one for each of them, and Evans soon returned with a matching pair of Sonic Blue Strat® guitars. Harrison&rsquo;s guitar, serial number 83840, still bore a decal from a music store where it was purchased at one point - &ldquo;Grimwoods; The music people; Maidstone and Whitstable&rdquo;. Thus, <em>Help!</em> marks the first appearance of a Stratocaster in Beatles music; heard in the low drone throughout that album&rsquo;s &ldquo;Ticket to Ride&rdquo; and in the solo for &ldquo;You&rsquo;re Going to Lose That Girl.&rdquo; Near the end of 1965, both Strats were put to even more prominent use on groundbreaking album <em>Rubber Soul</em>, most notably on the ringing chordal solo in &ldquo;Nowhere Man&rdquo;, and again on mid-1966&rsquo;s <em>Revolver</em>.</p>
<p>In 1967, sometime between the end of the <em>Sgt. Pepper&rsquo;s Lonely Hearts Club Band</em> sessions and the June 25 live worldwide telecast of &ldquo;All You Need is Love&rdquo;, Harrison took up paint and brush himself to give his Stratocaster a multicolored psychedelic dayglo paint job. It also appeared prominently in the &ldquo;I Am the Walrus&rdquo; segment of 1967&rsquo;s <em>Magical Mystery Tour</em> film. The guitar remained a favorite of Harrison&rsquo;s for the rest of the decade, and by December 1969 Harrison had painted &ldquo;Bebopalula&rdquo; on the upper body, &ldquo;Go Cat Go&rdquo; on the pickguard and &ldquo;Rocky&rdquo; - the guitar&rsquo;s nickname - on the headstock.</p>
<p>&mdash; 摘取自 <a href="https://www.fendercustomshop.com/series/limited-edition/limited-edition-george-harrison-rocky-strat/"># LIMITED EDITION GEORGE HARRISON ROCKY STRAT®</a></p>
</blockquote>
<p>其中提到，Rocky 是由一把 61&rsquo; Stratocaster, 并且是由 George Harrison 本人改造而来。</p>
<p>下面这段材料揭露了关于改装的细节（主要是颜料方面）：</p>
<blockquote>
<p>“During ’67, everybody started painting everything,” Harrison says, “and I decided to paint it. I got some <strong>Day-Glo</strong> paint, which was quite a new invention in them days, and just sat up late one night and did it.” (Harrison points out that some of his ex-wife Patti Boyd’s nail polish was used to paint the headstock.)</p>
<p>The guitar made appearances that year in the Beatles’ live performance of “All You Need Is Love” on Our World, the first global satellite TV program, and in the film <em>Magical Mystery Tour</em>, in the segment where the Beatles mime to “I Am the Walrus,”</p>
</blockquote>
<iframe width="560" height="315" src="https://www.youtube.com/embed/O7_XuXP0AaU?si=Jq-AZPG4fAT9VNdT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen  class="center-iframe"></iframe>
<br>
<p>Rocky 使用的主要颜料是 Day-Glo (<em>Day-Glo Color Corp</em>. 生产的一种荧光涂料), 而琴颈部位则是用他当时妻子 <strong>Patti Boyd</strong> 的指甲油涂绘而成</p>
<p>我本人非常喜欢 Rocky 的配色，奈何负担不起 Rocky Custom Shop 的高昂费用，也由于墨产 Rocky Player Series 的低性价比而未选择入手</p>
<p>恰巧我有一把 <a href="https://www.fender.com/en-US/electric-guitars/stratocaster/vintera-ii-60s-stratocaster/0149020302.html">Fender Vintera 60&rsquo;s Stratocaster</a>, 从年代和琴型上都与原版相对接近。于是我把它送到一位网友那里，完成 Rocky 的复刻</p>
<p>在改造前，它原本是日落色的，大概这样：</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickqian.github.io/Attachments/Music/My%20Gears/IMG-20240720210023358_scaled.jpg?v=48aa1c842049b4ae8dae25dfa903988e" alt="日落色" loading="lazy" height="194px" width="600px" />
</picture>

  <figcaption class="figure-caption"><p>日落色</p></figcaption>
</figure>
</p>
<p>在使用 硝基漆 和一些荧光涂料改造之后：</p>
<p>















  
  
      
      
  <picture align=center  class="d-block text-center">
  <img class="img-fluid" src="https://mickqian.github.io/Attachments/Music/My%20Gears/IMG-20240721170354370.jpg?v=48aa1c842049b4ae8dae25dfa903988e" alt="" loading="lazy" height="800px" width="600px" />
</picture>
</p>
<p>















  
  
      
      
  <picture align=center  class="d-block text-center">
  <img class="img-fluid" src="https://mickqian.github.io/Attachments/Music/My%20Gears/IMG-20240721165617205.jpg?v=48aa1c842049b4ae8dae25dfa903988e" alt="" loading="lazy" height="667px" width="500px" />
</picture>
</p>
<br>
<p>















  
  
      
      
  <picture align=center  class="d-block text-center">
  <img class="img-fluid" src="https://mickqian.github.io/Attachments/Music/My%20Gears/IMG-20240721171902613.jpg?v=48aa1c842049b4ae8dae25dfa903988e" alt="" loading="lazy" height="667px" width="500px" />
</picture>

可以看到还原度很高，但同时保留了一些原创部分。喷绘的效果还是挺令我满意的</p>
<p>我带着它参加了一些小型演出</p>
<h2 id="fender-blues-tweed-deluxe">Fender Blues Tweed Deluxe</h2>
<p><a href="https://www.fender.com/en-US/guitar-amplifiers/vintage-pro-tube/blues-deluxe-reissue/2232200000.html">Blues Deluxe™</a> 是 Fender 的一款晶体管音箱，我选择它有两个原因：</p>
<ol>
<li>音色：我对吉他音箱研究不多，但是 Fender 音箱的清音不用多说，很悦耳</li>
<li>外观：这款复古气息的粗花呢黄色音箱本身就是一款装饰品</li>
</ol>
<p>奈何功率太大，我很少使用它，最多拍视频的时候才想起来用一下</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Tuning</title>
      <link>https://mickqian.github.io/posts/ai/rl/peft/</link>
      <pubDate>Wed, 24 Jan 2024 19:19:32 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/ai/rl/peft/</guid>
      <description>PEFTs</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Term</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Full fine-tuning</td>
          <td style="text-align: left">Fine-Tune all the weights of a pretrained model</td>
      </tr>
      <tr>
          <td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Intrinsic_dimension">Intrinsic dimension</a></td>
          <td style="text-align: left">An attribute of a dataset, essentially the minimum variable needed to encode the data</td>
      </tr>
      <tr>
          <td style="text-align: left">low intrinsic dimension</td>
          <td style="text-align: left">A description of a dataset, describing that the intrinsic dimension of the dataset is low</td>
      </tr>
      <tr>
          <td style="text-align: left">$h$</td>
          <td style="text-align: left">The output of the model</td>
      </tr>
  </tbody>
</table>
<h2 id="introduction">Introduction</h2>
<p><strong>PEFT</strong>(Parameter Efficient Fine-Tuning) is a technique to reduce the training cost of full fine-tuning by minimize the
parameter count and the computation complexity.</p>
<p>According to <a href="/posts/ai/rl/peft/#unipelt">UniPELT</a>, existing PELT usually involves following variants:</p>
<ul>
<li>The functional form of $\Delta h$</li>
<li>The form of insertion into Transformer
<ul>
<li>Parallel: At <strong>input</strong> layer</li>
<li>Sequential: At <strong>output</strong> layer</li>
</ul>
</li>
<li>The representation modifies
<ul>
<li>attention layer</li>
<li>ffn llayer</li>
</ul>
</li>
<li>Composition function of $h$ and $\Delta h$</li>
</ul>
<h2 id="adapter-tuning">Adapter Tuning</h2>
<p>Only fine-tune the parameters of the layers close to downstream tasks.</p>
<p>While training, the parameter of the original pre-train model is frozen, with a newly-added adapter structure:</p>
<ol>
<li>Down-project layer: project the high-dim feature to lower dimension</li>
<li>Non-linear</li>
<li>Up-project layer: project back to high-dim</li>
<li>Skip-connection: $identity$ in the worst case</li>
</ol>
<h2 id="prefix-tuning">Prefix Tuning</h2>
<ul>
<li>Prefix: Prepend learnable task-related <strong>virtual tokens</strong> to input tokens at $W_{k} &amp; W_{v}$ of <strong>each layer</strong></li>
<li>An MLP after prefix layer(only in training): down-project a smaller prefix $P_{\theta}^{&rsquo;}$ to actual prefix$P_
{\theta}$, to stablize the training</li>
</ul>
<blockquote>
<p>[! NOTE]
Similar to <em>text prompt</em>, but <em>continuous</em> and <em>implicit</em></p>
</blockquote>
<h2 id="prompt-tuning">Prompt Tuning</h2>
<p>A simplified version of <a href="/posts/ai/rl/peft/#prefix-tuning">Prefix Tuning</a>, with:</p>
<ul>
<li>Prefix virtual tokens prepended only at input layer</li>
<li>MLP removed.</li>
</ul>
<h2 id="p-tuning">P-Tuning</h2>
<p>Notice the problem of LLM: The expression of the prompt has a significant impact on downstream tasks</p>
<p><strong>P-Tuning</strong> is proposed to change the <em>input</em> Prompt to learnable embedding.</p>
<h2 id="lora">LoRA</h2>
<p>All of the PEFT methods mentioned above has some problems:</p>
<ul>
<li>either: increase the model depth and inference time, e.g.<a href="/posts/ai/rl/peft/#adapter-tuning">Adapter Tuning</a></li>
<li>or: with learnable parameters which are hard to train</li>
</ul>
<p>It is observed that <strong>low intrinsic dimension</strong> is the key part of LLMs. Based on this observation, the attention matrix
can be re-designed as:
$$
h = \underbrace{W_{0}}<em>{\text{original weight}}x + \underbrace{\Delta W}</em>{\text{Adapte}}x = W_{0}x + BAx
$$
where:</p>
<ul>
<li>$A \in \mathbb{R}^{d \times r} \sim \mathcal{N}(0, \sigma^{2})$</li>
<li>$B \in \mathbb{R}^{r \times d}$</li>
<li>$d &gt; r$</li>
</ul>
<p>Advantages being:</p>
<ul>
<li>No additional depth introduced</li>
</ul>
<h2 id="unipelt">UniPELT</h2>
<p><strong>UniPELT</strong> provides a unified view of existing PEFTs, and compares each choices of variants:</p>
<ul>
<li>Parallel insertion form is bettern than Sequantial</li>
<li>Modified representation:
<ul>
<li>When the amout of parameter modified is huge, ffn is better
<ul>
<li>ffn is task-related</li>
</ul>
</li>
<li>Otherwise Attention
<ul>
<li>attention captures the text pattern</li>
</ul>
</li>
</ul>
</li>
<li>Scaling composition function is better</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Reinforcement Learning</title>
      <link>https://mickqian.github.io/posts/ai/rl/reinforcement-learning/</link>
      <pubDate>Tue, 16 Jan 2024 21:31:43 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/ai/rl/reinforcement-learning/</guid>
      <description>Personal takeaways of RL/RLHF/DPO</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<p>General:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Term</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Reinforcement Learning</td>
          <td style="text-align: left">A branch/paradigm of machine learning, concerned with how an intelligent agent behaves in a dynamic environment.</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>BLUE</strong>(bilingual evaluation understudy)</td>
          <td style="text-align: left">An algorithm for evaluating the quality of text which has been machine-translated from one natural language to another</td>
      </tr>
      <tr>
          <td style="text-align: left">Reward Model(Actor model)</td>
          <td style="text-align: left">A model aligned with human feedback, predicting the reward of given actions</td>
      </tr>
      <tr>
          <td style="text-align: left">$G_{t}$</td>
          <td style="text-align: left">Return(aka the future reward), total sum of <strong>discounted</strong> rewards after time $t$:  $G_{t} = {\sum}^{\infty}_{k = 0}\gamma^{k}R_{t + k + 1}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$V_{\pi}(s)$</td>
          <td style="text-align: left">State-value function, measures the expected return of state $s$: $V(s) = \mathbb{E}_{\pi}[G_{t}\vert S_{t} = s]$ under $\pi$</td>
      </tr>
      <tr>
          <td style="text-align: left">$Q_{\pi}(s,a)$</td>
          <td style="text-align: left">Action-value function, measures the expected return of action $a$ under state $s$: $Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_{t} \vert S_{t} = s, A_{t} = a]$ under $\pi$</td>
      </tr>
      <tr>
          <td style="text-align: left">Bellman Equations</td>
          <td style="text-align: left">A set of equations that decompose the value function into <strong>immediate reward</strong> + <strong>discounted future values</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">$A_{q}$</td>
          <td style="text-align: left">the action to update $Q$</td>
      </tr>
      <tr>
          <td style="text-align: left">$A_{t+1}$</td>
          <td style="text-align: left">the actual taken action</td>
      </tr>
  </tbody>
</table>
<p>In RL Algorithms (mostly adjectives):</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Term</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Model-based</td>
          <td style="text-align: left">Algorithms of RL relying on a (environment dynamic) model, which defines $P(s&rsquo;\vert s,a), R(s,a)$</td>
      </tr>
      <tr>
          <td style="text-align: left">Model-free</td>
          <td style="text-align: left">Algorithms of RL learning by the interaction of the model with environment</td>
      </tr>
      <tr>
          <td style="text-align: left">Policy-Based(Policy Gradient) Methods</td>
          <td style="text-align: left">A branch of RL: quantize each action as <strong>PDF</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">Value-Based Methods</td>
          <td style="text-align: left">A branch of RL: quantize each action as value(PMF ? )</td>
      </tr>
      <tr>
          <td style="text-align: left">Current policy</td>
          <td style="text-align: left">The policy(actions) actually taken by an agent in an episode</td>
      </tr>
      <tr>
          <td style="text-align: left">On-policy</td>
          <td style="text-align: left">Using the action in current(actually exploited/taken) policy to update $V$</td>
      </tr>
      <tr>
          <td style="text-align: left">Off-policy</td>
          <td style="text-align: left">Using an action not from current policy to update $V$</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
  </tbody>
</table>
<h2 id="introduction">Introduction</h2>
<h2 id="value-based">Value-based</h2>
<h3 id="dynamic-programming">Dynamic Programming</h3>
<p>We can use Dynamic Programming to iteratively update and query value functions ($V_{\pi}$), with the help of Bellman
equations, <strong>when the model is fully known</strong>.</p>
<h3 id="monte-carlo">Monte-Carlo</h3>
<p>#model_free</p>
<p>Instead of modeling the environment, <strong>MC methods</strong> learns from <strong>episodes of raw experience</strong>, approximating the
observed mean return as expected return.</p>
<p>To optimally learn in <strong>MC</strong>, we take following steps:</p>
<ol>
<li>Improve the policy greedily: $\pi(s) = \underset{a}{argmax}Q(s, a)$</li>
<li>Generate a new episode with the combination of the new policy $\pi$ and randomness(e.g. $\epsilon$-greedy), balancing
between exploitation and exploration</li>
<li>Estimate $Q$ with the generated episode $\pi$</li>
</ol>
<h3 id="temporal-difference-methods">Temporal Difference methods</h3>
<p>#model-free</p>
<blockquote>
<p>[!NOTE]
TD learning can learn from <strong>incomplete</strong> episodes</p>
</blockquote>
<h4 id="bootstrapping">Bootstrapping</h4>
<p><strong>Estimate</strong> the rewards, rather than exclusively carrying out the episode.</p>
<h4 id="value-estimation">Value Estimation</h4>
<p>The estimated Value funciont $V$ is updated towards an estimated return $R_{t+1} + \gamma V(S_{t+1})$</p>
<h4 id="sarsa">SARSA</h4>
<p>#on-policy</p>
<blockquote>
<p>[!TIP]
Define $A_{q}$ as the action to update $Q$</p>
</blockquote>
<p>State-Action-Reward-State-Action
In each $t$:</p>
<ol>
<li>Choose $A_{t} = \underset{a \in A}{argmax}{Q(S_{t}, a)}$ with $\epsilon$-greedy</li>
<li>Obtain $R_{t + 1}$</li>
<li>Set $A_{t+1} \sim \pi(\cdot|s) = A_{q}$, under <strong>current policy</strong></li>
<li>Update $Q$ with the <strong>advantage of actual $A_{t+1}$ over expected reward</strong>:</li>
</ol>


$$
Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha(\underbrace{R_{t+1} + \gamma Q(S_{t + 1}, A_{t + 1})}_{\text{value  of current  policy, on-policy}} - \underbrace{Q(S_{t},A_{t})}_{\text{expected  value}})
$$


<ol start="5">
<li>$t = t + 1$</li>
</ol>
<blockquote>
<p>[!NOTICE]
$A_{q} == A_{t + 1}$, making it on-policy</p>
</blockquote>
<h4 id="q-learning">Q-Learning</h4>
<p>#off-policy</p>
<p>Q-learning is an off-policy method, with the steps in one episodes ($t, S_{t}$) being:</p>
<ol>
<li>Choose $A_{t} = \underset{a \in A}{argmax}Q(S_{t}, a)$ with $\epsilon$-greedy</li>
<li>Obtain $R_{t + 1}$</li>
<li>Set $A_{t+1} \sim \pi(\cdot|s)$, $A_{q} = \underset{a \in A}{\max} Q(S_{t + 1}, a)$</li>
<li>Update $Q$ with the <strong>advantage of optimal $A_{t + 1}$ over expected reward</strong>:<br>


   $$Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha(\underbrace{R_{t+1} + \gamma \underset{a \in A}{\max} Q(S_{t + 1}, a)}_{\text{best  value  after $A_{t}$, off-policy}} - \underbrace{Q(S_{t}, A_{t})}_{\text{expected  value}})
   $$

   </li>
<li>$t = t + 1$</li>
</ol>
<blockquote>
<p>[!NOTICE]
$A_{q} = \underset{a \in A}{\max} Q(S_{t + 1}, a) \ne A_{t + 1}$, making it off-policy</p>
</blockquote>
<h4 id="dqn">DQN</h4>
<p>#off-policy</p>
<p>Deep Q-Network, An improvement of <strong>Q-Learning</strong>:</p>
<ul>
<li><strong>Experience Replay</strong>: All the episode steps $e_{t} = (S_{t}, A_{t}, R_{t}, S_{t+1})$ are stored in one replay memory
$D_{t} = {e_{1}, &hellip;, e_{t}}$. During Q-learning updates, samples are drawn at random from the replay memory and thus
one sample could be used multiple times.</li>
<li><strong>Periodically Updated Target</strong>: Q is optimized towards target values that are only <strong>periodically</strong> updated(not
updated in each iteration anymore). The Q network is cloned and kept frozen as the optimization target every <strong>C</strong>
steps (C is a hyperparameter).</li>
</ul>
<blockquote>
<p>[!WARNING]
Known for overestimating value function $Q$</p>
</blockquote>
<h2 id="policy-gradient">Policy Gradient</h2>


$$
\begin{align*}
J(\theta) = \underset{s \in S}{\sum\limits} d^{\pi}(s)V^{\pi}(s) = \underset{s \in S}{\sum\limits} d^{\pi} \underset{a
\in A}{\sum\limits} \pi_{\theta}(a|s)Q^{\pi}(s,a)
\end{align*}
$$


<h3 id="actor-critic">Actor-Critic</h3>
<p>Actor-Critic learns the <strong>value function</strong> in addition to the policy, assisting the policy update.</p>
<p>It consists of two models:</p>
<ul>
<li><strong>Actor</strong> updates the policy $\theta$ of $\pi_\theta(a|s)$, suggested by critic</li>
<li><strong>Critic</strong> updates the value estimation function $Q(a|s) | V_{w}(s)$</li>
</ul>
<p>The main process being, for $t \in (1, T)$:</p>
<ol>
<li>Sample $a \sim \pi_{\theta}(a|s), r_{t} \sim R(s,a), s&rsquo; \sim P(s&rsquo;|s,a)$, next action $a&rsquo; \sim \pi_{\theta}(a&rsquo;|s&rsquo;)$</li>
<li>Update <strong>Actor</strong> $\theta$:
$$
\theta \leftarrow \theta + \alpha_{\theta} Q_{w}(s,a)\nabla_{\theta} ln \pi_{\theta}(
a|s)</li>
</ol>
<p>$$</p>
<p>to maximize the reward
5. Compute the correction (TD error, measures the quality of current policy $a&rsquo;$):


$$
\delta_{t} = \underbrace{r_{t} + \gamma Q_{w}(s', a')}_{\text{Action-Value of a'}} - \underbrace{Q_{w}(s,a)}_{\text{actual reward}}
$$


6. Update <strong>Critic</strong>: $w \leftarrow w + \alpha_{w}\delta_{t}\nabla_{w}Q_{w}(s,a)$ to reduce estimate error (ideally,
$\delta_{t} \leftarrow 0$, as $a&rsquo; \sim \pi_{\theta}(a&rsquo;|s&rsquo;)$)
7. Update $a \leftarrow a&rsquo;, s \leftarrow s'$</p>
<blockquote>
<p>[!TIP]
Adversarial training, resembles GAN: (generator, discriminator)</p>
</blockquote>
<h3 id="a2c">A2C</h3>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Model</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Actor $\pi_{\theta}$</td>
          <td style="text-align: left">The target model</td>
      </tr>
      <tr>
          <td style="text-align: left">Critic</td>
          <td style="text-align: left">Estimate $V$</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
  </tbody>
</table>
<p>$$
L(\theta) = -\log \pi_{\theta}(a_{t}|s_{t})\hat A_{t}
$$</p>
<p>where:</p>
<ul>
<li>$\hat A$: advantage function, the advantage of $a_{t}$ compared with average, normally $V$</li>
</ul>
<blockquote>
<p>[!WARNING]
This objective function can lead to massive change to policy</p>
</blockquote>
<h3 id="a3c">A3C</h3>
<p><strong>Asynchronous Advantage Actor-Critic</strong> focuses on parallel training. Multiple actors are trained in parallel and get
synced with global parameters.</p>
<h3 id="dpg">DPG</h3>
<p>#model-free #off-policy</p>
<p><strong>Deterministic Policy Gradient</strong> models the policy as deterministic function $a = \mu(s)$.</p>
<p>It is trained by maximizing the objective function: the expected discounted reward:</p>
<p>$$
J(\theta) = \int_{S}\rho^{\mu}(s)Q(s, \mu_{\theta}(s))ds
$$</p>
<p>where:</p>
<ul>
<li>$\rho^{\mu}(s&rsquo;)$: discounted state distribution</li>
<li>$\mu$: the deterministic action predictor</li>
</ul>
<h3 id="ddpg">DDPG</h3>
<p>#model-free #off-policy</p>
<p><strong>Deep Deterministic Policy Gradient</strong></p>
<p>Combining <a href="/posts/ai/rl/reinforcement-learning/#dqn">DQN</a> (experience replay, freezing target model) and <a href="/posts/ai/rl/reinforcement-learning/#dpg">DPG</a></p>
<p>Key design:</p>
<ul>
<li>Better exploration: $\mu’(s) = \mu_{\theta}(s) + \mathcal{N}$, adding noise $\mathcal{N}$ to policy</li>
<li>Soft updates: Moving average of parameter $\theta$</li>
</ul>
<h3 id="td3">TD3</h3>
<p><strong>Twin Delayed Deep Deterministic</strong> applied tricks on <a href="/posts/ai/rl/reinforcement-learning/#ddpg">DDPG</a> to prevent overestimating value function:</p>
<ol>
<li>Clipped Double Q-learning: Action selection and Q-value estimation are made by two networks separately.</li>
<li>Delayed update of target the policy network: Instead of updating actor and critic in one iteration, <strong>TD3</strong> updates
the <strong>actor</strong> at a lower frequency than <strong>critic</strong>, waiting for it to become stable. It helps reducing the variance.</li>
<li>Target policy smoothing: Introduce a smoothing regularization strategy by adding $\epsilon \sim clip(\mathcal{N}(0,
\sigma), -c , +c)$ to the value function $Q_{w}(s&rsquo;, \mu_{\theta}(s&rsquo;) + \epsilon))$. It mitigates the risk of
deterministic policies overfitting the value function.</li>
</ol>
<h3 id="sac">SAC</h3>
<p><strong>Soft Actor-Critic</strong> learns a more random policy by incorporating the entropy of the policy $H(\pi)$ into the reward.</p>
<p>Three key components:</p>
<ul>
<li>An actor-critic architecture</li>
<li>An off-policy approach</li>
<li>Entropy Maximization to encourage exploration</li>
</ul>
<p>The policy is trained by maximizing the objective function: expected return + the entropy


$$
J(\theta) = \sum\limits_{t = 1}^athbb{E}_{s_{t},a_{t} \sim \rho_{\pi_
{\theta}}} [r(s_{t},a_{t}) + \alpha \mathcal{H}(\pi_{\theta}(* | s_{t}))]
$$

</p>
<h3 id="ppo-proximal-policy-optimization">PPO (Proximal Policy Optimization)</h3>
<p>#on-policy</p>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>clipped objective</li>
<li><strong>Proximal</strong> stands for <strong>Reward Model</strong></li>
</ul>
</blockquote>
<p>As a successor of <a href="/posts/ai/rl/reinforcement-learning/#a2c">A2C</a>, PPO defines 2 more models:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Model</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Reward $r_{\theta}$</td>
          <td style="text-align: left">Calculate $R$</td>
      </tr>
      <tr>
          <td style="text-align: left">Reference $\pi_{ref}$</td>
          <td style="text-align: left">Apply constraint and guidance to <em>Actor</em></td>
      </tr>
      <tr>
          <td style="text-align: left">$r^{\ast}$</td>
          <td style="text-align: left">Ground-truth reward function</td>
      </tr>
      <tr>
          <td style="text-align: left">$r_\phi$</td>
          <td style="text-align: left">MLE of $r^{\ast}$</td>
      </tr>
  </tbody>
</table>
<p>$$
L(\theta) = \underbrace{-\hat A_{t} \cdot min(r_{t}(\theta), clip(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon))}<em>{\text{A2C loss, $\le 1$ + $\epsilon$}}  -  \underbrace{\beta D</em>{KL}(\pi_{\theta}(y|x)||\pi_{ref}(y|x))}_{\text{penalty of being too distant to normal response}}
$$</p>
<p>where:</p>
<ul>
<li>$r_{t}(\theta) = \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}$: the ratio of new policy to old policy</li>
<li>$\epsilon$: normally 0.1 or 0.2</li>
</ul>
<ul>
<li>Generate two outputs from same input $x$: $y_{1}, y_{2}$
<ul>
<li>Objective: $\mathcal{L} = \underset{\pi_{\theta}}{\max}\mathbb{E}[r_{\theta}(x,y_{2})]$
<ul>
<li>Update:
<ul>
<li>Optimize with the reward of current batch</li>
<li>TRO(<strong>Trust Region Optimization</strong>): using <strong>gradient constraint</strong> to make sure the update process doesn&rsquo;t sabotage the stability of learning process.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>$r$ and $\pi$ can be optimized iteratively.</li>
<li>RLHF and PPO is difficult to train.</li>
</ul>
</blockquote>
<h3 id="dpodirect-preference-optimization">DPO(Direct Preference Optimization)</h3>
<blockquote>
<p>[!NOTE] The major difference
<strong>Direct</strong>: directly optimize with reward, rather than $V | Q$: <strong>expected</strong> rewards from a reward model</p>
</blockquote>
<p>Rewrite objective:</p>
<p>$$
\begin{align*}
\pi
&amp;= \underset{\pi}{\max}(r_{\phi}(x,y) - \beta D_{KL}(\pi_{\theta}(y|x)||\pi_{ref}(y|x)))\\
&amp;= \underset{\pi}{\max}(r_{\phi}(x,y) - \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})\\
&amp;= \underset{\pi}{\min}( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)} - r_{\phi}(x,y)/\beta)\\
&amp;= \underset{\pi}{\min}( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x) e^{r_{\phi}(x,y)/\beta} })
\end{align*}
$$</p>
<p>^0639d4</p>
<p>Define partition function: $Z = \Sigma_{y}{\pi_{ref}(y|x) e^{r_{\theta}(x,y)/\beta}}$, which relates to the reward of $\theta$ over $ref$</p>
<p>We can get the optimal strategy $\pi^{\ast}$ under $r_{\phi}$(irrelevant of $\theta$):</p>
<p>$$
\pi^{*}(y|x)  = \pi_{ref}(y|x)e^{\frac{r_{\phi} (x,y)}{\beta}} \frac{1}{Z(x)}
$$</p>
<p>^5ee375</p>
<p>Then Eq [[#^0639d4]] became:</p>
<p>$$
\begin{align*}
\pi
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x) e^{r_{\phi}(x,y)}{\beta}}\right)\\
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi^{\ast}(y|x) Z(x)}\right)\\
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi^{\ast}(y|x)}\right)\\
&amp;= \underset{\pi}{\min}\left( D_{KL}(\pi_{\theta}(y|x) || \pi^{\ast}(y|x))\right)
\end{align*}
$$</p>
<p>Apparently, the optimal $\pi$ is: $\pi_{\theta} \to \pi^{*}$.</p>
<p>Noticing that the reward function of E.Q. [[#^5ee375]] can be rewritten(reparameterized) as(where $\pi_{ref}$ is the human-preference data as ground-truth):</p>
<p>$$
r_{\phi} (x,y) = \beta \log \frac{\pi^{\ast}(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)
$$</p>
<blockquote>
<p>[!TIP] the reward function can be represted with best policy trained under it</p>
</blockquote>
<p>By replacing $r_{\phi} (x,y)$ in the objective of RLHF as $\pi^{*}$, we get an objective function without the <strong>reward function</strong>:</p>


$$
\begin{align}
\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{ref}) = -{{\mathbb{E}_{(x, y_{w}, y_{l}) \sim D}[\log \sigma{({\beta \frac{\pi_{\theta}(y_{w}|x)}{\pi_{ref}(y_{w}|x)} - \beta\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)} }})]}}
\end{align}
$$


<p>From this equation, we found that: <strong>Training the reward model in RLHF is equivalent to training $\pi_{\theta}$ with the derived objective function</strong>.</p>
<p>That is to say, no need of 4 models, we can achieve the same target of RLHF with directly training $\pi_{\theta}$.</p>
<h2 id="methods">Methods</h2>
<h3 id="rlhf">RLHF</h3>
<p><strong>RLHF(Reinforcement learning from human feedback)</strong> is a technique that trains a <strong>reward model</strong>.</p>
<p>It has following key concepts:</p>
<ul>
<li><strong>Reward Model</strong>: trained in advance directly from human feedback</li>
<li>human feedback: data collected by asking humans to <strong>rank</strong> instances of the agent&rsquo;s behavior</li>
</ul>
<p>The procedure is given by 3 steps</p>
<h4 id="1-sft">1. SFT</h4>
<p>Pre-train a (target) model: $\pi^{SFT}$</p>
<h4 id="2-reward-modeling-phase">2. Reward Modeling Phase</h4>
<p>Train a reward model: $r_{\phi}(x,y) = r, r \in (0, + \infty)$, where $r$ is the reward of the given input.</p>
<ul>
<li>
<p>Initialization: Often initialized from Pretrained Models</p>
</li>
<li>
<p>Data:</p>
<ul>
<li>$D$:  $Prompt: x \to (Generation: y, Reward: r)$, generated by human or models</li>
<li>Human Feedback: <strong>Ranking</strong> the outputs of different models under same prompt with $r$
<ul>
<li>effective ways of ranking: Comparing two/ ELO</li>
</ul>
</li>
<li>$(y_{win}, y_{loss})$ : sampled from generation</li>
</ul>
</li>
<li>
<p>Train the RM with Data
The Objective is (negative log-likelihood loss):

  
  $$
  \begin{align*}
  \mathcal{L}_{R}(r_{\phi}, D) = -{{\mathbb{E}_{(x, y_{w}, y_{l}) \sim
  D}[\log{\sigma({r_{\phi}(x, y_{w}) - r_{\phi}(x, y_{l})}})]}}
  \end{align*}
  $$

  </p>
<p>maximize the gap of rewards between better/worse response</p>
</li>
</ul>
<h4 id="3-rl-fine-tuning-phase-pi_thetax--py">3. RL Fine-Tuning Phase: $\pi_{\theta}(x) = p(y)$</h4>
<ul>
<li>In the past, training LM with RL was considered impossible.</li>
<li>One of the proposed feasible plan is PGR(<strong>Policy Gradient RL</strong>)/PPO(<strong>Proximal Policy Optimization</strong>)</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>Denoising Diffusion Models</title>
      <link>https://mickqian.github.io/posts/ai/models/denoising-diffusion-models/</link>
      <pubDate>Sun, 14 Jan 2024 14:31:43 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/ai/models/denoising-diffusion-models/</guid>
      <description>Personal takeaways of DDIM/DDPM</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Term</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Diffusion Models</td>
          <td style="text-align: left">models that can sample from a highly complex probability distribution(e.g. images of cars)</td>
      </tr>
      <tr>
          <td style="text-align: left">Non-equilibrium thermodynamics</td>
          <td style="text-align: left">a branch of <a href="https://en.wikipedia.org/wiki/Thermodynamics" title="Thermodynamics">thermodynamics</a> that deals with physical systems that are not in <a href="https://en.wikipedia.org/wiki/Thermodynamic_equilibrium" title="Thermodynamic equilibrium">thermodynamic equilibrium</a>, where &ldquo;there are no net <a href="https://en.wikipedia.org/wiki/Macroscopic" title="Macroscopic">macroscopic</a> <a href="https://en.wikipedia.org/wiki/Flow_(mathematics)" title="Flow (mathematics)">flows</a> of <a href="https://en.wikipedia.org/wiki/Matter" title="Matter">matter</a> nor of energy within a system or between systems&rdquo;. <br>It is often used by diffusion models as a technique to sample from distribution.</td>
      </tr>
      <tr>
          <td style="text-align: left">Diffusion</td>
          <td style="text-align: left">the <strong>net movement</strong> of anything, generally from a region of higher <a href="https://en.wikipedia.org/wiki/Concentration" title="Concentration">concentration</a> to a region of lower concentration. <br>Also a technique of <strong>Non-equilibrium thermodynamics</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">DDPM</td>
          <td style="text-align: left">model that improves the performance of <strong>diffusion models</strong> by <strong>variational inference</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">DDIM</td>
          <td style="text-align: left">a generalized version of DDPM, with better performance and less diversity and quality</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Jensen</strong> inequality</td>
          <td style="text-align: left">$f(\sum\limits a_{i}x_{i}) \le \sum\limits a_{i}f(x_{i})$, where $a \ge 0, \sum\limits a_{i} = 1$<br>In other words, the Expected Value of a convex function $\ge$ the value of the function at the Expected Input <br></td>
      </tr>
      <tr>
          <td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">Variational Lower Bound</a>(short for <strong>VLB</strong>, a.k.a. Evidence Lower BOund, short for <strong>ELBO</strong>)</td>
          <td style="text-align: left">A easy-to-train lower bound of Log-Likelihood, derived by using a prior $p(z)$ to approximate (implies <em>variational</em>) an <del>intractable</del> posterior $q$.</td>
      </tr>
      <tr>
          <td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant"><strong>Jacobian matrix</strong></a></td>
          <td style="text-align: left">A matrix derived from a vector of function of several variables, with all its first-order partial derivatives. Suppose $f: R^{n} \to R^{m}$:<br>$$<br>J = [\frac{\partial{f}}{\partial{x_{1}}}&hellip;\frac{\partial{f}}{\partial{x_{n}}}]<br>$$</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
  </tbody>
</table>
<h2 id="notations">Notations</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notation</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">$x_{0}$</td>
          <td style="text-align: left">the data point, where $t$ is the total count of timestamp</td>
      </tr>
      <tr>
          <td style="text-align: left">$x_{t}$</td>
          <td style="text-align: left">the data after applying $t$ times of forward iteration</td>
      </tr>
      <tr>
          <td style="text-align: left">$\epsilon_t$</td>
          <td style="text-align: left">the (standard gaussian) noise</td>
      </tr>
      <tr>
          <td style="text-align: left">$\epsilon_{\theta}(x_t,t)$</td>
          <td style="text-align: left">our model to predict the noise at each timestamp</td>
      </tr>
      <tr>
          <td style="text-align: left">$\mu_{\theta}(x_t, t)$</td>
          <td style="text-align: left">parameterized model to predict $x_{t-1}$ at time $t$</td>
      </tr>
      <tr>
          <td style="text-align: left">$p(x_{0:T})$</td>
          <td style="text-align: left">the joint distribution of $x_{0}, x_{1} &hellip; x_{T}$</td>
      </tr>
  </tbody>
</table>
<h2 id="introduction">Introduction</h2>
<p>This article will introduce the definitions of <strong>DDPM</strong> and <strong>DDIM</strong>.</p>
<p>As stated earlier, the work of <strong>DDIM</strong> is based on <strong>DDPM</strong>.</p>
<h2 id="ddpm">DDPM</h2>
<p><strong>Diffusion Models</strong> often involves modeling two processes:</p>
<ul>
<li><strong>forward process</strong>: noise data($x_{0}$) to data point($x_{t}$)</li>
<li><strong>reverse process</strong>: data point to noise data, the reversion of <em>forward process</em></li>
</ul>
<h3 id="forward-process">Forward Process</h3>
<p>As a improvement of Diffusion Models, <strong>DDPM</strong> models the forward process as:</p>
<p>$$
\begin{equation}
x_{t} = \alpha_{t} x_{t-1} + \beta_{t}\epsilon_{t}, \epsilon_{t} \sim \mathcal{N}(0,1), 0 \le t \le T, t \in \mathbb{Z}
\end{equation}
$$</p>
<p>where $\alpha, \beta &gt; 0, \alpha_{t}^{2}+ \beta_{t}^{2} = 1$. This can be viewed as:</p>
<ul>
<li>the remains from the previous data: $\alpha_{t} x_{t-1}$</li>
<li>the destruction by introducing noise $\epsilon_{t}$</li>
</ul>
<p>Accordingly, the conditional probability of $x_t$ would be:
$$
p(x_{t}| x_{t-1}) = \mathcal{N}(x_{t};\alpha_{t}x_{t-1}, \beta_{t}^{2}I)
$$</p>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>All $\alpha, \beta, T$ are constants</li>
<li>Apparently, this is a <strong>Markovian process</strong></li>
</ul>
</blockquote>
<h3 id="reverse-process">Reverse Process</h3>
<p>By applying the forward process for $T$ times, we have $t$ pairs of $(x_{t-1}, x_{t})$. This is our training data.</p>
<p>Reversing the forward process, the task of the reverse process should be:
learn how to get  $x_0$ from $x_{t}$, formally $x_0 \to x_{t}$.</p>
<p><strong>DDPM</strong> splits this process into $t$ steps of  $x_t \to x_{t-1}$.</p>
<blockquote>
<p>[!TIP]  The Methodology of DDPM
<strong>DDPM</strong> is a Likelihood-based Model.</p>
</blockquote>
<p>In the paper, they model each single step as a <strong>gaussian transition</strong>:
$$
p_{\theta}(x_{t-1}|x_{t}) = \mathcal{N}(x_{t-1};\mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t},t))
$$
where:</p>
<ul>
<li>$\mu_{\theta}(x_{t}, t)$: the mean value</li>
<li>$\Sigma_{\theta}(x_{t},t)$: the variance predictor (of reverse process).</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>The noise is not gaussian noise multiplied by a factor, but predicted directly.</li>
<li>Not to confuse:
* $\Sigma_{\theta}(x_{t},t)$: the noise in reverse process. It has been tested positive to the reverse process
* $\epsilon_\theta(x_{t},t) \to \epsilon_{t}$ : the noise predictor (<strong>of forward process</strong>)</li>
</ul>
</blockquote>
<h4 id="mean-value-predictor-mu_thetax_t-t">Mean Value Predictor: $\mu_{\theta}(x_{t}, t)$</h4>
<p>In order to model the $\mu_{\theta}(x_{t}, t)$, from Bayes&rsquo; Theorem we have:
$$
p(x_{t-1}| x_{t},x_0)= \frac{p(x_t|x_{t-1}) p(x_{t-1} | x_0)}{p(x_{t}|x_{0}) }
$$</p>
<p>The process of induction would be:</p>
<ol>
<li>Predict $x_t$, $x_{t-1}$ from $x_0$</li>
<li>Replace all the variables($x_{0}$) with $x_{t}$ in Equation 4</li>
</ol>
<h5 id="predict-x_0-with-x_t">Predict $x_{0}$ with $x_{t}$</h5>
<p>Applying forward process $p(x_{t}|x_{t-1})$ for $t$ times, we can rewrite $x_{t}$ as:</p>
<p>$$
x_{t} = \bar \alpha_{t} x_{0} + \bar{\beta_{t}} ^ {2}\epsilon_{t}, \bar \alpha_{t} = \prod \alpha_{i}, \bar \beta_{t} = \sqrt{1-\bar \alpha_{t}^{2}}
$$
And the probability version:
$$
p(x_t|x_{0}) = \mathcal{N}(x_{t}; \bar \alpha_{t} x_{0},  \bar \beta_{t} ^ {2}I)
$$</p>
<p>Now that we have $x_t$ from $x_0$, update Eq 4 (since $\mathcal{N}$ can be represented as probabilities, the result is conformed to $\mathcal{N}$ as well):</p>

$$
p(x_{t-1}| x_{t},x_{0}) = \mathcal{N}\left(x_{t-1}; \underbrace{\frac{\alpha_{t}\bar \beta_{t-1}^{2}}{\bar \beta_{t}^{2}}x_{t} + \frac{\bar \alpha_{t-1}\beta_{t}^{2}}{\bar \beta_{t}^{2}}x_{0}}_{\text{$\tilde \mu_t(x_{t}, x_{0})$}},\frac{\bar \beta_{t-1}^{2}\beta_{t}^{2}}{\bar \beta_{t}^{2}}I\right)
$$

<p>Let&rsquo;s define the predicted mean value of $x_{t-1}$ as $\tilde \mu_t(x_{t}, x_{0}) = \frac{\alpha_{t}\bar \beta_{t-1}^{2}}{\bar \beta_{t}^{2}}x_{t} + \frac{\bar \alpha_{t-1}\beta_{t}^{2}}{\bar \beta_{t}^{2}}x_{0}$.</p>
<p>Notice the meaning of it: <strong>With Bayes&rsquo; Theorem, using $x_{t}$ and $x_{0}$, we can derive the mean value of $x_{t-1}$.</strong></p>
<p>So naturally, we can make our $\mu_{\theta}(x_{t}, t)$, who have the same estimated output as  $\tilde \mu_t(x_{t}, x_{0})$, learn the distribution of it:
$$
\mu_{\theta}(x_{t}, t) = \tilde \mu_t(x_{t}, x_{0})
$$</p>
<blockquote>
<p>[!NOTE] Different ways of modeling $\mu_\theta$ is also acceptable, it&rsquo;s just that this is a better way (or not)</p>
</blockquote>
<p>However, we don&rsquo;t have $x_0$ to pass to $\tilde \mu_t(x_{t}, x_{0})$.  Luckily, we can <strong>predict</strong> $x_0$ from rewriting Equation 7:
$$
x_{0}= \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{t}
$$</p>
<blockquote>
<p>[!TIP]
This is actually an embodiment of the <a href="https://en.wikipedia.org/wiki/Predictor%E2%80%93corrector_method"><strong>predictor–corrector</strong></a> method</p>
</blockquote>
<p>Since we don&rsquo;t have $\epsilon$ in the reverse process, we can make a neural work to learn it: $\epsilon_\theta(x_t,t) \to \epsilon_t$ :
$$
x_{0}= \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{\theta}(x_{t}, t)
$$</p>
<p>Update the Eq 10:
$$
\mu_{\theta}(x_{t}, t) = \tilde \mu_t(x_{t}, x_{0}) = \tilde \mu_t\left(x_{t}, \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{\theta}(x_{t}, t)\right)= \frac{1}{\alpha_{t}}\left(x_{t} - \frac{\beta_{t}^2}{\bar\beta_{t}}\epsilon_{\theta}(x_{t},y, t)\right)
$$</p>
<h4 id="reverse-noise-predictor-sigma_thetax_tt">Reverse Noise Predictor: $\Sigma_{\theta}(x_{t},t)$</h4>
<p>It still remains to design $\Sigma_{\theta}(x_{t},t)$, since it encourages diversity.</p>
<p>The DDPM paper suggested not learning it, since it:</p>
<blockquote>
<p>resulted in unstable training and poorer sample quality</p>
</blockquote>
<p>By fixing it at some value $\Sigma_{\theta}(x_{t},t) = \sigma_{t}^{2}I$ , where either $\sigma_{t}^{2} = \beta_{t}$ or $\tilde{\beta_t}$ yielded similar performance.</p>
<h3 id="training--defining-loss">Training &amp; Defining Loss</h3>
<p>Conclusively, we have only defined one trainable model: $\epsilon_{\theta}(x_t, t)$</p>
<h4 id="to-reconstruct-x_0">To reconstruct $x_{0}$</h4>
<p>The training target can be <strong>MLE</strong>, the objective function being log-likelihood of reconstructing $x_{0}$:</p>

$$
\begin{align*}
\ln p(x_{0}) &= \int{\ln p(x_{0:T})dx_{1:T}} & \text{marginalization of marginal distribution}\\\\
&=  \ln \int{p(x_{0:T})dx_{1:T}}\\\\\\
&=  \ln \mathbb{E}_{q(x_{1:T}|x_{0})}[\int{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}}]\\\\
&\ge \mathbb{E}_{q(x_{1:T}|x_{0})}\left[\ln \frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}\right] & \text{Jensen Inequality of $\log$}\\\\
&= \underbrace{\mathbb{E}_{q(x_{1}|x_{0})}\left[\ln p_{\theta}({x_{0}|x_{1}})\right]}_{\text{reconstruction term}} - \sum_{t = 2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\left[D_{KL}(q(x_{t-1}|x_{t},x_{0})||p_{\theta}(x_{t-1}|x_{t}))\right]\\\\
&= \sum\limits_{t=1}^{T}  \gamma \mathbb{E}_{q(x_{t}|x_{0})}[||\epsilon_{t} - \epsilon_{\theta}(x_{t}, t) || ^{2}] & \text{$\gamma$ being some constants}
\end{align*}
$$

<h4 id="to-optimize-the-pixels">To optimize the pixels</h4>
<p>We design the loss function of $\theta$ as the <strong>Euclidean distance</strong> of the true and predicted mean of  $x_{t-1}$:
$$
\begin{align*}
\ell  &amp;= ||x_{t-1} - \hat x_{t-1}|| ^ 2 \newline
&amp;= ||x_{t-1} - \mu_\theta(x_{t},t)|| ^ 2 \newline
&amp;=|| (\frac{1}{\alpha_{t}}(x_{t} - \beta_{t}\epsilon_{t}) ) ^ {2} - \frac{1}{\alpha_{t}}(x_{t} - \beta_{t}\epsilon_{\theta}(x_{t}, t)) ^ {2}||\newline
&amp;= \frac{\beta_{t}^{2}}{\alpha_{t}^{2}} ||\epsilon_{t} - \epsilon_{\theta}(x_{t}, t) || ^2
\end{align*}
$$</p>
<h2 id="ddim">DDIM</h2>
<p>While the original <strong>DDPM</strong> is capable to generate satisfying images, it is known for poor performance: since the denoising(reverse) diffusion process usually take $T \sim 1000$ times of noise-prediction.</p>
<p><strong>DDIM</strong> is proposed to boost the reverse process as <strong>Non-Markovian Process</strong>, by directly taking any model trained on <strong>DDPM</strong> and sampling only  $T_{ddim}, T_{ddim} \le T$ timestamps, with some timestamps skipped. As a side-effect, the quality is compromised a little.</p>
<h3 id="reverse-process-1">Reverse Process</h3>
<p>It still takes the same approach as DDPM: predict $x_0$ from $x_t$ first.</p>
<p>From Eq 4, we can see that the sampling/training do involves $x_t$, but doesn&rsquo;t actually involves $p(x_{t-1}|x_{t})$ (which is defined in our reverse model). Instead, it defines:</p>
<p>$$
q_\sigma(x_{t-1}|x_{t}, x_0) = \mathcal{N}(x_{t-1};\sqrt{\bar \alpha_{t-1}}x_0 + \sqrt{1 - \bar \alpha_{t-1} - \sigma_t^2}\frac{x_t - \sqrt{\bar \alpha_t}x_0}{\sqrt{1 - \bar \alpha_t}}, \sigma_t^2I)
$$</p>
<p>Hence, the relation between $x_{t-1}$ and $x_t$ is:</p>
<p>
$$
x_{t-1} = \sqrt{\alpha_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1 - \alpha_{t}}\epsilon_{\theta}(x_{t},t)} {\sqrt{\bar \alpha_{t}}} \right)}_{\text{predicted $x_0$}} + 
\underbrace{\sqrt{1 - \bar \alpha_{t-1} - \sigma_{t}^{2}\epsilon_{t}(x_{t},t)}}_{\text{predicted  noise}} + \underbrace{\sigma_{t} \epsilon_{t}}_{\text{random noise}}
$$


where:
$$
\sigma_t = \eta \sqrt{(\frac{1 - \bar \alpha_t}{1 - \bar \alpha_{t-1}}) \left(1 - \frac{\bar \alpha_t}{\bar \alpha_{t-1}}\right)}
$$
where $\eta \in (0,1)$ is a constant, indicating the level of random noise:</p>
<ul>
<li>$\eta = 1$: The random noise is maximized, which is <strong>DDPM</strong>.</li>
<li>$\eta = 0$: The random noise is totally removed, making it a deterministic process/Implicit model, which is <strong>DDIM</strong>. It relies entirely on the predicted noise, while sacrificing some diversity with lowering random noise level.</li>
</ul>
<p>As for the timestamps chosen, they are determined empirically.</p>
<style type="text/css">.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style>
<div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice tip" >
<p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"></use></svg></span>Tip</p><p>In fact, $\eta$ represents the degree of moving some of the noise from predicted noise $\epsilon_t$ to sampled noise $\epsilon$: the bigger the $\eta$, the less deterministic, the larger random noise will be introduced to the reverse process.</p></div>
<h2 id="short-summary">Short Summary</h2>
<p>Conclusively, both models apply the same forward process, and have the same target: $x_{t} \to x_{0}$, though they have differences in the reverse process:</p>
<ul>
<li><strong>DDPM</strong> maximize the random noise, and in order to mitigate the negative effects it has,  takes more timestamps in the reverse process</li>
<li><strong>DDIM</strong> boost the performance by only selecting some of the timestamps, and reduce the random noise level</li>
</ul>
<h2 id="conditioned-generation">Conditioned Generation</h2>
<p>While being able to generate high quality images with reasonable speed with the models mentioned above, it is a common feature to generated <strong>conditioned output</strong>.</p>
<p>Given condition $y$, our goal is to derive $p(x_{t-1}|x_{t},y)$</p>
<h3 id="classifier-guided-diffusion">Classifier Guided Diffusion</h3>
<p>Using bayes&rsquo; rule, we have:
$$
p(x_{t-1}|x_{t},y) = \frac{p(x_{t-1}|x_{t})p(y|x_{t-1},x_{t})}{p(y|x_{t})}
$$</p>
<p>Using the notations in <a href="/posts/ai/models/denoising-diffusion-models/#reverse-process">Reverse Process</a>:
$$
p(x_{t-1}|x_{t},y) \propto \exp(-||x_{t-1}-\mu(x_{t})-\Sigma_{t}^{2}\underbrace{\nabla_{x_{t}}\log p(y|x_{t})}<em>{\text{classifier}}||^{2}/2\Sigma</em>{t}^{2})
$$</p>
<p>So $\mu_{\theta}(x_{t}, t,y) = \mu(x_{t})+\Sigma_{t}^{2}\nabla_{x_{t}}\log p(y|x_{t})), \Sigma_{t} = \sigma_{t}$</p>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>The gradient of the prob is easy to get with <em>autograd</em>, if the classifier can output the prob</li>
<li>The classifier guides the model only when inferencing</li>
</ul>
</blockquote>
<h3 id="classifier-free-diffusion">Classifier-Free Diffusion</h3>
<p>To infer without a classifier, we need to blend the condition $y$ into training process.</p>
<p>By directly modeling the conditioned reverse process as  $p(x_{t-1}|x_{t},y) = \mathcal{N}(x_{t-1};\mu(x_{t},y), \sigma_{t}^{2}I)$, following the modeling of Eq. 11, we have:
$$
\mu(x_{t}, y) = \frac{1}{\alpha_{t}}\left(x_{t} - \frac{\beta_{t}^2}{\bar\beta_{t}}\epsilon_{\theta}(x_{t},y, t)\right)
$$</p>
<p>The $\epsilon_{\theta}(x_{t},y, t)$ can be trained to predict the noise under condition.</p>
<blockquote>
<p>[!WARNING]
The conditioned noise predictor depends on $y$, so retraining is required if the $y$ is changed</p>
</blockquote>
<h2 id="score-based-generative-models">Score-based generative models</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notation</th>
          <th style="text-align: left">Meaning</th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Ancestral Sampling</td>
          <td style="text-align: left">A sample method, auto-regressive</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Score Distillation Sampling</td>
          <td style="text-align: left">A sampling method(sampler) to generate samples from a diffusion model by <strong>optimizing a loss function</strong>. Basically, it utilizes(distills) the score function of a teacher diffusion model, to train a larger model, with the final result as a sample (as $t \to 0$).</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Informant_(statistics)">score function</a></td>
          <td style="text-align: left">The gradient of the <strong>log</strong>-likelihood function</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">$U(x) = -\log q(x)$</td>
          <td style="text-align: left">An energy function. The lower the likelihood, the higher the energy</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">$\mu_{\theta}(x_t, t)$</td>
          <td style="text-align: left">parameterized model to predict $x_{t-1}$ at time $t$</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">$p(x_{0:T})$</td>
          <td style="text-align: left">the joint distribution of $x_{0}, x_{1} &hellip; x_{T}$</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Langevin dynamic</a></td>
          <td style="text-align: left">A Markov chain Monte Carlo(MCMC) method for obtraining random samples</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left"><a href="/">Fisher Divergence</a></td>
          <td style="text-align: left"></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>Most often, we don&rsquo;t care about the probability $q(x)$ of a certain input $x$, but how it changes through time: therefore, we can utilize score function(gradient, changes) $s(x):=\nabla_{x}\log q(x)$</p>
<blockquote>
<p>[!TIP]
It is also an advantage of modeling the <strong>score</strong>: don&rsquo;t have to make sure <strong>probability</strong> sum up to 1</p>
</blockquote>
<p>With $s(x)$ allowing us to sample from $q(x)$ using thermodynamics, our goals changes to: <strong>model $q(x)$</strong></p>
<p>$$
dx_{t} = \nabla \log q(x) d_{t} + d{W_{t}}
$$</p>
<h3 id="loss">Loss</h3>
<p>We learn a model $s_{\theta}$ to <strong>match</strong>(approximate) the <strong>score</strong> $\nabla \log q$:</p>
<p>$$
s_{\theta} \approx \nabla \log q(x)
$$
&ndash; This is score matching.</p>
<p>Typically, score matching is formalized as minimizing <strong>Fisher divergence</strong> function . By expanding the integral, and performing an integration by parts, we have our loss function:
$$
\mathcal{L} = \mathbb{E}<em>{q}[||s</em>{\theta}(x) - \nabla \log q(x)||^{2}]
$$
However, it&rsquo;s infeasible since it requires access to unknown score  $\nabla \log q(x)$.</p>
<p>Fortunately, we have <strong>score matching</strong> techniques(e.g. <a href="https://en.wikipedia.org/wiki/Scoring_rule#Hyv%C3%A4rinen_scoring_rule" title="Scoring rule">Hyvärinen scoring rule</a>) which minimize the Fisher divergence without knowledge of the gorund-truth score:
$$
\mathcal{L} = \mathbb{E}<em>{q}\left[\nabla</em>{x} s_{\theta}(x)+ \frac{1}{2}||s_{\theta}(x)||_{2}^{2}\right]
$$</p>
<p>Since $s_{\theta}$ is modeled by ourself, its output and gradients can be easily calculated. We use Monte-Carlo methods with gradient descent to optimize it.</p>
<h3 id="sample--inference">Sample / Inference</h3>
<p>But how do we generate a sample.</p>
<p>Once we have trained a score-based model $s_{\theta}(x)$, we can use an iterative procedure called <strong><a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Langevin dynamics</a></strong> to draw samples from it:
$$
x_{i + 1} \leftarrow x_{i} + \epsilon \nabla_{x} \log p(x) + \sqrt{2\epsilon} z_{i}, z_{i} \sim \mathcal{N}(0, I)
$$
Notice some white noise is injected, to avoid all samples collapse into some limited local optimas.</p>
<p>This seems decent, but in fact: in low-density regions, the estimated scores are inaccurate.</p>
<p>It&rsquo;s natural to augment the low-density regions by perturbing our datapoint: injecting $\mathcal{N}$. It can solve the problem in low-density, however since the training data is perturbed, the generated samples are too.</p>
<p>Multiple (decreasing) noise levels $\sigma$ are applied as an input to score funcion $s$, with the output of previous model $i$ as the input of the next model $i+1$. The whole process resembles an <strong>Anneald Langevn Dynamics</strong></p>
<h2 id="sde">SDE</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notation</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">SDE</a></td>
          <td style="text-align: left">A DE in which one or more of the terms is a <strong>stohastic</strong> process</td>
      </tr>
      <tr>
          <td style="text-align: left">$\mathcal{U}(T_{a},T_{b})$</td>
          <td style="text-align: left">Uniform distribution over the time interval $[T_{a}, T_{b}]$</td>
      </tr>
      <tr>
          <td style="text-align: left">In <a href="/posts/ai/models/denoising-diffusion-models/#ddpm">DDPM</a>, we define $t$ as <em>discrete</em> timestamps, however it&rsquo;s more natural to model it as <em>continuous</em> time.</td>
          <td></td>
      </tr>
  </tbody>
</table>
<h3 id="forward-process-1">Forward Process</h3>
<p>With this premise, we model the forward process with <strong>Stochastic</strong> DE(Differential equation), but not funtion on timestamps:
$$
dx = f_{t}(x) + g_{t}dw
$$</p>
<h3 id="reverse-process-2">Reverse Process</h3>
<p>Similarly, we want to model $p(x_{t}|x_{t + \Delta{t}})$:
$$
\begin{align}
p(x_{t}|x_{t + \Delta{t}}) &amp;= \frac{p(x_{t + \Delta_{t}} | x_{t})p(x_{t})}{p(x_{t+ \Delta{t}})} \\
&amp;= p(x_{t + \Delta{t}} | x_{t})\exp(\log p(x_{t}) - \log p(x_{t+\Delta{t}})) \\
&amp;\propto \left(-\frac{||x_{t + \Delta_{t}} - x_{t} - f_{t}(x_{t})\Delta{t}||^{2}}{2g_{t}^{2}\Delta t}+  \log p(x_{t}) - \log p(x_{t+\Delta{t}})\right)
\end{align}
$$</p>
<p>In order to calculate the unknown diff, we apply <strong>Taylor expansion</strong>:
$$
\log p(x_{t+\Delta{t}}) \approx \log p(x_{t}) + (x_{t+\Delta t} - x_{t}) \cdot \nabla_{x_{t}}\log p(x_{t}) + \underbrace{\Delta t \frac{\partial \log p(x_{t})}{\partial t}}<em>{\text{$x</em>{t}$&rsquo;s deritive of $t$}}
$$</p>
<p>Update Equation 26-3 with it, we have:
$$
p(x_{t}|x_{t + \Delta{t}})  \sim \mathcal{N}(f_{t+\Delta t}(x_{t + \Delta t}) - g_{t + \Delta t}^{2}\nabla_{x_{t + \Delta t}} \log p(x_{t + \Delta t})\Delta t; g_{t + \Delta t}^{2}\Delta t I)
$$</p>
<p>and the SDE of <strong>reverse process</strong>:
$$
dx = [f_{t}(x) - g_{t}^{2}\nabla_{x}\log p_{t}(x)]dt + g_{t}dw
$$</p>
<h3 id="training">Training</h3>
<p>$$
\mathcal{L} = \mathbb{E}<em>{t \in \mathcal{U}(0, T)}\mathbb{E}</em>{p_{t}(x)}[\lambda(t)||\nabla_{x}\log p_{t}(x) - s_{\theta}(x,t)||^{2}_{2}]
$$
where:</p>
<ul>
<li>$\lambda : \mathbb{R} \to \mathbb{R}_{&gt;0}$  is a positive weighting function</li>
</ul>
<h2 id="probability-flow-ode">Probability flow ODE</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notation</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">PF(Probability flow) ODE</td>
          <td style="text-align: left">The ODE of an SDE</td>
      </tr>
  </tbody>
</table>
<p>Despite capable of generating high-quality samples, <em>samplers</em> based on Langevin MCMC and SDE solvers do not provide a way to compute the exact log-likelihood of score-based generative models.</p>
<p>It has been proved that, it is possible to convert any SDE into an ODE(ordinary differential equation) without changing its marginal distributions $p_{t}(x)$</p>
<h3 id="forward-process-2">Forward Process</h3>
<p>With a sequence of complex calculations(including F-P function &amp; Dirac function), we have:</p>
<p>$$
dx = [f(x,t) - \frac{1}{2}(g^{2}(t)-\sigma_{t}^{2})\nabla_{x}\log p_{t}(x)]dt
$$</p>
<h3 id="reverse-process-3">Reverse Process</h3>
<p>The reverse process of PF-ODE is given by:</p>
<p>$$
dx = [f(x,t) - \frac{1}{2}g^{2}(t)\nabla_{x}\log p_{t}(x)]dt
$$</p>
<blockquote>
<p>[!TIP]
When $\nabla_{x}\log p_{t}(x)$ replaces $s_{\theta}(x,t)$, PF ODE becomes a special case of a neural ODE</p>
</blockquote>
<h2 id="samplers">Samplers</h2>
<h3 id="euclidean">Euclidean</h3>
<p>$$
\begin{equation}\left.\frac{d\boldsymbol{x}<em>t}{dt}\right|</em>{t=t_{n+1}}\approx \frac{\boldsymbol{x}<em>{t</em>{n+1}} - \boldsymbol{x}<em>{t_n}}{t</em>{n+1} - t_n}\end{equation}
$$
一阶近似</p>
<h3 id="heun-solver">Heun solver</h3>
<h3 id="dpm-solver">DPM solver</h3>
<h3 id="amed-solver">AMED solver</h3>
]]></content:encoded>
    </item>
    
    <item>
      <title>VAE</title>
      <link>https://mickqian.github.io/posts/ai/models/vae/</link>
      <pubDate>Sun, 14 Jan 2024 14:05:03 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/ai/models/vae/</guid>
      <description>Takeaways from the maths of VAE</description>
      <content:encoded><![CDATA[<h2 id="terminology">Terminology</h2>
<table>
  <thead>
      <tr>
          <th>Notations</th>
          <th>Mean</th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$X \sim p_{r}$</td>
          <td>the input data</td>
          <td></td>
      </tr>
      <tr>
          <td>$z$</td>
          <td>the encoded latent</td>
          <td></td>
      </tr>
      <tr>
          <td>$\theta$</td>
          <td>the parameterized model</td>
          <td></td>
      </tr>
      <tr>
          <td>$\phi$</td>
          <td>the encoder</td>
          <td></td>
      </tr>
      <tr>
          <td>$p_{\theta}(x)$</td>
          <td>the likelihood of the data-reconstruction</td>
          <td></td>
      </tr>
      <tr>
          <td>$p(z)$</td>
          <td>the distribution of latent variable $z$ as a prior, often $\mathcal{N}(0,1)$</td>
          <td></td>
      </tr>
      <tr>
          <td>$q_{\phi}(z|x)$</td>
          <td>variational distribution</td>
          <td></td>
      </tr>
      <tr>
          <td>$q_{\phi}(z|x)$</td>
          <td>variational distribution</td>
          <td></td>
      </tr>
      <tr>
          <td><strong>MDL</strong>(Minimum Description Length)</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td><a href="https://en.wikipedia.org/wiki/Information_content">Self-Information $I$</a></td>
          <td>the amount of information, interpreted as the level of &ldquo;surprise&rdquo;<br>$$I(\mathcal{w}<em>{n}) = f(P(\mathcal{w}</em>{n})) = -\log(P(\mathcal{w}_{n})) \ge 0$$</td>
          <td></td>
      </tr>
      <tr>
          <td>Entropy $H(X)$</td>
          <td>the average amount of information in a message. A measure of <strong>uncertainty</strong>. <br>$$H(X) = E[I(X)] = E[-\ln(P(X))]$$<br></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h2 id="background">Background</h2>
<p><a href="https://en.wikipedia.org/wiki/Autoencoder"><strong>AutoEncoder</strong></a> is proposed to compress data and reduct dimensionality as a generalization of PCA, and largely used in <strong>signal processing</strong>, until someone found new samples can be generated by adding noise to latents and decoded by decoder.</p>
<p>However, the ability of AutoEncoder to generate new samples by the distribution of the latents $z$, this is why &amp; when <strong>Variational AutoEncoder</strong> is developed.</p>
<blockquote>
<p>[!TIP]
AE is an approach of <strong>MDL</strong></p>
</blockquote>
<h2 id="requirements">Requirements</h2>
<ul>
<li>In order to be able to generate new samples using decoder, we will be happy if $z \sim \mathcal{N}(0, 1)$</li>
</ul>
<h2 id="modeling">Modeling</h2>
<p>We apply <strong>Maximum Likelihood Estimation</strong> here.</p>
<p><strong>Log Likelihood</strong> is defined as:
$$
Likelihood = \log P_{\theta}(X)
$$</p>
<p>which represents the ability of the model to reconstruct the input data.</p>
<p>Hence, from the definition of the loss function:</p>
<p>$$
\mathcal{L}(\theta) = - \mathbb E_{x \sim data}[\log p_{\theta}(x)]
$$</p>
<p>Normally, the $x\sim data$ is neglected.</p>
<p>Our goal is to minimize the loss function, in the mean time force encoder to encode $X$ as $z \sim \mathcal{N}(\mu, \sigma^{2}I)$</p>
<h3 id="implicit-model">Implicit Model</h3>
<p>We define $z$ as an implicit variable, making our model an <strong>implicit model</strong>.</p>
<p>Rewrite the log-likelihood:
$$
p_{\theta}(x) = \int{p_{\theta}}(x|z)p_{\theta }(z)dz
$$
where $\theta$ is the parameter of the implicit model (encoder and decoder).</p>
<p>However there&rsquo;s a common problem for implicit models: the integration relies on the exhaustion on implicit variable $z$.</p>
<p>In our case, as $z \sim \mathcal{N}(\mu, \sigma^{2}I)$, it is deem impossible.</p>
<h3 id="mc">MC</h3>
<p>Monte-Carlo is a method to approximate an intractable <del>equation</del>(integration) by sampling a lot of data ($p_{\theta}(x | z)$):
$$
\begin{align*}
p_{\theta}(x) &amp;= \int{p_{\theta}}(x|z)p_{\theta }(z)dz\\
&amp;\approx \frac{1}{m} \sum\limits_{j =1}^{m} p_{\theta}(x | z_{j})
\end{align*}
$$
But that does not enforce  $z \sim \mathcal{N}(\mu, \sigma^{2}I)$.</p>
<h3 id="variational-bayes">Variational Bayes</h3>
<h4 id="deriving-elbo">Deriving ELBO</h4>
<p>Considering the log-likelihood can be rewritten in the following process:</p>
<p>$$
\begin{align*}
\log p_{\theta}(x) &amp;= \log p_{\theta}(x) \int_{z}p_{\phi}(z|x)dz &amp;\text{Normalization}
\\
&amp;= \int_{z}p_{\theta}(z|x)\log p_{\theta}(x)dz
\\
&amp;=  \int_{z}p_{\theta}(z|x) \log \frac{p_{\theta}(x,z)}{p(z|x)} dz  &amp;\text{Bayes&rsquo; Theorem}
\\
&amp;= \int_{z}(p_{\theta}(z|x)\log p_{\theta}(x,z) - p_{\theta}(z|x)\log p(z|x))dz
\\
&amp;= \log p_{\theta}(x,z) - \log p_{\theta}(z|x)
\end{align*}
$$</p>
<p>Since the posterior $\log p_{\theta}(z|x)$ is intractable (only involves integration on latent variable $z$, see <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes&rsquo; Theorem</a>), a new distribution(which is easy to <em>learn</em>) $q_{\phi}(z|x)$ is used to approximate it, where $\phi$ is the encoder.</p>
<p>Let&rsquo;s continue by replacing:</p>

$$
\begin{align*}
\underbrace{\log p(x)}_{\text{evidence}} &= \log p_\theta(x,z) - \log q_{\phi}(z|x) \newline
&= \int_{z} q_{\phi}(z|x)\log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}dz \newline
&= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)} \cdot  \frac{q_{\phi}(z|x)}{p(z|x)})dz \newline
&= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)})dz + \int_{z}q_{\phi}(z|x)\log(\frac{q_{\phi}(z|x)}{p(z|x)})dz \newline
&= \mathcal L(\theta,\phi; x) + D_{KL}(q_{\phi}, p_{\theta}) \newline
&\ge \underbrace{\mathcal L(\theta,\phi; x)}_{\text{ ELBO }} & \text{$D_{KL}\ge 0$} 
\end{align*} 
$$

<p>$\mathcal L(\theta, \phi; x) = \mathbb{E}_{z \sim q(.|x)}{\log \frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}}$  is <strong>ELBO</strong>(Evidence Lower Bound), as it is the lower bound of evidence $\mathcal{L}(\theta)$, omitting the <strong>KL</strong> term. Maximizing ELBO is directly:</p>
<ul>
<li>maximizing log-likelihood</li>
<li>minimizing KL-Divergence of posterior $p_{\theta}$ and variational distribution $q_{\phi}$</li>
</ul>
<h4 id="maximizing-elbo">Maximizing ELBO</h4>
<p>And we can break it down further:

$$
\begin{align*}
\underbrace{\mathcal L(\theta, \phi; x)}_{\text{ELBO}} &= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)})dz = \mathcal{H}[q_{\phi}(z|x)] + \mathbb{E}_{z}[p_{\theta}(x,z)] \\\\
&= \int_{z}q_{\phi}(z|x)\log(\frac{p(z) * p_{\theta}(x|z) }{q_{\phi}(z|x)})dz & \text{Bayes' Theorem}\\\\
&= \int_{z}q_{\phi}(z|x)\log\frac{p(z) }{q_{\phi}(z|x)}dz + \int_{z}q_{\phi}(z|x)\log p_{\theta}(x|z)dz\\\\ 
&= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}_{\text{$\mathcal L_{reg}$}} + \underbrace{\mathbb E_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]}_{\text{$\mathcal L_{reconstruct}$}}\\\\ 
\end{align*}
$$
</p>
<blockquote>
<p>[!Note]
$\int_{z}p(z)*f(z)dz = \mathbb E_{z \sim p(.)}[f(z)]$, which is the expectation of p with z sampled from $p(z)$</p>
</blockquote>
<p>This is ELBO:</p>
<ul>
<li>$\mathcal L_{reg}$: the KL-divergence of variational distribution and prior distribution</li>
<li>$\mathcal L_{reconstruct}$: the Expectation of log reconstruct-likelihood under <em>variational distribution</em></li>
</ul>
<p>Since $\mathcal{L}(\theta) = -\log p(x) \le - \text{ELBO}$, by maximizing ELBO, we can indirectly minimize $L(\theta)$.</p>
<p>Hence, we define $\mathcal{L} = -\text{ELBO}$.</p>
<h3 id="training">Training</h3>
<p>$$
\begin{align*}
\text{ELBO} &amp;= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}<em>{\text{$\mathcal L</em>{reg}$}} + \underbrace{\mathbb E_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]}<em>{\text{$\mathcal L</em>{reconstruct}$}}\\
&amp;= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}<em>{\text{$\mathcal L</em>{reg}$}} + MSE(x, \hat x)
\end{align*}
$$</p>
<p>As $z$  is <strong>sampled</strong> from $\sim q_{\phi}(z|x)$, which is a variational distribution, the gradient of ELBO will not be able to propagate back to encoder $\phi$ (in-differentiable, chain rule).</p>
<p>Thus, <strong>re-parameterization</strong> is applied: $z = \mu + \epsilon \times \sigma, \hat z \sim \mathcal{N}(0, I)$, where $\phi(X) = (\mu, \epsilon)$. This way, the gradient is passed back to $\phi$, by representing $z$ with the output of $\phi$, where $z$ participates in the loss-calculation</p>
<h2 id="problems">Problems</h2>
<h3 id="blurry-output">Blurry output</h3>
<ul>
<li>the prior: $p(z) \sim \mathcal{N}(0, I)$</li>
<li>MSE is used to measure $L_{reconstruct}$</li>
<li></li>
</ul>
<p>DAE:
corrupt X，降低图片的冗余度（图片的冗余性一般都很高）</p>
<h2 id="dall-e">Dall E</h2>
<p>两阶段：</p>
<ol>
<li>clip 构造对比学习的正负样本对</li>
<li>文本 -&gt; clip encoder -&gt; text embedding -&gt; (diffusion) prior -&gt; image embedding -&gt; diffusion model decoder -&gt; image</li>
</ol>
<p>transformer encoder 本质上是自回归模型，可以基于自注意力和输入，自回归地生成同类型的内容</p>
<p>![[Pasted image 20230618153050.png]]</p>
<p>![[Pasted image 20230618154911.png]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Bossa Nova Songs Progression</title>
      <link>https://mickqian.github.io/posts/music/bossa-nova-songs-progression/</link>
      <pubDate>Sat, 06 Jan 2024 21:15:22 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/music/bossa-nova-songs-progression/</guid>
      <description>Chord progression of some well-known bossa-nova songs</description>
      <content:encoded><![CDATA[<script src="/js/opensheetmusicdisplay.min.js">

</script>
<script>

function load_and_render(path, div_id) {
    const osmd = new opensheetmusicdisplay.OpenSheetMusicDisplay(div_id);
    osmd.setOptions({defaultFontFamily: "Petaluma Script"});

    osmd.load(path).then(function() {
      osmd.setOptions({defaultFontFamily: "Petaluma Script"});
      osmd.EngravingRules.DefaultColorCursor = "currentColor";
      osmd.EngravingRules.DefaultFontFamily = "Petaluma Script";
      osmd.EngravingRules.RenderRehearsalMarks = false;
      osmd.EngravingRules.RenderTitle = false;
      osmd.EngravingRules.RenderLyricist = false;
      osmd.EngravingRules.RenderChordSymbols = true;
      osmd.EngravingRules.setChordSymbolLabelText(opensheetmusicdisplay.ChordSymbolEnum.diminishedseventh, "");

      osmd.render();
    });
}
</script>
<link rel="stylesheet" href="/css/main.css">
<h1 id="desafinado">Desafinado</h1>
<div class="osmd" id="osmd-container-Desafinado"></div>
<script>
load_and_render("/MusicXML/desafinado_musescore.musicxml", "osmd-container-Desafinado");
</script>
<iframe width="560" height="315" src="https://www.youtube.com/embed/oqHONL-LZ58?si=8qLHfRC5OjhtQejx" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/IWoOWpZujCc?si=rBlI_sTWn10EKodQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="one-note-samba">One Note Samba</h1>
<div class="osmd" id="osmd-container-One Note Samba"></div>
<script>
load_and_render("/MusicXML/One Note Samba.musicxml","osmd-container-One Note Samba");
</script>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ptaqr5bZ2gA?si=FMcmszaGiuxOZs1Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="só-danço-samba">Só Danço Samba</h1>
<div class="osmd" id="osmd-container-So Danco Samba"></div>
<script>
load_and_render("/MusicXML/So Danco Samba.musicxml", "osmd-container-So Danco Samba");
</script>
<iframe src="https://www.soundslice.com/slices/P1jMc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/YJMEu1oxLjA?si=mu0rvLSbTI934dfM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/f6e6W304LhA?si=CUY1PaBZd3fY3c5H" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="doralice">Doralice</h1>
<div class="osmd" id="osmd-container-Doralice"></div>
<script>
load_and_render("/MusicXML/Doralice.musicxml", "osmd-container-Doralice");
</script>
<iframe src="https://www.soundslice.com/slices/1jkyc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/hU3IUUIZxEg?si=BuuJFM-MI6ZFFb-w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="o-pato">O Pato</h1>
<div class="osmd" id="osmd-container-O Pato"></div>
<script>
load_and_render("/MusicXML/O Pato.musicxml", "osmd-container-O Pato");
</script>
<iframe src="https://www.soundslice.com/slices/dHYMc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/jkBk8U2ikf8?si=S6-IWYlE6gFjPyWY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="chega-de-saudade-no-more-blues">Chega De Saudade (No More Blues)</h1>
<div class="osmd" id="osmd-container-Chega De Saudade (No More Blues)"></div>
<script>
load_and_render("/MusicXML/Chega De Saudade (No More Blues).musicxml", "osmd-container-Chega De Saudade (No More Blues)");
</script>
<iframe width="560" height="315" src="https://www.youtube.com/embed/c6wlgDiyrLQ?si=SFnmysADjupwS56B" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="aguas-de-março">Aguas De Março</h1>
<iframe src="https://www.soundslice.com/slices/QSHcc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<h1 id="menina-flor">Menina Flor</h1>
<iframe src="https://www.soundslice.com/slices/DPLMc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<h1 id="corcovado">Corcovado</h1>
<iframe src="https://www.soundslice.com/slices/3Rbcc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
]]></content:encoded>
    </item>
    
    <item>
      <title>Music of George Harrison</title>
      <link>https://mickqian.github.io/posts/music/music-of-george-harrison/</link>
      <pubDate>Sat, 06 Jan 2024 19:37:13 +0800</pubDate>
      
      <guid>https://mickqian.github.io/posts/music/music-of-george-harrison/</guid>
      <description>personal rankings of George Harrison&amp;rsquo;s Singles/Albums/Guitar Solos</description>
      <content:encoded><![CDATA[<link rel="stylesheet" href="/css/main.css">
<h1 id="single">Single</h1>
<h2 id="1-something">1. Something</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UelDrZ1aFeY?si=PaPM8YoThkbpquuE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="2-here-comes-the-sun">2. Here Comes the Sun</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/KQetemT1sWc?si=SmPBWEga3ou7gDNM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="3-while-my-guitar-gently-weeps">3. While My Guitar Gently Weeps</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/VJDJs9dumZI?si=XMjvBDs71NjqWrjh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="4-all-those-years-ago">4. All Those Years Ago</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/eNL40ql4CYk?si=fuAQaEGwfqsm9yvU&amp;start=144" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="5-wah-wah">5. Wah-Wah</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/NDVAQE7nplU?si=Ydjz3KdqNk_kqStL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="album">Album</h1>
<p>Beatles records are excluded</p>
<h2 id="1-all-things-must-pass">1. All Things Must Pass</h2>
<h2 id="2-brainwashed">2. Brainwashed</h2>
<h2 id="3-george-harrison">3. George Harrison</h2>
<h2 id="4--living-in-the-material-world">4.  Living in the Material World</h2>
<h1 id="guitar-solo">Guitar Solo</h1>
<h2 id="1-dark-sweet-lady">1. Dark Sweet Lady</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Mqq_yEJlwDQ?si=gxQtr_L-y7tWevCx&amp;start=101" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="2-something">2. Something</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UelDrZ1aFeY?si=xUAGs_4t-_ddr0Ag&amp;start=98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="3-the-light-that-has-lighted-the-world">3. The Light That Has Lighted the World</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Tcg_19WGPJs?si=_oKNGBCOx8naUBja&amp;start=82" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="4-give-me-love">4. Give me Love</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/WMof20FwaOs?si=nveFJrXxggUvED6h&amp;start=110" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="5-any-road">5. Any Road</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/r8fFdc-karA?si=FJcZcUzybW0JodEd&amp;start=210" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="6-cheer-down">6. Cheer Down</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Yk-5iu1QE0E?si=Ist4VWRXwviqgjM1&amp;start=144" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="7-stuck-inside-a-cloud">7. Stuck Inside a Cloud</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/m1_zW9erCCM?si=0AlEcpq2-MykneL5&amp;start=111" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="8-crippled-inside">8. Crippled Inside</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/EX7roXRmkOU?si=pmsqtfpwrlSk8zo0&amp;start=77" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="9-how-do-you-sleep">9. How Do You Sleep?</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/teD9t-lO_o0?si=Vt8mZOYRjDYVdtav&amp;start=157" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="10-isnt-it-a-pity">10. Isn&rsquo;t it a Pity</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-n-LULDiJxk?si=nsReB-tVfznad_mC&amp;start=125" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="11-rising-sun">11. Rising Sun</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/BW4plsNcKQo?si=k-eBX-1xlgVfjTnX&amp;start=222" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>]]></content:encoded>
    </item>
    
    <item>
      <title></title>
      <link>https://mickqian.github.io/posts/2025-02-11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mickqian.github.io/posts/2025-02-11/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;type check: Multimodality&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang/pull/3405&#34;&gt;https://github.com/sgl-project/sglang/pull/3405&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;mit, vit yinliangsheng&lt;/li&gt;
&lt;li&gt;RLHF vllm,&lt;/li&gt;
&lt;/ol&gt;</description>
      <content:encoded><![CDATA[<ol>
<li>type check: Multimodality</li>
<li><a href="https://github.com/sgl-project/sglang/pull/3405">https://github.com/sgl-project/sglang/pull/3405</a></li>
<li>mit, vit yinliangsheng</li>
<li>RLHF vllm,</li>
</ol>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
