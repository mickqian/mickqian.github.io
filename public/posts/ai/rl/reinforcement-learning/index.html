<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Reinforcement Learning | Mick&#39; Blog</title>
<meta name="keywords" content="RL, RLHF, DPO">
<meta name="description" content="Personal takeaways of RL/RLHF/DPO">
<meta name="author" content="Mick">
<link rel="canonical" href="https://mickjagger19.github.io/posts/ai/rl/reinforcement-learning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.7da7716a1f2d0725f74c6ae7f8d6adafc43aabe2b366b65bfbf433448e2a2001.css" integrity="sha256-fadxah8tByX3TGrn&#43;Natr8Q6q&#43;KzZrZb&#43;/QzRI4qIAE=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://mickjagger19.github.io/favicon.ico">
<link rel="apple-touch-icon" href="https://mickjagger19.github.io/apple-touch-icon.png">
<link rel="alternate" hreflang="en" href="https://mickjagger19.github.io/posts/ai/rl/reinforcement-learning/">

<meta name="twitter:title" content="Reinforcement Learning | Mick&#39; Blog" />
<meta name="twitter:description" content="Personal takeaways of RL/RLHF/DPO" />
<meta property="og:title" content="Reinforcement Learning | Mick&#39; Blog" />
<meta property="og:description" content="Personal takeaways of RL/RLHF/DPO" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mickjagger19.github.io/posts/ai/rl/reinforcement-learning/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2024-01-16T21:31:43&#43;08:00" />
  <meta property="article:modified_time" content="2024-01-16T21:31:43&#43;08:00" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://mickjagger19.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reinforcement Learning",
      "item": "https://mickjagger19.github.io/posts/ai/rl/reinforcement-learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reinforcement Learning | Mick' Blog",
  "name": "Reinforcement Learning",
  "description": "Personal takeaways of RL/RLHF/DPO",
  "keywords": [
    "RL", "RLHF", "DPO"
  ],
  "wordCount" : "1728",
  "inLanguage": "en",
  "datePublished": "2024-01-16T21:31:43+08:00",
  "dateModified": "2024-01-16T21:31:43+08:00",
  "author":{
    "@type": "Person",
    "name": "Mick"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mickjagger19.github.io/posts/ai/rl/reinforcement-learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Mick' Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mickjagger19.github.io/favicon.ico"
    }
  }
}
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
      integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
        integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
        crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            
            throwOnError: false
        });
    });
</script>












































































































<link rel="stylesheet" href="/css/main.css">
<link rel="stylesheet" href="/assets/scss/main.scss">
<link rel="stylesheet" href="/css/custom.css">


<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

</head>

<body class=" dark type-posts kind-page layout-" id="top"><script data-no-instant>
    function switchTheme(theme) {
        switch (theme) {
            case 'light':
                document.body.classList.remove('dark');
                break;
            case 'dark':
                document.body.classList.add('dark');
                break;
            
            default:
                if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
                    document.body.classList.add('dark');
                }
        }
    }

    function isDarkTheme() {
        return document.body.className.includes("dark");
    }

    function getPrefTheme() {
        return localStorage.getItem("pref-theme");
    }

    function setPrefTheme(theme) {
        switchTheme(theme)
        localStorage.setItem("pref-theme", theme);
    }

    const toggleThemeCallbacks = {}
    toggleThemeCallbacks['main'] = (isDark) => {
        
        if (isDark) {
            setPrefTheme('light');
        } else {
            setPrefTheme('dark');
        }
    }

    
    
    
    window.addEventListener('toggle-theme', function () {
        
        const isDark = isDarkTheme()
        for (const key in toggleThemeCallbacks) {
            toggleThemeCallbacks[key](isDark)
        }
    });

    
    function toggleThemeListener() {
        
        window.dispatchEvent(new CustomEvent('toggle-theme'));
    }

</script>
<script>
    
    (function () {
        const defaultTheme = 'dark';
        const prefTheme = getPrefTheme();
        const theme = prefTheme ? prefTheme : defaultTheme;

        switchTheme(theme);
    })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mickjagger19.github.io/" accesskey="h" title="Mick&#39; Blog (Alt + H)">Mick&#39; Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mickjagger19.github.io/posts/" title="Posts" class="active"
                >
                <span>Posts
                </span>
                </a>
            </li>
            <li>
                <a href="https://mickjagger19.github.io/archives/" title="Archive"
                >
                <span>Archive
                </span>
                </a>
            </li>
            <li>
                <a href="https://mickjagger19.github.io/search/" title="Search (Alt &#43; /)"data-no-instant accesskey=/
                >
                <span>Search
                </span>
                </a>
            </li>
            <li>
                <a href="https://mickjagger19.github.io/tags/" title="Tags"
                >
                <span>Tags
                </span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
    <header class="post-header">
        <h1 class="post-title">Reinforcement Learning</h1>
        <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
       stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"
       style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"
                                        style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6"
                                                                                style="user-select: text;"></line><line
          x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10"
                                                                              style="user-select: text;"></line></svg>
  <span>January 16, 2024</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor"
       stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"
                                                                                         fill="none"></path><circle
          cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>9 min</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor"
       stroke-width="1" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"
                                                                                         fill="none"></path><circle
          cx="12" cy="7" r="4"></circle><path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2"></path></svg>Mick</span>

            
            
        </div>
    </header> <div class="toc side right">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#terminologies" aria-label="Terminologies">Terminologies</a></li>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#value-based" aria-label="Value-based">Value-based</a><ul>
                        
                <li>
                    <a href="#dynamic-programming" aria-label="Dynamic Programming">Dynamic Programming</a></li>
                <li>
                    <a href="#monte-carlo" aria-label="Monte-Carlo">Monte-Carlo</a></li>
                <li>
                    <a href="#temporal-difference-methods" aria-label="Temporal Difference methods">Temporal Difference methods</a><ul>
                        
                <li>
                    <a href="#bootstrapping" aria-label="Bootstrapping">Bootstrapping</a></li>
                <li>
                    <a href="#value-estimation" aria-label="Value Estimation">Value Estimation</a></li>
                <li>
                    <a href="#sarsa" aria-label="SARSA">SARSA</a></li>
                <li>
                    <a href="#q-learning" aria-label="Q-Learning">Q-Learning</a></li>
                <li>
                    <a href="#dqn" aria-label="DQN">DQN</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#policy-gradient" aria-label="Policy Gradient">Policy Gradient</a><ul>
                        
                <li>
                    <a href="#actor-critic" aria-label="Actor-Critic">Actor-Critic</a></li>
                <li>
                    <a href="#a2c" aria-label="A2C">A2C</a></li>
                <li>
                    <a href="#a3c" aria-label="A3C">A3C</a></li>
                <li>
                    <a href="#dpg" aria-label="DPG">DPG</a></li>
                <li>
                    <a href="#ddpg" aria-label="DDPG">DDPG</a></li>
                <li>
                    <a href="#td3" aria-label="TD3">TD3</a></li>
                <li>
                    <a href="#sac" aria-label="SAC">SAC</a></li>
                <li>
                    <a href="#ppo-proximal-policy-optimization" aria-label="PPO (Proximal Policy Optimization)">PPO (Proximal Policy Optimization)</a></li>
                <li>
                    <a href="#dpodirect-preference-optimization" aria-label="DPO(Direct Preference Optimization)">DPO(Direct Preference Optimization)</a></li></ul>
                </li>
                <li>
                    <a href="#methods" aria-label="Methods">Methods</a><ul>
                        
                <li>
                    <a href="#rlhf" aria-label="RLHF">RLHF</a><ul>
                        
                <li>
                    <a href="#1-sft" aria-label="1. SFT">1. SFT</a></li>
                <li>
                    <a href="#2-reward-modeling-phase" aria-label="2. Reward Modeling Phase">2. Reward Modeling Phase</a></li>
                <li>
                    <a href="#3-rl-fine-tuning-phase-pi_thetax--py" aria-label="3. RL Fine-Tuning Phase: $\pi_{\theta}(x) = p(y)$">3. RL Fine-Tuning Phase: $\pi_{\theta}(x) = p(y)$</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

    <div class="post-content"><h2 id="terminologies">Terminologies<a hidden class="anchor" aria-hidden="true" href="#terminologies">¶</a></h2>
<p>General:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Reinforcement Learning</td>
<td style="text-align:left">A branch/paradigm of machine learning, concerned with how an intelligent agent behaves in a dynamic environment.</td>
</tr>
<tr>
<td style="text-align:left"><strong>BLUE</strong>(bilingual evaluation understudy)</td>
<td style="text-align:left">An algorithm for evaluating the quality of text which has been machine-translated from one natural language to another</td>
</tr>
<tr>
<td style="text-align:left">Reward Model(Actor model)</td>
<td style="text-align:left">A model aligned with human feedback, predicting the reward of given actions</td>
</tr>
<tr>
<td style="text-align:left">$G_{t}$</td>
<td style="text-align:left">Return(aka the future reward), total sum of <strong>discounted</strong> rewards after time $t$:  $G_{t} = {\sum}^{\infty}_{k = 0}\gamma^{k}R_{t + k + 1}$</td>
</tr>
<tr>
<td style="text-align:left">$V_{\pi}(s)$</td>
<td style="text-align:left">State-value function, measures the expected return of state $s$: $V(s) = \mathbb{E}_{\pi}[G_{t}\vert S_{t} = s]$ under $\pi$</td>
</tr>
<tr>
<td style="text-align:left">$Q_{\pi}(s,a)$</td>
<td style="text-align:left">Action-value function, measures the expected return of action $a$ under state $s$: $Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_{t} \vert S_{t} = s, A_{t} = a]$ under $\pi$</td>
</tr>
<tr>
<td style="text-align:left">Bellman Equations</td>
<td style="text-align:left">A set of equations that decompose the value function into <strong>immediate reward</strong> + <strong>discounted future values</strong></td>
</tr>
<tr>
<td style="text-align:left">$A_{q}$</td>
<td style="text-align:left">the action to update $Q$</td>
</tr>
<tr>
<td style="text-align:left">$A_{t+1}$</td>
<td style="text-align:left">the actual taken action</td>
</tr>
</tbody>
</table>
<p>In RL Algorithms (mostly adjectives):</p>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Model-based</td>
<td style="text-align:left">Algorithms of RL relying on a (environment dynamic) model, which defines $P(s&rsquo;\vert s,a), R(s,a)$</td>
</tr>
<tr>
<td style="text-align:left">Model-free</td>
<td style="text-align:left">Algorithms of RL learning by the interaction of the model with environment</td>
</tr>
<tr>
<td style="text-align:left">Policy-Based(Policy Gradient) Methods</td>
<td style="text-align:left">A branch of RL: quantize each action as <strong>PDF</strong></td>
</tr>
<tr>
<td style="text-align:left">Value-Based Methods</td>
<td style="text-align:left">A branch of RL: quantize each action as value(PMF ? )</td>
</tr>
<tr>
<td style="text-align:left">Current policy</td>
<td style="text-align:left">The policy(actions) actually taken by an agent in an episode</td>
</tr>
<tr>
<td style="text-align:left">On-policy</td>
<td style="text-align:left">Using the action in current(actually exploited/taken) policy to update $V$</td>
</tr>
<tr>
<td style="text-align:left">Off-policy</td>
<td style="text-align:left">Using an action not from current policy to update $V$</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">¶</a></h2>
<h2 id="value-based">Value-based<a hidden class="anchor" aria-hidden="true" href="#value-based">¶</a></h2>
<h3 id="dynamic-programming">Dynamic Programming<a hidden class="anchor" aria-hidden="true" href="#dynamic-programming">¶</a></h3>
<p>We can use Dynamic Programming to iteratively update and query value functions ($V_{\pi}$), with the help of Bellman
equations, <strong>when the model is fully known</strong>.</p>
<h3 id="monte-carlo">Monte-Carlo<a hidden class="anchor" aria-hidden="true" href="#monte-carlo">¶</a></h3>
<p>#model_free</p>
<p>Instead of modeling the environment, <strong>MC methods</strong> learns from <strong>episodes of raw experience</strong>, approximating the
observed mean return as expected return.</p>
<p>To optimally learn in <strong>MC</strong>, we take following steps:</p>
<ol>
<li>Improve the policy greedily: $\pi(s) = \underset{a}{argmax}Q(s, a)$</li>
<li>Generate a new episode with the combination of the new policy $\pi$ and randomness(e.g. $\epsilon$-greedy), balancing
between exploitation and exploration</li>
<li>Estimate $Q$ with the generated episode $\pi$</li>
</ol>
<h3 id="temporal-difference-methods">Temporal Difference methods<a hidden class="anchor" aria-hidden="true" href="#temporal-difference-methods">¶</a></h3>
<p>#model-free</p>
<blockquote>
<p>[!NOTE]
TD learning can learn from <strong>incomplete</strong> episodes</p>
</blockquote>
<h4 id="bootstrapping">Bootstrapping<a hidden class="anchor" aria-hidden="true" href="#bootstrapping">¶</a></h4>
<p><strong>Estimate</strong> the rewards, rather than exclusively carrying out the episode.</p>
<h4 id="value-estimation">Value Estimation<a hidden class="anchor" aria-hidden="true" href="#value-estimation">¶</a></h4>
<p>The estimated Value funciont $V$ is updated towards an estimated return $R_{t+1} + \gamma V(S_{t+1})$</p>
<h4 id="sarsa">SARSA<a hidden class="anchor" aria-hidden="true" href="#sarsa">¶</a></h4>
<p>#on-policy</p>
<blockquote>
<p>[!TIP]
Define $A_{q}$ as the action to update $Q$</p>
</blockquote>
<p>State-Action-Reward-State-Action
In each $t$:</p>
<ol>
<li>Choose $A_{t} = \underset{a \in A}{argmax}{Q(S_{t}, a)}$ with $\epsilon$-greedy</li>
<li>Obtain $R_{t + 1}$</li>
<li>Set $A_{t+1} \sim \pi(\cdot|s) = A_{q}$, under <strong>current policy</strong></li>
<li>Update $Q$ with the <strong>advantage of actual $A_{t+1}$ over expected reward</strong>:</li>
</ol>


$$
Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha(\underbrace{R_{t+1} + \gamma Q(S_{t + 1}, A_{t + 1})}_{\text{value  of current  policy, on-policy}} - \underbrace{Q(S_{t},A_{t})}_{\text{expected  value}})
$$


<ol start="5">
<li>$t = t + 1$</li>
</ol>
<blockquote>
<p>[!NOTICE]
$A_{q} == A_{t + 1}$, making it on-policy</p>
</blockquote>
<h4 id="q-learning">Q-Learning<a hidden class="anchor" aria-hidden="true" href="#q-learning">¶</a></h4>
<p>#off-policy</p>
<p>Q-learning is an off-policy method, with the steps in one episodes ($t, S_{t}$) being:</p>
<ol>
<li>Choose $A_{t} = \underset{a \in A}{argmax}Q(S_{t}, a)$ with $\epsilon$-greedy</li>
<li>Obtain $R_{t + 1}$</li>
<li>Set $A_{t+1} \sim \pi(\cdot|s)$, $A_{q} = \underset{a \in A}{\max} Q(S_{t + 1}, a)$</li>
<li>Update $Q$ with the <strong>advantage of optimal $A_{t + 1}$ over expected reward</strong>:<br>


   $$Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha(\underbrace{R_{t+1} + \gamma \underset{a \in A}{\max} Q(S_{t + 1}, a)}_{\text{best  value  after $A_{t}$, off-policy}} - \underbrace{Q(S_{t}, A_{t})}_{\text{expected  value}})
   $$

   </li>
<li>$t = t + 1$</li>
</ol>
<blockquote>
<p>[!NOTICE]
$A_{q} = \underset{a \in A}{\max} Q(S_{t + 1}, a) \ne A_{t + 1}$, making it off-policy</p>
</blockquote>
<h4 id="dqn">DQN<a hidden class="anchor" aria-hidden="true" href="#dqn">¶</a></h4>
<p>#off-policy</p>
<p>Deep Q-Network, An improvement of <strong>Q-Learning</strong>:</p>
<ul>
<li><strong>Experience Replay</strong>: All the episode steps $e_{t} = (S_{t}, A_{t}, R_{t}, S_{t+1})$ are stored in one replay memory
$D_{t} = {e_{1}, &hellip;, e_{t}}$. During Q-learning updates, samples are drawn at random from the replay memory and thus
one sample could be used multiple times.</li>
<li><strong>Periodically Updated Target</strong>: Q is optimized towards target values that are only <strong>periodically</strong> updated(not
updated in each iteration anymore). The Q network is cloned and kept frozen as the optimization target every <strong>C</strong>
steps (C is a hyperparameter).</li>
</ul>
<blockquote>
<p>[!WARNING]
Known for overestimating value function $Q$</p>
</blockquote>
<h2 id="policy-gradient">Policy Gradient<a hidden class="anchor" aria-hidden="true" href="#policy-gradient">¶</a></h2>


$$
\begin{align*}
J(\theta) = \underset{s \in S}{\sum\limits} d^{\pi}(s)V^{\pi}(s) = \underset{s \in S}{\sum\limits} d^{\pi} \underset{a
\in A}{\sum\limits} \pi_{\theta}(a|s)Q^{\pi}(s,a)
\end{align*}
$$


<h3 id="actor-critic">Actor-Critic<a hidden class="anchor" aria-hidden="true" href="#actor-critic">¶</a></h3>
<p>Actor-Critic learns the <strong>value function</strong> in addition to the policy, assisting the policy update.</p>
<p>It consists of two models:</p>
<ul>
<li><strong>Actor</strong> updates the policy $\theta$ of $\pi_\theta(a|s)$, suggested by critic</li>
<li><strong>Critic</strong> updates the value estimation function $Q(a|s) | V_{w}(s)$</li>
</ul>
<p>The main process being, for $t \in (1, T)$:</p>
<ol>
<li>Sample $a \sim \pi_{\theta}(a|s), r_{t} \sim R(s,a), s&rsquo; \sim P(s&rsquo;|s,a)$, next action $a&rsquo; \sim \pi_{\theta}(a&rsquo;|s&rsquo;)$</li>
<li>Update <strong>Actor</strong> $\theta$:
$$
\theta \leftarrow \theta + \alpha_{\theta} Q_{w}(s,a)\nabla_{\theta} ln \pi_{\theta}(
a|s)</li>
</ol>
<p>$$</p>
<p>to maximize the reward
5. Compute the correction (TD error, measures the quality of current policy $a&rsquo;$):


$$
\delta_{t} = \underbrace{r_{t} + \gamma Q_{w}(s', a')}_{\text{Action-Value of a'}} - \underbrace{Q_{w}(s,a)}_{\text{actual reward}}
$$


6. Update <strong>Critic</strong>: $w \leftarrow w + \alpha_{w}\delta_{t}\nabla_{w}Q_{w}(s,a)$ to reduce estimate error (ideally,
$\delta_{t} \leftarrow 0$, as $a&rsquo; \sim \pi_{\theta}(a&rsquo;|s&rsquo;)$)
7. Update $a \leftarrow a&rsquo;, s \leftarrow s'$</p>
<blockquote>
<p>[!TIP]
Adversarial training, resembles GAN: (generator, discriminator)</p>
</blockquote>
<h3 id="a2c">A2C<a hidden class="anchor" aria-hidden="true" href="#a2c">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Actor $\pi_{\theta}$</td>
<td style="text-align:left">The target model</td>
</tr>
<tr>
<td style="text-align:left">Critic</td>
<td style="text-align:left">Estimate $V$</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<p>$$
L(\theta) = -\log \pi_{\theta}(a_{t}|s_{t})\hat A_{t}
$$</p>
<p>where:</p>
<ul>
<li>$\hat A$: advantage function, the advantage of $a_{t}$ compared with average, normally $V$</li>
</ul>
<blockquote>
<p>[!WARNING]
This objective function can lead to massive change to policy</p>
</blockquote>
<h3 id="a3c">A3C<a hidden class="anchor" aria-hidden="true" href="#a3c">¶</a></h3>
<p><strong>Asynchronous Advantage Actor-Critic</strong> focuses on parallel training. Multiple actors are trained in parallel and get
synced with global parameters.</p>
<h3 id="dpg">DPG<a hidden class="anchor" aria-hidden="true" href="#dpg">¶</a></h3>
<p>#model-free #off-policy</p>
<p><strong>Deterministic Policy Gradient</strong> models the policy as deterministic function $a = \mu(s)$.</p>
<p>It is trained by maximizing the objective function: the expected discounted reward:</p>
<p>$$
J(\theta) = \int_{S}\rho^{\mu}(s)Q(s, \mu_{\theta}(s))ds
$$</p>
<p>where:</p>
<ul>
<li>$\rho^{\mu}(s&rsquo;)$: discounted state distribution</li>
<li>$\mu$: the deterministic action predictor</li>
</ul>
<h3 id="ddpg">DDPG<a hidden class="anchor" aria-hidden="true" href="#ddpg">¶</a></h3>
<p>#model-free #off-policy</p>
<p><strong>Deep Deterministic Policy Gradient</strong></p>
<p>Combining <a href="/posts/ai/rl/reinforcement-learning/#dqn">DQN</a> (experience replay, freezing target model) and <a href="/posts/ai/rl/reinforcement-learning/#dpg">DPG</a></p>
<p>Key design:</p>
<ul>
<li>Better exploration: $\mu’(s) = \mu_{\theta}(s) + \mathcal{N}$, adding noise $\mathcal{N}$ to policy</li>
<li>Soft updates: Moving average of parameter $\theta$</li>
</ul>
<h3 id="td3">TD3<a hidden class="anchor" aria-hidden="true" href="#td3">¶</a></h3>
<p><strong>Twin Delayed Deep Deterministic</strong> applied tricks on <a href="/posts/ai/rl/reinforcement-learning/#ddpg">DDPG</a> to prevent overestimating value function:</p>
<ol>
<li>Clipped Double Q-learning: Action selection and Q-value estimation are made by two networks separately.</li>
<li>Delayed update of target the policy network: Instead of updating actor and critic in one iteration, <strong>TD3</strong> updates
the <strong>actor</strong> at a lower frequency than <strong>critic</strong>, waiting for it to become stable. It helps reducing the variance.</li>
<li>Target policy smoothing: Introduce a smoothing regularization strategy by adding $\epsilon \sim clip(\mathcal{N}(0,
\sigma), -c , +c)$ to the value function $Q_{w}(s&rsquo;, \mu_{\theta}(s&rsquo;) + \epsilon))$. It mitigates the risk of
deterministic policies overfitting the value function.</li>
</ol>
<h3 id="sac">SAC<a hidden class="anchor" aria-hidden="true" href="#sac">¶</a></h3>
<p><strong>Soft Actor-Critic</strong> learns a more random policy by incorporating the entropy of the policy $H(\pi)$ into the reward.</p>
<p>Three key components:</p>
<ul>
<li>An actor-critic architecture</li>
<li>An off-policy approach</li>
<li>Entropy Maximization to encourage exploration</li>
</ul>
<p>The policy is trained by maximizing the objective function: expected return + the entropy


$$
J(\theta) = \sum\limits_{t = 1}^athbb{E}_{s_{t},a_{t} \sim \rho_{\pi_
{\theta}}} [r(s_{t},a_{t}) + \alpha \mathcal{H}(\pi_{\theta}(* | s_{t}))]
$$

</p>
<h3 id="ppo-proximal-policy-optimization">PPO (Proximal Policy Optimization)<a hidden class="anchor" aria-hidden="true" href="#ppo-proximal-policy-optimization">¶</a></h3>
<p>#on-policy</p>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>clipped objective</li>
<li><strong>Proximal</strong> stands for <strong>Reward Model</strong></li>
</ul>
</blockquote>
<p>As a successor of <a href="/posts/ai/rl/reinforcement-learning/#a2c">A2C</a>, PPO defines 2 more models:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Reward $r_{\theta}$</td>
<td style="text-align:left">Calculate $R$</td>
</tr>
<tr>
<td style="text-align:left">Reference $\pi_{ref}$</td>
<td style="text-align:left">Apply constraint and guidance to <em>Actor</em></td>
</tr>
<tr>
<td style="text-align:left">$r^{\ast}$</td>
<td style="text-align:left">Ground-truth reward function</td>
</tr>
<tr>
<td style="text-align:left">$r_\phi$</td>
<td style="text-align:left">MLE of $r^{\ast}$</td>
</tr>
</tbody>
</table>
<p>$$
L(\theta) = \underbrace{-\hat A_{t} \cdot min(r_{t}(\theta), clip(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon))}<em>{\text{A2C loss, $\le 1$ + $\epsilon$}}  -  \underbrace{\beta D</em>{KL}(\pi_{\theta}(y|x)||\pi_{ref}(y|x))}_{\text{penalty of being too distant to normal response}}
$$</p>
<p>where:</p>
<ul>
<li>$r_{t}(\theta) = \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}$: the ratio of new policy to old policy</li>
<li>$\epsilon$: normally 0.1 or 0.2</li>
</ul>
<ul>
<li>Generate two outputs from same input $x$: $y_{1}, y_{2}$
<ul>
<li>Objective: $\mathcal{L} = \underset{\pi_{\theta}}{\max}\mathbb{E}[r_{\theta}(x,y_{2})]$
<ul>
<li>Update:
<ul>
<li>Optimize with the reward of current batch</li>
<li>TRO(<strong>Trust Region Optimization</strong>): using <strong>gradient constraint</strong> to make sure the update process doesn&rsquo;t sabotage the stability of learning process.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>$r$ and $\pi$ can be optimized iteratively.</li>
<li>RLHF and PPO is difficult to train.</li>
</ul>
</blockquote>
<h3 id="dpodirect-preference-optimization">DPO(Direct Preference Optimization)<a hidden class="anchor" aria-hidden="true" href="#dpodirect-preference-optimization">¶</a></h3>
<blockquote>
<p>[!NOTE] The major difference
<strong>Direct</strong>: directly optimize with reward, rather than $V | Q$: <strong>expected</strong> rewards from a reward model</p>
</blockquote>
<p>Rewrite objective:</p>
<p>$$
\begin{align*}
\pi
&amp;= \underset{\pi}{\max}(r_{\phi}(x,y) - \beta D_{KL}(\pi_{\theta}(y|x)||\pi_{ref}(y|x)))\\
&amp;= \underset{\pi}{\max}(r_{\phi}(x,y) - \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})\\
&amp;= \underset{\pi}{\min}( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)} - r_{\phi}(x,y)/\beta)\\
&amp;= \underset{\pi}{\min}( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x) e^{r_{\phi}(x,y)/\beta} })
\end{align*}
$$</p>
<p>^0639d4</p>
<p>Define partition function: $Z = \Sigma_{y}{\pi_{ref}(y|x) e^{r_{\theta}(x,y)/\beta}}$, which relates to the reward of $\theta$ over $ref$</p>
<p>We can get the optimal strategy $\pi^{\ast}$ under $r_{\phi}$(irrelevant of $\theta$):</p>
<p>$$
\pi^{*}(y|x)  = \pi_{ref}(y|x)e^{\frac{r_{\phi} (x,y)}{\beta}} \frac{1}{Z(x)}
$$</p>
<p>^5ee375</p>
<p>Then Eq [[#^0639d4]] became:</p>
<p>$$
\begin{align*}
\pi
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x) e^{r_{\phi}(x,y)}{\beta}}\right)\\
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi^{\ast}(y|x) Z(x)}\right)\\
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi^{\ast}(y|x)}\right)\\
&amp;= \underset{\pi}{\min}\left( D_{KL}(\pi_{\theta}(y|x) || \pi^{\ast}(y|x))\right)
\end{align*}
$$</p>
<p>Apparently, the optimal $\pi$ is: $\pi_{\theta} \to \pi^{*}$.</p>
<p>Noticing that the reward function of E.Q. [[#^5ee375]] can be rewritten(reparameterized) as(where $\pi_{ref}$ is the human-preference data as ground-truth):</p>
<p>$$
r_{\phi} (x,y) = \beta \log \frac{\pi^{\ast}(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)
$$</p>
<blockquote>
<p>[!TIP] the reward function can be represted with best policy trained under it</p>
</blockquote>
<p>By replacing $r_{\phi} (x,y)$ in the objective of RLHF as $\pi^{*}$, we get an objective function without the <strong>reward function</strong>:</p>


$$
\begin{align}
\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{ref}) = -{{\mathbb{E}_{(x, y_{w}, y_{l}) \sim D}[\log \sigma{({\beta \frac{\pi_{\theta}(y_{w}|x)}{\pi_{ref}(y_{w}|x)} - \beta\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)} }})]}}
\end{align}
$$


<p>From this equation, we found that: <strong>Training the reward model in RLHF is equivalent to training $\pi_{\theta}$ with the derived objective function</strong>.</p>
<p>That is to say, no need of 4 models, we can achieve the same target of RLHF with directly training $\pi_{\theta}$.</p>
<h2 id="methods">Methods<a hidden class="anchor" aria-hidden="true" href="#methods">¶</a></h2>
<h3 id="rlhf">RLHF<a hidden class="anchor" aria-hidden="true" href="#rlhf">¶</a></h3>
<p><strong>RLHF(Reinforcement learning from human feedback)</strong> is a technique that trains a <strong>reward model</strong>.</p>
<p>It has following key concepts:</p>
<ul>
<li><strong>Reward Model</strong>: trained in advance directly from human feedback</li>
<li>human feedback: data collected by asking humans to <strong>rank</strong> instances of the agent&rsquo;s behavior</li>
</ul>
<p>The procedure is given by 3 steps</p>
<h4 id="1-sft">1. SFT<a hidden class="anchor" aria-hidden="true" href="#1-sft">¶</a></h4>
<p>Pre-train a (target) model: $\pi^{SFT}$</p>
<h4 id="2-reward-modeling-phase">2. Reward Modeling Phase<a hidden class="anchor" aria-hidden="true" href="#2-reward-modeling-phase">¶</a></h4>
<p>Train a reward model: $r_{\phi}(x,y) = r, r \in (0, + \infty)$, where $r$ is the reward of the given input.</p>
<ul>
<li>
<p>Initialization: Often initialized from Pretrained Models</p>
</li>
<li>
<p>Data:</p>
<ul>
<li>$D$:  $Prompt: x \to (Generation: y, Reward: r)$, generated by human or models</li>
<li>Human Feedback: <strong>Ranking</strong> the outputs of different models under same prompt with $r$
<ul>
<li>effective ways of ranking: Comparing two/ ELO</li>
</ul>
</li>
<li>$(y_{win}, y_{loss})$ : sampled from generation</li>
</ul>
</li>
<li>
<p>Train the RM with Data
The Objective is (negative log-likelihood loss):

  
  $$
  \begin{align*}
  \mathcal{L}_{R}(r_{\phi}, D) = -{{\mathbb{E}_{(x, y_{w}, y_{l}) \sim
  D}[\log{\sigma({r_{\phi}(x, y_{w}) - r_{\phi}(x, y_{l})}})]}}
  \end{align*}
  $$

  </p>
<p>maximize the gap of rewards between better/worse response</p>
</li>
</ul>
<h4 id="3-rl-fine-tuning-phase-pi_thetax--py">3. RL Fine-Tuning Phase: $\pi_{\theta}(x) = p(y)$<a hidden class="anchor" aria-hidden="true" href="#3-rl-fine-tuning-phase-pi_thetax--py">¶</a></h4>
<ul>
<li>In the past, training LM with RL was considered impossible.</li>
<li>One of the proposed feasible plan is PGR(<strong>Policy Gradient RL</strong>)/PPO(<strong>Proximal Policy Optimization</strong>)</li>
</ul>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">¶</a></h2>


    </div>

    <footer class="post-footer"><ul class="post-tags"><li>
                <a href="https://mickjagger19.github.io/tags/rl/">RL</a>
            </li><li>
                <a href="https://mickjagger19.github.io/tags/rlhf/">RLHF</a>
            </li><li>
                <a href="https://mickjagger19.github.io/tags/dpo/">DPO</a>
            </li></ul>
<nav class="paginav">
    <a class="prev" href="https://mickjagger19.github.io/posts/ai/models/the-gan-family/">
    <span class="title">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
           stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
           class="feather feather-arrow-left" style="user-select: text;"><line x1="19" y1="12" x2="5" y2="12"
                                                                               style="user-select: text;"></line><polyline
              points="12 19 5 12 12 5" style="user-select: text;"></polyline></svg>&nbsp;
    </span>
        <br>
        <span>GAN &amp; Variants</span>
    </a>
    <a class="next" href="https://mickjagger19.github.io/posts/ai/models/denoising-diffusion-models/">
    <span class="title">
      &nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
                 stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                 class="feather feather-arrow-right" style="user-select: text;"><line x1="5" y1="12" x2="19" y2="12"
                                                                                      style="user-select: text;"></line><polyline
            points="12 5 19 12 12 19" style="user-select: text;"></polyline></svg>
    </span>
        <br>
        <span>Denoising Diffusion Models</span>
    </a>
</nav>

    </footer>
    <div class="comments-separator"></div>
</article>
    </main>
    
<footer class="footer">
  <span>&copy; 2024 <a href="https://mickjagger19.github.io/">Mick&#39; Blog</a></span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied!';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

</body>

</html>
